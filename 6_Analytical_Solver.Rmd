---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Analytic Solver for Quadratic Programming Problems
The advantages and disadvantages of analytical solvers for quadratic programming problems are discussed in this chapter. It is beyond the scope of this thesis to explain the underlying mathematical principles of how a solver solves quadratic problems; only the applications and analysis are discussed. The main reason for dealing with quadratic programming solution methods is to use them as a benchmark for PSO.

## Quadratic Programming (QP)
A quadratic program is a minimization problem of a function that returns a scalar value and consists of a quadratic term and a linear term that depend on the variable of interest. In addition, the problem may be constrained by several linear inequalities that bound the solution. The general formulation used is to find $x$ that minimizes the following problem:
$$
  min \ \frac{1}{2} \cdot x^T \times D \times x - d^T \times x 
$$
and is valid under the linear constraints:
$$
  A^T \times x \geq b_0
$$

Some other sources notate the problem with different signs or coefficients, all of which are interchangeable with the above problem. In addition, the above problem has the same notation used in the R package `quadprog`, which reduces the substitution overhead. All modern programming languages have many solvers for quadratic problems. They differ mainly in the computation time for certain problems and the requirements. Some commercial QP solvers additionally accept more complex constraints, such as absolute (e.g., $|A^T \times x| \geq a_0$) or mixed-integer (e.g., $x \in \mathbb{N}$). Especially the mixed-integer constraint problems lead to a huge increase in memory requirements.

## QP Solver from quadprog
The most common free QP solver used in R comes from the package [quadprog] (https://cran.r-project.org/web/packages/quadprog/quadprog.pdf), which consists of a single function called `solve.QP()`. Its implementation routine is the dual method of Goldfarb and Idnani published in [@GoId1982] and [@GoId1983]. It uses the above QP with the condition that $D$ must be a symmetric positive definite matrix. This means that $D\in \mathbb{R}^{N \times N}$ and $x^T D x > 0 \ for all \ x \ in \mathbb{R}^N$, which is equivalent to all eigenvalues being greater than zero. In most cases this is not achieved by estimating the covariance matrix $\sum$, but it is possible to find the nearest positive definite matrix of $\textstyle\sum$ using the function `nearPD()` from the [matrix](https://cran.r-project.org/web/packages/Matrix/Matrix.pdf) R package. The error encountered often does not exceed a percentage change in elements over $10^{-15} \%$, which is negligible for the context of this work. The function `solve.QP()` for an $N$ dimensional vector of interest, has the following arguments, which are also found in the above formulation of a QP:

+ `Dmat`: Symmetric positive definite matrix $D \in \mathbb{R}^{N \times N}$ of the quadratic term
+ `dvec`: Vector $d \in \mathbb{R}^{N}$ of the linear term
+ `Amat`: Constraint matrix $A$
+ `bvec`: Constraint vector $b_0$
+ `meq = 1`: means that the first row of $A$ is treated as an equality constraint

The return of `solve.QP()` is a list and contains, among others, the following attributes of interest:

+ `solution`: Vector containing the solution $x$ of the quadratic programming problem (e.g. portfolio weights)
+ `value`: Scalar, the value of the quadratic function at the solution

## Example: Solving MVP with `solve.QP()` {#exampleanalyticalmvp}
This section provides insights into the effects of diversification and the use of `solve.QP` by creating ten different efficiency frontiers from a pool of ten assets. Each efficiency frontier $i \in \{1, 2, \cdots, 10\}$ consists of $N_i = i$ assets and is created by adding the asset with the next smallest variance first. After loading the returns for ten of the largest stocks in the U.S. market, the variance is calculated to rank all columns in ascending order of variance, as shown in the code below:
```{r, class.source="code_fold_it_collapsed"}
returns_raw <- buffer(
  get_yf(
    tickers = c("IBM", "GOOG", "AAPL", "MSFT", "AMZN", 
                "NVDA", "JPM", "META", "V", "WMT"), 
    from = "2016-01-01", 
    to = "2021-12-31"
  )$returns, 
  "AS_10_assets"
)

# re-arrange: low var first
vars <- sapply(returns_raw, var)
returns_raw <- returns_raw[, order(vars, decreasing = F)]
```

The next step is to create a function `mvp()` that has the arguments `return` and `lambda`. It computes the expected returns `mu` and the estimated positive definite covariance `cov`. It then solves an MVP with constraints $\textstyle\sum w = 1$ and $w \geq 0$, which yields the key features `mu`, `var` and `composition` of the portfolio. 

```{r, class.source="code_fold_it_collapsed"}
mvp <- function(returns, lambda){
  tc <- tryCatch({
    mu <- ret_to_geomeanret(returns)

    cov <- as.matrix(nearPD(cov(returns))$mat)

    mat <- list(
      Dmat = lambda * cov,
      dvec = (1-lambda) * mu,
      Amat = t(rbind(
        rep(1, ncol(returns)), # sum up to 1
        diag(1, nrow=ncol(returns), ncol=ncol(returns)) # long only
      )),
      bvec = c(
        1, # sum up to 1
        rep(0, ncol(returns)) # long only
      ),
      meq = 1
    )
  
    qp <- solve.QP(
      Dmat = mat$Dmat, dvec = mat$dvec, 
      Amat = mat$Amat, bvec = mat$bvec, meq = mat$meq
    )
    
    res <- list(
      "mu" = mu %*% qp$solution,
      "var" = t(qp$solution) %*% cov %*% qp$solution,
      "composition" = setNames(qp$solution, colnames(returns))
    )
    TRUE
  }, error = function(e){FALSE})
  

  if(tc){
    return(res)
  }else{
    return(list(
      "mu" = NA,
      "var" = NA,
      "composition" = NA
    ))
  }
}
```

Each $\lambda \in \{0.01, 0.02, \cdots, 1\}$ and each combination of ascending number of assets results in a portfolio that can be created with two for loops.

```{r, class.source="code_fold_it_collapsed"}
df <- data.frame(
  "index"=1, 
  "var"=as.numeric(var(returns_raw[, 1])), 
  "return" = as.numeric(ret_to_geomeanret(returns_raw[, 1])), 
  row.names=NULL
)
for(i in 2:ncol(returns_raw)){
  returns <- returns_raw[, 1:i]
  for(lambda in seq(0.01, 1, 0.01)){
    res <- mvp(returns, lambda)
    
    df <- rbind(
      df, 
      data.frame("index"=i, "var"=res$var, "return" = res$mu)
    )
  }
}

```

The result is filtered and names are added to represent the number of assets. Now the diagram can be created:

```{r, echo = knitr::is_html_output(), class.source="code_fold_it_collapsed"}
df <- df %>% 
  filter(!is.na(return)) %>% 
  distinct() %>% 
  mutate(name = paste0("n_", index)) %>% 
  arrange(name) %>% 
  mutate(name = factor(name, levels=paste0("n_", ncol(returns_raw):1)))


max_show_sd <- df %>% 
  group_by(index) %>% 
  summarise(max_x = max(var)) %>% 
  pull(max_x) %>% 
  mean() %>% 
  sqrt()

plot_ly(
    data = df[df$index!=1,], 
    x=~sqrt(var), 
    y=~return, 
    name=~name, 
    mode="lines", 
    type = 'scatter', 
    color = ~name, 
    colors = c("green", "red")
  ) %>% 
  add_trace(
    data=df[df$index==1,],  
    x=~sqrt(var), 
    y=~return, 
    showlegend=T, 
    marker=list(color="red"), 
    mode="markers", 
    name="n_1") %>% 
  layout(
    xaxis=list(range=c(sqrt(min(df$var))*0.9, max_show_sd), title="standard deviation"),
    yaxis=list(range=c(min(df$return)*0.9, (max(df$return)+mean(df$return))*0.5), title="return")) %>% 
  html_save()

```

It can be seen, that each asset added results in a minimum variance portfolio with smaller or equal standard deviation. Nevertheless, we started with the asset that has the smallest standard deviation of `r sqrt(min(vars))`. This is the effect of diversification mentioned by Markowitz.



## Example: Solving ITP with `solve.QP()` {#exampleitpsolveqp}
This example analyzes how many assets are needed to minimize the variance between the replication and historical returns of the S&P 500 from 2016-01-01 to 2021-12-31. The constraints are set to be long only and the weights should sum to one. To gradually reduce the number of assets, the five assets with the lowest weights are discarded and serve as the new asset pool for the next replication until only five assets are left. First, the required data can be downloaded from the `R/` directory using existing functions. The function `get_spx_composition()` uses web scraping to read the components of [wikipedia](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies) and converts them into monthly compositions of the S&P 500. The pool is formed from all assets present in the last month of the time frame, reduced by assets with missing values. The code below loads the returns of all assets in the pool and the S&P 500:

```{r, class.source="code_fold_it_collapsed"}
from <- "2016-01-01"
to <- "2021-12-31"

spx_composition <- buffer(
  get_spx_composition(),
  "AS_spx_composition"
)


pool_returns_raw <- buffer(
  get_yf(
    tickers = spx_composition %>% 
      filter(Date<=to) %>% 
      filter(Date==max(Date)) %>% 
      pull(Ticker), 
    from = from, 
    to = to
  )$returns, 
  "AS_sp500_assets"
)
pool_returns_raw <- 
  pool_returns_raw[, colSums(is.na(pool_returns_raw))==0]


bm_returns <- buffer(
  get_yf(tickers = "%5EGSPC", from = from, to = to)$returns, 
  "AS_sp500"
) %>% setNames(., "S&P 500")

```
The required data is now available and the function for the ITP can be created. It requires `pool_returns` with variable number of columns and the single-column matrix `bm_returns`.
```{r, class.source="code_fold_it_collapsed"}

itp <- function(pool_returns, bm_returns){
  mat <- list(
    Dmat = cov(pool_returns),
    dvec = cov(pool_returns, bm_returns),
    Amat = t(rbind(
      rep(1, ncol(pool_returns)), # sum up to 1
      diag(1, 
           nrow=ncol(pool_returns), 
           ncol=ncol(pool_returns)) # long only
    )),
    bvec = c(
      1, # sum up to 1
      rep(0, ncol(pool_returns)) # long only
    ),
    meq = 1
  )
  
  qp <- solve.QP(
    Dmat = mat$Dmat, dvec = mat$dvec, 
    Amat = mat$Amat, bvec = mat$bvec, meq = mat$meq
  )

  res <- list(
    "var" = as.numeric(
      var(pool_returns %*% qp$solution - bm_returns)),
    "solution" = setNames(qp$solution, colnames(pool_returns))
  )
}
```

The duplication and successive discarding of assets can begin. The results are stored in `res` and used to display the results.

```{r, class.source="code_fold_it_collapsed"}
res <- NULL
for(i in rev(seq(5, ncol(pool_returns_raw), 5))){
  if(i==ncol(pool_returns_raw)){
    temp <- itp(pool_returns_raw, bm_returns)
  }else{
    temp <- itp(pool_returns_raw[, names(sort(temp$solution, decreasing = T)[1:i])], bm_returns)
  }
  res <- rbind(res, data.frame("N"=i, "var"=temp$var, "sd"=sqrt(temp$var), row.names = NULL))
}
```

```{r, echo = knitr::is_html_output(), class.source="code_fold_it_collapsed"}
plot_ly(data=res, x=~N, y=~sd, mode="lines", type = 'scatter') %>% 
  layout(yaxis=list(range=c(0, mean(max(res$sd),mean(res$sd)) ))) %>% 
  html_save()
```


It can be seen that the standard deviation stagnates at about $N=100$. This leads to the conclusion that a sparse replication with one hundred assets is sufficient in this particular case to track the historical performance of the S&P 500 over this period.




