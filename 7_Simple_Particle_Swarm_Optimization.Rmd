---
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---
# Particle Swarm Optimization (PSO)
The PSO was developed by J. Kennedy as a global optimization method based on swarm intelligence and presented to the public in 1995 by Eberhart and Kennedy [@KeEb1995]. The original PSO was intended to resemble a flock of birds flying through the sky without collisions. Therefore, its first applications were found in particle physics to analyze moving particles in high-dimensional spaces, which the name Particle recalls. Later, it was adapted in Evolutionary Computation to exploit a set of potential solutions in high dimensions and to find the optima by cooperating with other particles in the swarm [@PaVr2002]. Since it does not require gradient information, it is easier to apply than other global optimization methods. It can find the optimum by considering only the result of the function to be optimized. This means that the function can be arbitrarily complex and it is still possible to reach the global optimum. Other advantages are the low computational costs, since only basic mathematical operators are used.

## The Algorithm
Each particle $d$ with position $x_d$ moves in the search space $\mathbb{R}^N$ and has its own velocity $v_d$ and remembers its previous best position $P_d$. After each iteration, the velocity changes in the direction of the intrinsic velocity, the best previous position, and the global best position $p_g$ of all particles. A position change from $i$ to $i+1$ can be calculated by the following two equations [@PaVr2002]:

\begin{align*}
  v_d^{i+1} &= wv_d^{i} + c_p r_1^i (P_d^i - x_d^i) + c_g r_2^i (p_g^i - x_d^i) \\
  x_d^{i+1} &= x_d^i + v_d^{i+1}
\end{align*}

Where $r_1$ and $r_2$ are uniformly distributed random numbers in [0, 1]. The cognitive parameter $c_p$ acts as a weighting of the direction to its previous best position of the particle. This contrasts with the social parameter $c_g$, which is a weighting of the direction to the global best position. The inertial weight $w$ is crucial for the convergence behavior by remembering part of its previous trajectory. A study reviewed in [@PaVr2002] showed that these parameters can be set to $c_p=c_g=0.5$ and $w$ should decrease from $1.2$ to $0$. However, some problems benefit from a more precise tuning of these parameters. To allow effortless translation to code, the above formula for $d = 1, 2, \cdots, D$ particles can be given in the following matrix notation:

\begin{align*}
  V^{i+1} &= w \cdot V^{i} + c_p \cdot r_1^i \cdot (P^i-X^i) + c_g \cdot r_2^i \cdot (p_g^i - X^i) \\
  X^{i+1} &= X^i + V^{i+1}
\end{align*}

With current positions $X \in \mathbb{R}^{N \times D}$, current velocities $V \in \mathbb{R}^{N \times D}$, previous best positions $P \in \mathbb{R}^{N \times D}$, and global best position $p_g \in \mathbb{R}^{N}$. The parameters $w$, $c_p$, $c_g$, $r_1$ and $r_2$ are stile scalars.

## `pso()` Function
In this section, a general PSO function is created that follows the structure of other optimization heuristics in R, in particular the existing PSO implementation from the R package `pso`. The key component of the problem is a objective function called `fn()`, which returns a scalar that needs to be minimized. The function itself mainly needs a vector `pos` that describes the position of a particle (e.g. weights). The other main parameters for the PSO function are `par`, which is a position of a particle used to derive the dimension of the problem and as the first position of a particle. The argument can only contain `NA`'s, resulting in completely random starting positions. The last two arguments are `lower` and `upper` bounds (e.g. weights greater than 0 and less than 1). All other parameters have default values that can be overridden by passing a list called `control`. The resulting structure is:
```{r pso1}
pso <- function(
    par, 
    fn, 
    lower, 
    upper, 
    control = list()
  ){

}
```

Before the main data structure can be initialized, some sample inputs must be created for the `pso()` function as described below:

```{r pso2}
par <- rep(NA, 2)
fn <- function(x){return(sum(abs(x)))}
lower <- -10
upper <- 10
control = list(
  s = 10, # swarm size
  c.p = 0.5, # inherit best
  c.g = 0.5, # global best
  maxiter = 100, # iterations
  w0 = 1.2, # starting inertia weight
  wN = 0, # ending inertia weight
  save_traces = F # save more information
)
```

Now it is time to initialize the random positions `X`, their fitness `X_fit` and their random velocities `V` with the function `mrunif()` which produces a matrix of uniformly distributed random numbers between `lower` and `upper`:

```{r pso3}
X <- mrunif(
  nr = length(par), nc=control$s, lower=lower, upper=upper
)
if(all(!is.na(par))){
  X[, 1] <- par
}
X_fit <- apply(X, 2, fn)
V <- mrunif(
  nr = length(par), nc=control$s, 
  lower=-(upper-lower), upper=(upper-lower)
)/4
```

The velocities are compressed by a factor of 4 to start with a maximum movement of one quarter of the space in each axis. The personal best positions `P` are the same as `X` and the global best position is the position with the smallest fitness:


```{r pso4}
P <- X
P_fit <- X_fit
p_g <- P[, which.min(P_fit)]
p_g_fit <- which.min(P_fit)
```

The required data structure is available and the optimization can start with the calculation of the new velocities and the transformation of the old positions. When particles have left the valid space, they are pushed back to the edge and the velocities are set to zero. Then the fitness is calculated and the personal best and global best positions are saved if they have improved.

```{r pso5}
trace_data <- NULL
for(i in 1:control$maxiter){
  # move particles
  V <- 
    (control$w0-(control$w0-control$wN)*i/control$maxiter) * V + 
    control$c.p * runif(1) * (P-X) + 
    control$c.g * runif(1) * (p_g-X)
  X <- X + V
  
  # set velocity to zeros if not in valid space
  V[X > upper] <- 0
  V[X < lower] <- 0
  
  # move into valid space
  X[X > upper] <- upper
  X[X < lower] <- lower
  
  # evaluate objective function
  X_fit <- apply(X, 2, fn)
  
  # save new previews best
  P[, P_fit > X_fit] <- X[, P_fit > X_fit]
  P_fit[P_fit > X_fit] <- X_fit[P_fit > X_fit]
  
  # save new global best
  if(any(P_fit < p_g_fit)){
    p_g <- P[, which.min(P_fit)]
    p_g_fit <- min(P_fit)
  }
  
  if(control$save_traces==TRUE){
    trace_data <- rbind(trace_data, data.frame("iter"=i, t(X)))
  }
}
```
The best fitness after $100$ iterations is `r p_g_fit` and the best possible solution is $0$.

```{r pso6, include = knitr::is_html_output(), class.source="code_fold_it_collapsed"}
# the resulting pso() function
pso <- function(
    par, 
    fn, 
    lower, 
    upper, 
    control = list()
  ){

  # use default control values if not set
  control_ = list(
    s = 10, # swarm size
    c.p = 0.5, # inherit best
    c.g = 0.5, # global best
    maxiter = 200, # iterations
    w0 = 1.2, # starting inertia weight
    wN = 0, # ending inertia weight
    save_traces = F # save more information
  )
  control <- c(control, control_[!names(control_) %in% names(control)])
  
  # init data-structure
  X <- mrunif(
    nr = length(par), nc=control$s, lower=lower, upper=upper
  )
  if(all(!is.na(par))){
    X[, 1] <- par
  }
  X_fit <- apply(X, 2, fn)
  V <- mrunif(
    nr = length(par), nc=control$s, 
    lower=-(upper-lower), upper=(upper-lower)
  )/4
  P <- X
  P_fit <- X_fit
  p_g <- P[, which.min(P_fit)]
  p_g_fit <- which.min(P_fit)
  
  
  trace_data <- NULL
  for(i in 1:control$maxiter){
    
    # move particles
    V <- 
      (control$w0-(control$w0-control$wN)*i/control$maxiter) * V + 
      control$c.p * runif(1) * (P-X) + 
      control$c.g * runif(1) * (p_g-X)
    X <- X + V
    
    # set velocity to zeros if not in valid space
    V[X > upper] <- -V[X > upper]
    V[X < lower] <- -V[X < lower]
    
    # move into valid space
    X[X > upper] <- upper
    X[X < lower] <- lower
    
    # evaluate objective function
    X_fit <- apply(X, 2, fn)
    
    # save new previews best
    P[, P_fit > X_fit] <- X[, P_fit > X_fit]
    P_fit[P_fit > X_fit] <- X_fit[P_fit > X_fit]
    
    # save new global best
    if(any(P_fit < p_g_fit)){
      p_g <- P[, which.min(P_fit)]
      p_g_fit <- min(P_fit)
    }
    
    if(control$save_traces){
      trace_data <- rbind(trace_data, data.frame("iter"=i, t(X)))
    }
  }
  
  res <- list(
    "solution" = p_g,
    "fitness" = p_g_fit
  )
  if(control$save_traces){
    res$trace_data <- trace_data
  }
  return(res)
}
```




## Animation 2-Dimensional
This section provides insights into the behavior of the PSO by visualizing multiple iterations in a GIF. The GIF only works in Adobe Acrobat DC or in the Markdown/HTML version of this paper. The amazing animation template is inspired by [R'tichoke](https://www.r-bloggers.com/2021/10/how-to-build-a-basic-particle-swarm-optimiser-from-scratch-in-r/). The PSO core from the above chapter was used to complete the `pso()` function and is tested here with seed 0. The function `fn` to be evaluated can be found in [R'tichoke](https://www.r-bloggers.com/2021/10/how-to-build-a-basic-particle-swarm-optimiser-from-scratch-in-r/). 

```{r gif_create}
set.seed(0)

fn <- function(pos){
  -20 * exp(-0.2 * sqrt(0.5 *((pos[1]-1)^2 + (pos[2]-1)^2))) - 
  exp(0.5*(cos(2*pi*pos[1]) + cos(2*pi*pos[2]))) + 
  exp(1) + 20
}

res <- pso(
  par = rep(NA, 2),
  fn = fn,
  lower = -10,
  upper = 10,
  control = list(
    s = 10,
    maxiter = 30,
    w0 = 0.8,
    save_traces = T
  )
)
```

The function `fn` has many local minima and a global minima at $(1,1)$ with the value $0$. The background color scale ranges from 0 as red to 20 as purple. The PSO has 10 particles, iterated 30 times with an inertia weight decreasing from 0.8 to 0. The iterations are visualized in the following GIF:

```{r gif_animate, echo=F}
grid <- expand.grid(seq(-10, 10, length.out = 100), seq(-10, 10, length.out = 100), stringsAsFactors = F)
grid$z <- apply(grid, 1, fn)


background <- ggplot() +
  geom_contour_filled(data = grid, aes(x = Var1, y = Var2, z = z), color = "black", alpha = 0.5) +
  scale_fill_brewer(palette = "Spectral") +
  theme(axis.line=element_blank(),
      axis.text.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks=element_blank(),
      axis.title.x=element_blank(),
      axis.title.y=element_blank(),
      legend.position="none",
      panel.background=element_blank(),
      panel.border=element_blank(),
      panel.grid.major=element_blank(),
      panel.grid.minor=element_blank(),
      plot.background=element_blank())
ggsave(background, filename = "gifs/pso_2dim/background.jpg", scale = 1, dpi=150)


anim <- ggplot(res$trace_data) +
  background_image(jpeg::readJPEG("gifs/pso_2dim/background.jpg")) +
  geom_point(aes(X1, X2)) +
  xlim(-10, 10) +
  ylim(-10, 10) +
  labs(x = "X", y = "Y") +
  transition_time(iter) +
  ease_aes("linear")



if(!knitr::is_latex_output()){
  temp <- sapply(list.files('gifs/pso_2dim/', full.names = T), file.remove)
  suppressMessages(animate(anim, renderer = file_renderer('gifs/pso_2dim/'), device = "jpeg", nframes=100))
  images <- list.files('gifs/pso_2dim/')
  for(i in 1:length(images)){
    file.rename(
      paste0('gifs/pso_2dim/',images[i]), 
      paste0("gifs/pso_2dim/gganim_plot", as.numeric(gsub(".jpg", "", gsub(pattern = "gganim_plot","", images[i]))), ".jpg")
    )
  }
  anim
}

```
```{=latex}
\animategraphics[loop, width=8cm]{10}{./gifs/pso_2dim/gganim_plot}{1}{50}
```


## Example MVP
This example uses the `solve.QP` approach from \@ref(exampleanalyticalmvp) with ten assets as the benchmark. Briefly, the goal is to create an MVP from ten of the largest U.S. stocks between 2018-01-01 and 2019-12-31 for each possible $\lambda$. The PSO has 300 particles and 200 iterations for each lambda. The main characteristics of all portfolios created with the `solve.QP` compared to the PSO are shown below:

```{r pso7, echo = knitr::is_html_output(), class.source="code_fold_it_collapsed"}
set.seed(0)

returns_raw <- buffer(
  get_yf(
    tickers = c("IBM", "GOOG", "AAPL", "MSFT", "AMZN", 
                "NVDA", "JPM", "META", "V", "WMT"), 
    from = "2018-01-01", 
    to = "2019-12-31"
  )$returns, 
  "AS_10_assets"
)

# re-arrange: low var first
vars <- sapply(returns_raw, var)
returns_raw <- returns_raw[, order(vars, decreasing = F)]

mvp_QP <- function(returns, lambda){
  tc <- tryCatch({
    mu <- ret_to_geomeanret(returns)

    cov <- as.matrix(nearPD(cov(returns))$mat)

    mat <- list(
      Dmat = lambda * cov,
      dvec = (1-lambda) * mu,
      Amat = t(rbind(
        rep(1, ncol(returns)), # sum up to 1
        diag(1, nrow=ncol(returns), ncol=ncol(returns)) # long only
      )),
      bvec = c(
        1, # sum up to 1
        rep(0, ncol(returns)) # long only
      ),
      meq = 1
    )
  
    qp <- solve.QP(
      Dmat = mat$Dmat, dvec = mat$dvec, 
      Amat = mat$Amat, bvec = mat$bvec, meq = mat$meq
    )
    
    res <- list(
      "mu" = mu %*% qp$solution,
      "var" = t(qp$solution) %*% cov %*% qp$solution,
      "composition" = setNames(qp$solution, colnames(returns))
    )
    TRUE
  }, error = function(e){FALSE})
  

  if(tc){
    return(res)
  }else{
    return(list(
      "mu" = NA,
      "var" = NA,
      "composition" = NA
    ))
  }
}


mvp_PSO <- function(returns, lambda, silent = T){
  tc <- tryCatch({
    mu <- ret_to_geomeanret(returns)

    cov <- as.matrix(nearPD(cov(returns))$mat)

    mat <- list(
      Dmat = lambda * cov,
      dvec = (1-lambda) * mu,
      Amat = t(rbind(
        rep(1, ncol(returns)), # sum up to 1
        diag(1, nrow=ncol(returns), ncol=ncol(returns)) # long only
      )),
      bvec = c(
        1, # sum up to 1
        rep(0, ncol(returns)) # long only
      ),
      meq = 1
    )
  
    calc_fit <- function(x){
      0.5 * t(x) %*% mat$Dmat %*% x - t(mat$dvec) %*% x
    }
    calc_const <- function(x){
      const <- t(mat$Amat) %*% x - mat$bvec
      const[mat$meq] <- -max(0, abs(const[mat$meq])-0.01)
      -sum(min(0, const))
    }
    pso_res <- pso(
      par = rep(NA, ncol(returns)),
      fn = function(x){
        fitness <- calc_fit(x)
        constraints <- calc_const(x)
        return(fitness + 10 * constraints)
      },
      lower = 0,
      upper = 1,
      control = list(
        s = 300, # swarm size
        c.p = 0.5, # inherit best
        c.g = 0.5, # global best
        maxiter = 200, # iterations
        w0 = 1.2, # starting inertia weight
        wN = 0, # ending inertia weight
        save_traces = F # save more information
      )
    )
    if(!silent){
      p0("constraint: ", sum(calc_const(pso_res$solution)))
      p0("fitness: ", calc_fit(pso_res$solution))
    }
    
    res <- list(
      "mu" = mu %*% pso_res$solution,
      "var" = t(pso_res$solution) %*% cov %*% pso_res$solution,
      "composition" = setNames(pso_res$solution, colnames(returns)),
      "fit" = calc_fit(pso_res$solution),
      "constraint" = sum(calc_const(pso_res$solution))
    )
    TRUE
  }, error = function(e){FALSE})
  

  if(tc){
    return(res)
  }else{
    return(list(
      "mu" = NA,
      "var" = NA,
      "composition" = NA
    ))
  }
}


df <- NULL
runs <- 100
for(i in 0:round((runs-1)/2)){
  lambda <- 1-i/runs
  temp <- mvp_QP(returns = returns_raw, lambda = lambda)
  df <- rbind(df, data.frame("type"="QP_MVP", "lambda"=lambda, "mu"=temp$mu, "var"=temp$var, "constraint"=0))
  
  temp <- mvp_PSO(returns = returns_raw, lambda = lambda)
  df <- rbind(df, data.frame("type"="PSO_MVP", "lambda"=lambda, "mu"=temp$mu, "var"=temp$var, "constraint"=temp$constraint))
}

df$sd = sqrt(df$var)



df_qp <- df[df$type=="QP_MVP" & df$lambda %in% seq(1,0.8,-0.1),]
df_diff <- data.frame(
  "mu_pso" = df[df$type=="PSO_MVP",]$mu,
  "sd_pso" = df[df$type=="PSO_MVP",]$sd,
  "mu_qp" = df[df$type=="QP_MVP",]$mu,
  "sd_qp" = df[df$type=="QP_MVP",]$sd
)

shapes <- list()
for(i in 1:nrow(df_diff)){
  shapes[[i]] <- list(
    type = "line", 
    y0 = df_diff[i,]$mu_pso, 
    y1 = df_diff[i,]$mu_qp, 
    yref = "y",
    xref = "x",
    x0 = df_diff[i,]$sd_pso, 
    x1 = df_diff[i,]$sd_qp, 
    line = list(color = "lightgrey"),
    layer='below'
  )
}


plot_ly(
  data = df, 
  x=~sd, 
  y=~mu, 
  name=~type, 
  mode="markers", 
  type = 'scatter', 
  color = ~type, 
  colors = c("green", "red")
  ) %>% 
  add_annotations(
    data=df_qp,   
    x=~sd, 
    y=~mu, 
    text = ~paste0("lambda: ",lambda),
    ay = -30,
    ax = -30
  ) %>% 
  # add_segments(
  #   #data=df_diff,   
  #   x=df_diff$sd_pso, 
  #   y=df_diff$mu_pso, 
  #   yend = df_diff$sd_diff,
  #   xend = df_diff$mu_diff
  # ) %>% 
  layout(
    xaxis=list(range=c(0.5*min(df$sd), 1.2*max(df[df$type=="QP_MVP",]$sd)), showgrid = FALSE),
    yaxis=list(range=c(0.5*min(df$mu), 1.2*max(df[df$type=="QP_MVP",]$mu)), showgrid = FALSE),
    shapes = shapes,
    legend = list(x = 0.1, y = 0.9)
  ) %>% 
  html_save()
```

The dots for each $\lambda$ are connected with a grey line to visualize the error of the PSO. It can be seen that its possible to solve MVP's with a PSO approach.

## Example: ITP-MSTE
The same ITP-MSTE solved with `solve.QP()` in \@ref(exampleitpsolveqp) is used as the benchmark for the PSO. In summary, the goal is to create a portfolio that minimizes the variance of the returns of itself and the S&P 500 between 2018-01-01 and 2019-12-31. The pool of assets includes all assets that are present in 2019-12-31 and have no missing values. The constraints are long only and the weights should sum to one. The parameters for the PSO are a swarm size of 100, 100 iterations, the inertia weight starts at $1.2$, the upper bound is $0.1$, and a starting position is the zero vector. The PSO was run ten times, and the aggregated best and mean runs are compared to the `solve.QP()` approach for seed 0 in the table below:

```{r pso8, echo = knitr::is_html_output(), class.source="code_fold_it_collapsed"}
set.seed(0)
 
from <- "2018-01-01"
to <- "2019-12-31"

spx_composition <- buffer(
  get_spx_composition(),
  "AS_spx_composition"
)


pool_returns_raw <- buffer(
  get_yf(
    tickers = spx_composition %>% 
      filter(Date<=to) %>% 
      filter(Date==max(Date)) %>% 
      pull(Ticker), 
    from = from, 
    to = to
  )$returns, 
  "AS_sp500_assets"
)
pool_returns_raw <- 
  pool_returns_raw[, colSums(is.na(pool_returns_raw))==0]


bm_returns <- buffer(
  get_yf(tickers = "%5EGSPC", from = from, to = to)$returns, 
  "AS_sp500"
) %>% setNames(., "S&P 500")



itp_QP <- function(pool_returns, bm_returns){

  mat <- list(
    Dmat = t(pool_returns) %*% pool_returns,
    dvec = t(pool_returns) %*% bm_returns,
    Amat = t(rbind(
      rep(1, ncol(pool_returns)), # sum up to 1
      diag(1, 
           nrow=ncol(pool_returns), 
           ncol=ncol(pool_returns)) # long only
    )),
    bvec = c(
      1, # sum up to 1
      rep(0, ncol(pool_returns)) # long only
    ),
    meq = 1
  )
  
  qp <- solve.QP(
    Dmat = mat$Dmat, dvec = mat$dvec, 
    Amat = mat$Amat, bvec = mat$bvec, meq = mat$meq
  )

  
  res <- list(
    "value" = qp$value,
    "var" = as.numeric(
      var(pool_returns %*% qp$solution - bm_returns)),
    "solution" = setNames(qp$solution, colnames(pool_returns))
  )
}


itp_PSO <- function(pool_returns, bm_returns, silent = T){
  mat <- list(
    Dmat = t(pool_returns) %*% pool_returns,
    dvec = t(pool_returns) %*% bm_returns,
    Amat = t(rbind(
      rep(1, ncol(pool_returns)), # sum up to 1
      diag(1, 
           nrow=ncol(pool_returns), 
           ncol=ncol(pool_returns)) # long only
    )),
    bvec = c(
      1, # sum up to 1
      rep(0, ncol(pool_returns)) # long only
    ),
    meq = 1
  )
  
  calc_fit <- function(x){
    as.numeric(0.5 * t(x) %*% mat$Dmat %*% x - t(mat$dvec) %*% x)
  }
  calc_const <- function(x){
    const <- t(mat$Amat) %*% x - mat$bvec
    const[mat$meq] <- -max(0, abs(const[mat$meq])-0.01)
    -min(0, const)
  }
  pso_res <- pso(
    par = rep(0, ncol(pool_returns)),
    fn = function(x){
      fitness <- calc_fit(x)
      constraints <- calc_const(x)
      return(fitness+10*constraints)
    },
    lower = 0,
    upper = 0.1,
    control = list(
      s = 100, # swarm size
      c.p = 0.5, # inherit best
      c.g = 0.5, # global best
      maxiter = 100, # iterations
      w0 = 1.2, # starting inertia weight
      wN = 0, # ending inertia weight
      save_traces = F # save more information
    )
  )
  if(!silent){
    p0("constraint: ", sum(calc_const(pso_res$solution)))
    p0("fitness: ", calc_fit(pso_res$solution))
  }
  
  res <- list(
    "composition" = setNames(pso_res$solution, colnames(pool_returns)),
    "fit" = calc_fit(pso_res$solution),
    "constraint" = calc_const(pso_res$solution),
    "var" = as.numeric(
      var(pool_returns %*% pso_res$solution - bm_returns))
  )
  
  return(res)
}


df <- NULL
time <- system.time(
  temp_qp <- itp_QP(pool_returns_raw, bm_returns)
)
df <- data.frame("type"="ITP-MSTE_QP", "var"=temp_qp$var, "fitness"=temp_qp$value, "constraint"=0, "time"=time[3])
for(i in 1:10){
  time <- system.time(
    temp_pso <- itp_PSO(pool_returns = pool_returns_raw, bm_returns)
  )
  df <- rbind(
    df, 
    data.frame(
      "type"="ITP-MSTE_PSO", 
      "var"=temp_pso$var, 
      "fitness"=temp_pso$fit, 
      "constraint"=temp_pso$constraint, 
      "time"=time[3]
    )
  )
  
  
}
df$sd <- sqrt(df$var)

df_summary <- rbind(
  df %>% filter(type == "ITP-MSTE_QP"),
  df %>% filter(type == "ITP-MSTE_PSO") %>% filter(fitness == min(fitness)) %>% mutate(type = "ITP-MSTE_PSO_best"),
  df %>% filter(type == "ITP-MSTE_PSO") %>% group_by(type) %>% summarise_all(., mean) %>% mutate(type = "ITP-MSTE_PSO_mean")
)
rownames(df_summary) <- NULL
df_summary$var <- round(df_summary$var, 12)
df_summary$sd <- round(df_summary$sd, 6)
df_summary$fitness <- round(df_summary$fitness, 7)
df_summary$constraint <- round(df_summary$constraint, 18)
df_summary$time <- round(df_summary$time, 1)

reactable(
  df_summary %>% select(type, sd, var, fitness, constraint, time),
  wrap = FALSE,
  #compact = T,
  columns = list(
    type = colDef(width=150),
    var = colDef(format = colFormat(digits=9), show =F),
    sd = colDef(format = colFormat(digits=5)),
    constraint = colDef(name = "constraint break")
  )#,
  #theme = reactableTheme(style = list(fontFamily = "-apple-system, BlinkMacSystemFont, Segoe UI, Helvetica, Arial, sans-serif"))
) %>% 
  html_save(., vwidth = 800)
```

It can be seen that in all PSO runs, sufficient fitness was achieved with negligible constraint breaks, but much more computation time was required.

## Pros and Cons for Continuous Problems
A PSO approach has advantages and disadvantages, since on the one hand any problem can theoretically be solved, but it cannot be guaranteed that the solution is also optimal. In addition, the calculations take much longer than with the `solve.QP()` approach, which raises the question why a PSO approach should have any benefit at all. This is exactly the case, if the solution of the problem is no longer possible by the `solve.QP()` alone, as it is for example the case with mixed-integer-quadratic-problems. In these types of problems, the condition for $x$ is to be a integer vector. These problems could be solved by the `solve.QP()` approach only continuously and then rounded. However, this rounding error can become arbitrarily large, which is why the chances of the PSO approach to achieve a better solution are greater than with the `solve.QP()` approach.


## Discrete Problems and Transaction Costs
A continuous solution for a portfolio is not sufficient for the practice, because most of the time only integer amounts of assets can be bought. It's even worse if lot sizes are needed, because these can only be bought in minimum denomination of e.g. ten thousand. Lot sizes are often used in fixed income products. The biggest drawbacks of rounding a continuous solution are the disregarding of conditions and the difference in the objective value, which often cant reach the new optimum. A solution with broken conditions is not acceptable in practice and a `solve.QP()` approach only produces one solution, which is why its insecure to hope for a sufficient solution after rounding. The PSO doesn't have these drawbacks and can be easily used for discrete problems by rounding the input of the objective function `fn()`. In a portfolio with net asset value $\text{nav}$ consisting of only American stocks with weights $w_i$ and closing prices $p_i$ can be discretized to $w_i^d$ by the following formula:

$$
  w_i^d =\text{round}(w_i \cdot \frac{\text{nav}}{p_i})\cdot \frac{p_i}{\text{nav}}
$$

## Example: Discrete ITP-MSTE

This example analyses the error of rounding a solution with the `solve.QP()` approach and compares it to a discrete PSO. A second discrete PSO is added, that takes the continuous solution of the `solve.QP()` and uses it as starting position of one particle. The ITP-MSTE focuses to track the S&P 500 with its top 100 weighted assets and tries to construct a portfolio with long only, $1 < \textstyle\sum w_i > 0.99$ and $\text{nav} = 10000$ in the time frame from 2018-01-01 to 2019-12-31. The used prices are closing prices and both PSO's have 200 particles and 100 iterations. The results can be observed in the table below:

```{r pso9, echo = knitr::is_html_output(), class.source="code_fold_it_collapsed"}
set.seed(0)

nav <- 10000
 
from <- "2018-01-01"
to <- "2019-12-31"

spx_composition <- buffer(
  get_spx_composition(),
  "AS_spx_composition"
)


pool_data <- buffer(
  get_yf(
    tickers = spx_composition %>% 
      filter(Date<=to) %>% 
      filter(Date==max(Date)) %>% 
      pull(Ticker), 
    from = from, 
    to = to
  ), 
  "AS_sp500_asset_data"
)
pool_data$returns <- 
  pool_data$returns[, colSums(is.na(pool_data$returns))==0]
pool_data$prices <- pool_data$prices[, colnames(pool_data$returns)]


bm_returns <- buffer(
  get_yf(tickers = "%5EGSPC", from = from, to = to)$returns, 
  "AS_sp500"
) %>% setNames(., "S&P 500")


pool_returns <- pool_data$returns
mat <- list(
  Dmat = t(pool_returns) %*% pool_returns,
  dvec = t(pool_returns) %*% bm_returns,
  Amat = t(rbind(
    rep(1, ncol(pool_returns)), # sum up to 1
    diag(1, 
         nrow=ncol(pool_returns), 
         ncol=ncol(pool_returns)) # long only
  )),
  bvec = c(
    1, # sum up to 1
    rep(0, ncol(pool_returns)) # long only
  ),
  meq = 1
)

# search 100 best tickers
qp <- solve.QP(
  Dmat = mat$Dmat, dvec = mat$dvec, 
  Amat = mat$Amat, bvec = mat$bvec, meq = mat$meq
)
sub_ticker <- colnames(pool_returns)[order(qp$solution, decreasing = T)[1:100]]


# Default solve.QP
time_qp <- system.time({
  res_qp <- itp_QP(pool_data$returns[,sub_ticker], bm_returns)
  prices <- last(pool_data$prices[,sub_ticker])
  res_qp_discrete <- setNames(as.vector(round(res_qp$solution*nav/prices)*prices/nav), names(res_qp$solution))
})
res_qp_discrete_fit <- as.numeric(0.5 * t(res_qp_discrete) %*% mat$Dmat[names(res_qp_discrete), names(res_qp_discrete)] %*% res_qp_discrete - t(as.vector(mat$dvec[names(res_qp_discrete)])) %*% res_qp_discrete)
res_qp_discrete_sum_wgt <- sum(res_qp_discrete)


# Default PSO
pool_returns <- pool_data$returns[, sub_ticker]
mat <- list(
  Dmat = t(pool_returns) %*% pool_returns,
  dvec = t(pool_returns) %*% bm_returns,
  Amat = t(rbind(
    rep(1, ncol(pool_returns)), # sum up to 1
    diag(1, 
         nrow=ncol(pool_returns), 
         ncol=ncol(pool_returns)) # long only
  )),
  bvec = c(
    1, # sum up to 1
    rep(0, ncol(pool_returns)) # long only
  ),
  meq = 1
)
  
calc_fit <- function(x){
  as.numeric(0.5 * t(x) %*% mat$Dmat %*% x - t(mat$dvec) %*% x)
}
calc_const <- function(x){
  const <- t(mat$Amat) %*% x - mat$bvec
  const[mat$meq] <- -max(0, abs(const[mat$meq]+0.01)-0.01)
  -min(0, const)
}
time_pso <- system.time({
  pso_res <- pso(
    par = rep(0, ncol(pool_returns)),
    fn = function(x){
      x <- as.vector(round(x*nav/prices)*prices/nav)
      fitness <- calc_fit(x)
      constraints <- calc_const(x)
      return(fitness + 10*constraints)
    },
    lower = 0,
    upper = 1,
    control = list(
      s = 200, # swarm size
      c.p = 0.5, # inherit best
      c.g = 0.5, # global best
      maxiter = 100, # iterations
      w0 = 1.2, # starting inertia weight
      wN = 0, # ending inertia weight
      save_traces = F # save more information
    )
  )
})
pso_res$solution <- setNames(as.vector(round(pso_res$solution*nav/prices)*prices/nav), names(res_qp$solution))
res_pso_fit <- as.numeric(0.5 * t(pso_res$solution) %*% mat$Dmat[names(pso_res$solution), names(pso_res$solution)] %*% pso_res$solution - t(as.vector(mat$dvec[names(pso_res$solution),])) %*% pso_res$solution)



# PSO with solve.QP starting position
time_pso_2 <- system.time({
  pso_2_res <- pso(
    par = res_qp$solution,
    fn = function(x){
      x <- as.vector(round(x*nav/prices)*prices/nav)
      fitness <- calc_fit(x)
      constraints <- calc_const(x)
      return(fitness + 10*constraints)
    },
    lower = 0,
    upper = 1,
    control = list(
      s = 200, # swarm size
      c.p = 0.5, # inherit best
      c.g = 0.5, # global best
      maxiter = 100, # iterations
      w0 = 1.2, # starting inertia weight
      wN = 0, # ending inertia weight
      save_traces = F # save more information
    )
  )
})
pso_2_res$solution <- setNames(as.vector(round(pso_2_res$solution*nav/prices)*prices/nav), names(res_qp$solution))
res_pso_2_fit <- as.numeric(0.5 * t(pso_2_res$solution) %*% mat$Dmat[names(pso_2_res$solution), names(pso_2_res$solution)] %*% pso_2_res$solution - t(as.vector(mat$dvec[names(pso_2_res$solution),])) %*% pso_2_res$solution)


reactable(
  data.frame(
  "type" = c("solve.QP discrete", "PSO", "PSO with solve.QP as init solution"),
  "fitness" = c(res_qp_discrete_fit, res_pso_fit, res_pso_2_fit),
  "sum_wgt" = c(res_qp_discrete_sum_wgt, sum(pso_res$solution), sum(pso_2_res$solution)),
  "time" = c(time_qp[3], time_pso[3], time_pso_2[3])
  ),
  columns = list(
    fitness = colDef(format = colFormat(digits=9)),
    sum_wgt = colDef(format = colFormat(digits=3)),
    time = colDef(format = colFormat(digits=3))
  )
) %>% 
  html_save()
```

It can be seen that the rounded `solve.QP()` solution still has a good fitness but the constraints are not satisfied. The PSO has no constrain breaks and still reached a fitness close to the rounded `solve.QP()`. The PSO with `solve.QP()` solution as starting position has beaten both approaches. This indicates that a hybrid approach consisting of both the `solve.QP()` and afterwards the PSO for intelligent rounding with observed constraints would be a good heuristic for problems in practice. 





