% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  oneside]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Asset Allocation using Particle Swarm Optimization in R},
  pdfauthor={Axel Roth},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{animate}
\usepackage{fvextra}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines=true, breakanywhere=true, commandchars=\\\{\}}

\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Asset Allocation using Particle Swarm Optimization in R}
\author{Axel Roth}
\date{2022-10-13}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\renewcommand{\chaptermark}[1]{\markboth{\uppercase{#1}}{\uppercase{#1}}}
\markboth{\uppercase{Preface}}{\uppercase{Preface}}

\textbar\textbar\textbar in progress\textbar\textbar\textbar{}\\
(soll vor dem TOC kommen denke ich)

\renewcommand{\chaptermark}[1]{\markboth{\uppercase{\thechapter. \ #1}}{}}

\hypertarget{abstract}{%
\chapter*{Abstract}\label{abstract}}
\addcontentsline{toc}{chapter}{Abstract}

\textbar\textbar\textbar in progress\textbar\textbar\textbar{}\\
(zusammenfassung: Vor dem TOC)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  motivation
\item
  structure
\item
  results
\end{enumerate}

\hypertarget{software-information-and-usage}{%
\chapter{Software information and usage}\label{software-information-and-usage}}

\textbar\textbar\textbar in progress\textbar\textbar\textbar{}
wie ich das buch schreibe, R markodwn bookdown und so und welche versionen ich nutze

\hypertarget{r-version-and-packages}{%
\section{R-Version and Packages}\label{r-version-and-packages}}

\hypertarget{reproducibility}{%
\section{Reproducibility}\label{reproducibility}}

github und code im bookdown

\hypertarget{r-functions}{%
\section{R-functions}\label{r-functions}}

zb plotly\_save

\hypertarget{open-data-sources}{%
\chapter{Open Data Sources}\label{open-data-sources}}

To increase reproducibility, all data are free and can be loaded from the quantmod R package with the function \texttt{getSymbols()}. It is possible to choose between different data sources like yahoo-finance (default), alpha-vantage, google and others.

\hypertarget{r-functions-1}{%
\section{R-Functions}\label{r-functions-1}}

The following functions were created to increase the ease of data collection with the quantmod R package, which can be found in the \texttt{R/} directory in the attached \href{https://github.com/AxelCode-R/Master-Thesis}{github repository}.

\hypertarget{get_yf}{%
\subsection{\texorpdfstring{\texttt{get\_yf()}}{get\_yf()}}\label{get_yf}}

This function is the main wrapper for collecting data with \texttt{getSymbols()} from yahoo-finance, and converts prices to returns with the \texttt{pri\_to\_ret()} function explained in \ref{pritoret}. The output is a list containing prices and returns as \href{https://cran.r-project.org/web/packages/xts/xts.pdf}{xts} objects. The arguments that can be passed to \texttt{get\_yf()} are:

\vspace{-0.4cm}

\begin{itemize}
\tightlist
\item
  \texttt{tickers}: Vector of symbols (asset names, e.g.~``APPL'', ``GOOG'', \ldots)
\item
  \texttt{from\ ="2018-01-01"}: R-Date
\item
  \texttt{to\ =\ "2019-12-31"}: R-Date
\item
  \texttt{price\_type\ =\ "close"}: Type of prices to be recorded (e.g.~``open'', ``high'', ``low'', ``closed'', ``adjusted'')
\item
  \texttt{return\_type\ =\ "adjusted"}: Type of return to be recorded (e.g.~``open'', ``high'', ``low'', ``closed'', ``adjusted'')
\item
  \texttt{print\ =\ F}: Should the function print the return of \texttt{getSymbols()}
\end{itemize}

\hypertarget{buffer}{%
\subsection{\texorpdfstring{\texttt{buffer()}}{buffer()}}\label{buffer}}

To make data reusable and reduce compilation time, this function stores the data collected with \texttt{get\_yf()}. It receives an R expression, evaluates it and stores it in the \texttt{buffer\_data/} directory under the specified name. If this name already exists, it loads the R object from the RData files without evaluating the expression. The evaluation and overwriting of the existing RData file can be forced with \texttt{force=T}.

\hypertarget{mathfundations}{%
\chapter{Mathematical Fundations}\label{mathfundations}}

This chapter provides an overview of the mathematical calculations and conventions used in this thesis. It is important to note that most mathematical formulas are written in matrix notation. In most cases, this will result in a direct translation to R code. All necessary assumptions required for the modeled return structure are listed in this chapter so that any reader can understand the formulas given. It is important to note that reality is too complex and can only be partially modeled. Simple, basic models are used that do not stand up to reality, but these models or variations of them are commonly used in the financial world and have proven to be helpful. The complexity of solving advanced and basic models does not differ in PSO because the dimension of the objective function is based on the number of elements that can be selected, see chapter \ref{challenges}.

\hypertarget{basic-operators}{%
\section{Basic Operators}\label{basic-operators}}

A compendium comparing commonly used mathematical symbols with R code and their meaning is given in the following table:\\
\includegraphics{Master_Thesis_files/figure-latex/operators-1.png}
To understand the listed operators more deeply, the following examples visualize the resulting dimensions and should provide insights into the use of such operators.

Matrix-Product:
\[\mathbb{R}^{x \times y} \times \mathbb{R}^{y \times z} = \mathbb{R}^{x \times z} \]
and for a vector
\[v \in \mathbb{R}^{N \times 1}: \ \ \mathbb{R}^{1 \times N} \times \mathbb{R}^{N \times 1} = v^T \times v = \sum v_i^2 = \text{scalar}\]
Outer-Product:
\[\mathbb{R}^{x \times 1} \otimes \mathbb{R}^{1 \times x} = \mathbb{R}^{x \times x} \]
Scalar or element-wise Matrix multiplication:
\[R^{x \times y} \cdot R^{x \times y} = \begin{bmatrix}r_{11}^2 &\cdots  & r_{1y}^2 \\ \vdots & \ddots & \vdots  \\ r_{x1}^2 & \cdots & r_{xy}^2 \end{bmatrix}\]

\hypertarget{return-calculation}{%
\section{Return Calculation}\label{return-calculation}}

Any portfolio optimization strategy based on historical data must start with returns. These returns are calculated using \href{https://www.investopedia.com/terms/a/adjusted\%20closingprice.asp}{adjusted closing prices}, which show the percentage change over time. Adjusted closing prices reflect dividends and are adjusted for stock splits and rights offerings. These returns are essential for comparing assets and analyzing dependencies.

\hypertarget{simple-returns}{%
\subsection{Simple Returns}\label{simple-returns}}

The default time frame for all raw data in this thesis is one working day and only simple rates of return are used. Assuming there is an asset with price \(P\) on working day \(t_i\) and the following working day \(t_{i+1}\), it follows that the simple rate of return on \(t_{i+1}\) can be calculated as follows:
\[
  R_{i+1} = \frac{P_{t_{i+1}}}{P_{t_i}}-1
\]

\hypertarget{markowitz-modern-portfolio-theory-mpt}{%
\section{Markowitz Modern Portfolio Theory (MPT)}\label{markowitz-modern-portfolio-theory-mpt}}

In 1952, Harry Markowitz published his first seminal paper, which had a significant impact on modern finance, primarily by outlining the implications of diversification and efficient portfolios. The definition of an efficient portfolio is a portfolio that has either the maximum expected return for a given risk target or the minimum risk for a given expected return target. A simple quote to define diversification might be, ``A portfolio has the same return but less variance than the sum of its parts.'' This is true when assets are not perfectly correlated, as bad and good performance can offset each other, reducing the likelihood of extreme events. For more information, see \citep{Mari2005}.

\hypertarget{assumptions-of-markowitz-portfolio-theory}{%
\subsection{Assumptions of Markowitz Portfolio Theory}\label{assumptions-of-markowitz-portfolio-theory}}

The following list contains all Markowitz assumptions according to \citep{Mari2005}:

\vspace{-0.4cm}

\begin{itemize}
\tightlist
\item
  Perfect market without taxes or transaction costs
\item
  Assets are infinitely divisible
\item
  Expected Returns, Variances and Covariances contain all information
\item
  Investors are risk-averse, they will only accept greater risk if they
  are compensated with a higher expected return
\end{itemize}

The assumption that the returns are normally distributed is not required, but is assumed in this case to simplify the problem. It is obvious that these assumptions are unrealistic in reality. More details on the requirements for using other distributions can be found in \citep{Mari2005}.

\hypertarget{portfolio-math}{%
\section{Portfolio Math}\label{portfolio-math}}

Proofs of the basic calculations required for portfolio optimization, as shown in \citep{Eric2021}, are provided in this section. Returns are presented differently than in most sources, as this is the most common data format used in practice. Suppose there are \(N\) assets described by a return vector \(R\) of random variables and a portfolio weight vector \(w\), respectively:
\[
  R = 
  \begin{bmatrix}
    R_{1} & R_{2} & \cdots & R_{N}  
 \end{bmatrix}
 , \ \ 
 w = 
  \begin{bmatrix}
    w_{1} \\ 
    w_{2} \\
    \vdots \\
    w_{N}  
 \end{bmatrix}
\]

In this thesis, each return is simplified as being normally distributed with \(R_i = \mathbb{N}(\mu_i, \sigma_i^2)\). As a result, linear combinations of normally distributed random variables are jointly normally distributed and have a mean, variance, and covariance that can be used to fully describe them.

\hypertarget{expected-returns}{%
\subsection{Expected Returns}\label{expected-returns}}

The following formula can be used to get the expected returns of a vector with normally distributed random variables \(R \in \mathbb{R}^{1\times N}\):
\begin{align*}
  E[R] &=
  \begin{bmatrix}
    E[R_{1}] & E[R_{2}] & \cdots & E[R_{N}]  
 \end{bmatrix}\\
 &=
 \begin{bmatrix}
    \mu_{1} & \mu_{2} & \cdots & \mu_{N} 
 \end{bmatrix}
 =
 \mu
\end{align*}
and \(\mu_i\) can be estimated in R using historical data and the formula for the geometric mean of returns (also called compound returns). The function to calculate the geometric mean of returns from an xts object can be found in \ref{geomeanret}.

\hypertarget{expected-portfolio-returns}{%
\subsection{Expected Portfolio Returns}\label{expected-portfolio-returns}}

The following equation can be used to obtain the linear combination of expected returns \(\mu\) and a weighting vector \(w\) (e.g.~portfolio weights):
\begin{align*}
 \mu \times w &=
  \begin{bmatrix}
    E[\mu_{1}] & E[\mu_{2}] & \cdots & E[\mu_{N}]
 \end{bmatrix}
  \times 
  \begin{bmatrix}
    w_{1} \\ 
    w_{2} \\
    \cdots \\
    w_{N}  
 \end{bmatrix} \\
 &=
 E[\mu_{1}] \cdot w_1 + E[\mu_{2}] \cdot w_2 + \cdots + E[\mu_{N}] \cdot w_{N} 
 =
 \mu_P
\end{align*}

\hypertarget{covariance}{%
\subsection{Covariance}\label{covariance}}

The general formula of the covariance matrix \(\textstyle\sum\) of a random vector \(R\) with \(N\) normally distributed elements and \(\sigma_{i,j}\) as correlation of two unique values is described as follows:
\begin{align*}
  Cov(R) &= E[(R-\mu)^T \otimes (R-\mu)] \\
  &=   \begin{bmatrix}
    \sigma_1^2 & \sigma_{1,2} & \cdots & \sigma_{1,N} \\
    \sigma_{2, 1} & \sigma_2^2 & \cdots & \sigma_{2, N} \\
    \vdots  & \vdots & \ddots & \vdots \\
    \sigma_{N, 1} & \sigma_{N, 2} & \cdots & \sigma_N^2 \\
 \end{bmatrix}\\
  &=\textstyle\sum
\end{align*}
and can be estimated in R with the basis function \texttt{cov()} and historical data.

\hypertarget{portvar}{%
\subsection{Portfolio Variance}\label{portvar}}

Let \(R\) be a random vector with \(N\) normally distributed elements and \(w\) a weight vector. Assuming that the covariance matrix \(\sum\) of \(R\) is known, the variance of the linear combination of \(R\) can be calculated as follows:
\begin{align*}
  Var(R \times w) &= E[(R \times w - \mu \times w)^2] \\
  &= E[((R - \mu) \times w)^2]
\end{align*}

Since \((R - \mu) \times w\) is a scalar, it can be transformed from \(((R - \mu) \times w)^2\) to \(((R - \mu) \times w)^T \cdot ((R - \mu) \times w)\) and results in:
\begin{align*}
  Var(R \times w) &= E[((R - \mu) \cdot w)^T \times ((R - \mu) \times w)]\\ 
  &= E[(w^T \times (R - \mu)^T) \cdot ((R - \mu) \times w)]\\ 
  &= w^T \times E[(R - \mu)^T \otimes (R - \mu)] \times w \\
  &= w^T \times Cov(R) \times w \\
  &= w^T \times \textstyle\sum \times w
\end{align*}

The same is true for an estimate of \(\textstyle\sum\).

\hypertarget{portfolioreturns}{%
\subsection{Portfolio Returns}\label{portfolioreturns}}

Suppose there are \(N\) assets forming a portfolio with weights \(w\) at time \(t_0\), and the portfolio is to pass through several time steps until \(t_T\) without rebalancing. What are the portfolio returns at each time step \(t_i\)? Clearly, assets with positive performance in the current time step will have a higher weight in the next time step. This can be done by adjusting the weights after each time step depending on the returns. The formula for holding a portfolio with weights \(\textstyle\sum w = 1\) and return matrix \(R \in \mathbb{R}^{T \times N}\), has return \(Z_i-1\) on \(t_i\) with \(i=0, 1, \cdots, T\) for:
\[
  Z_i =
  \begin{cases}
  (1+R_i)\cdot w &\text{ if }i=0\\
  (1+R_i)\cdot \frac{Z_{i-1}}{\sum Z_{i-1}} &\text{ if }i>0
  \end{cases}
\]

This calculation of portfolio returns is implemented in the function \texttt{calc\_portfolio\_returns()} below.

\hypertarget{r-functions-2}{%
\section{R-Functions}\label{r-functions-2}}

\textbar\textbar\textbar in progress\textbar\textbar\textbar{}\\

\hypertarget{pritoret}{%
\subsection{\texorpdfstring{\texttt{pri\_to\_ret()}}{pri\_to\_ret()}}\label{pritoret}}

\textbar\textbar\textbar in progress\textbar\textbar\textbar{}\\
((prices to returns))

\hypertarget{ret_to_cumret}{%
\subsection{\texorpdfstring{\texttt{ret\_to\_cumret()}}{ret\_to\_cumret()}}\label{ret_to_cumret}}

\textbar\textbar\textbar in progress\textbar\textbar\textbar{}\\
((returns to cumulated returns normalized to 100))

\hypertarget{geomeanret}{%
\subsection{\texorpdfstring{\texttt{ret\_to\_geomeanret()}}{ret\_to\_geomeanret()}}\label{geomeanret}}

The geometric mean of returns is a better estimator than the arithmetic mean of returns because it captures the exact mean price changes over a period of time. The variance estimated from the daily returns is a daily variance, so the returns must have the same time base. This can be done by calculating the geometric mean of the returns from multiple daily returns. Assuming there is an asset with returns \(r_1 = 0.01\), \(r_2=-0.03\), and \(r_3=0.02\), it follows that the geometric mean return \(r^{geom}\) can be calculated as:
\[
  r^{geom} = ((1+r_1) \cdot (1+r_2) \cdot (1+r_3))^{1/3}-1 = -0.0002353887
\]
And the advantage is that it is a daily average return that gives exactly the same result as the real return, that is:
\[
  (1+r^{geom})^3 = (1+r_1) \cdot (1+r_2) \cdot (1+r_3)
\]
This is not the case with the arithmetic mean of the returns. The general formula for calculating the geometric mean return of \(n\) days is:
\[
  r^{geom} = (\prod_{i=1}^n (1+r_i))^{\frac{1}{n}}-1
\]
and as R code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ret\_to\_geomeanret }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(xts\_ret)\{}
  \FunctionTok{sapply}\NormalTok{((}\DecValTok{1}\SpecialCharTok{+}\NormalTok{xts\_ret), prod)}\SpecialCharTok{\^{}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(xts\_ret))}\SpecialCharTok{{-}}\DecValTok{1}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{calc_portfolio_returns}{%
\subsection{\texorpdfstring{\texttt{calc\_portfolio\_returns()}}{calc\_portfolio\_returns()}}\label{calc_portfolio_returns}}

This is the implementation of a vectorial calculation of portfolio returns over multiple periods with a weighting vector \texttt{weights} at \(t_0\) and no re-balancing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{calc\_portfolio\_returns }\OtherTok{\textless{}{-}} 
  \ControlFlowTok{function}\NormalTok{(xts\_returns, weights, }\AttributeTok{name=}\StringTok{"portfolio"}\NormalTok{)\{}
  \ControlFlowTok{if}\NormalTok{(}\FunctionTok{sum}\NormalTok{(weights)}\SpecialCharTok{!=}\DecValTok{1}\NormalTok{)\{}
\NormalTok{    xts\_returns}\SpecialCharTok{$}\NormalTok{temp\_\_\_X1 }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{    weights }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(weights, }\DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{sum}\NormalTok{(weights))}
\NormalTok{  \}}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{cumprod}\NormalTok{((}\DecValTok{1}\SpecialCharTok{+}\NormalTok{xts\_returns)) }\SpecialCharTok{*} \FunctionTok{matrix}\NormalTok{(}
    \FunctionTok{rep}\NormalTok{(weights, }\FunctionTok{nrow}\NormalTok{(xts\_returns)), }\AttributeTok{ncol=}\FunctionTok{length}\NormalTok{(weights), }\AttributeTok{byrow=}\NormalTok{T)}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{xts}\NormalTok{(}
    \FunctionTok{rowSums}\NormalTok{(res}\SpecialCharTok{/}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{rowSums}\NormalTok{(res[}\SpecialCharTok{{-}}\FunctionTok{nrow}\NormalTok{(xts\_returns),])))}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }
    \AttributeTok{order.by=}\FunctionTok{index}\NormalTok{(xts\_returns)) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{setNames}\NormalTok{(., name)}
  \FunctionTok{return}\NormalTok{(res)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This function has the same results as the \texttt{Return.portfolio()} function from the \texttt{PortfolioAnalytics} package.

\hypertarget{activ-vs-passiv-investing}{%
\chapter{Activ vs Passiv Investing}\label{activ-vs-passiv-investing}}

\textbar\textbar\textbar in progress\textbar\textbar\textbar{}\\
The fundation of Asset Management

passiv vs activ studie
\texttt{https://www.scirp.org/journal/paperinformation.aspx?paperid=92983}

gut gut
\url{file:///C:/Users/Axel/Desktop/Master-Thesis-All/Ziel\%20was\%20beantwortet\%20werden\%20soll/Quellen\%20nur\%20wichtige/Rasmussen2003_Book_QuantitativePortfolioOptimisat.pdf}

\hypertarget{challenges}{%
\chapter{Challenges of Passiv Investing}\label{challenges}}

In this chapter, we analyze two common challenges of passive investing and create simple use cases to test the PSO. The first challenge is the mean-variance portfolio (MVP) from Markowitz's modern portfolio theory, which, simply put, is an optimal allocation of assets in terms of risk and return. The second challenge is the index tracking problem, which attempts to construct a portfolio with minimal tracking error to a given benchmark.

\hypertarget{mean-variance-portfolio-mvp}{%
\section{Mean-Variance Portfolio (MVP)}\label{mean-variance-portfolio-mvp}}

Markowitz showed that diversifying risk across multiple assets reduces overall portfolio risk. This result was the beginning of the widely used modern portfolio theory, which uses mathematical models to create portfolios with minimal variance for a given return target. All such optimal portfolios for a given return target are called efficient and constitute the efficient frontier. Markowitz's original MVP without constraints can be solved in a closed form, which is explained in \citep{Eric2021}. These types of MVP's have no practical use, so only MVP's with constraints and without closed forms are of interest in this thesis.

\hypertarget{mvp-objective}{%
\subsection{MVP Objective}\label{mvp-objective}}

Let there be \(N\) assets and their returns on \(T\) different days, creating a return matrix \(R \in \mathbb{R}^{T \times N}\). Each element \(R_{t,i}\) contains the return of the \(i\)-th asset on day \(t\). The covariance matrix of the returns is \(\textstyle\sum \in \mathbb{R}^{N \times N}\) and the expected returns are \(\mu \in \mathbb{R}^{N}\). The MVP with the risk aversion parameter \(\lambda \in [0,1]\), as shown in \citep{Mari2005}, can be formalized as follows:
\begin{equation} 
\underset{w}{min} \ \ \ \lambda \ w^T \textstyle\sum w - (1-\lambda) \ \mu^T w
\label{eq:MVP}
\end{equation}

The risk aversion parameter \(\lambda\) defines the tradeoff between risk and return. With \(\lambda = 1\), the minimization problem contains only the variance term, leading to a minimum variance portfolio, and \(\lambda = 0\) transforms the problem into a minimization of negative expected returns, leading to a maximum return portfolio. All possible portfolios created by \(\lambda \in [0, 1]\) define the efficient frontier.

\hypertarget{mvp-example}{%
\subsection{MVP example}\label{mvp-example}}

All possible MVPs together define the efficiency frontier, which is analyzed in this section without going into the details of its calculation. This example uses three assets (stocks: IBM, Google, Apple) and calculates the solution of the MVP for each \(\lambda\). First, the daily returns of these three assets from 2018-01-01 to 2019-12-31 are loaded.

The cumulative daily returns are:\\
\includegraphics{Master_Thesis_files/figure-latex/MVP_ex2-1.png}
The expected daily returns and the covariance matrix for the three assets can be estimated using the formulas from chapter \ref{mathfundations}:

\begin{verbatim}
estimation of expected daily returns:
         AAPL           IBM          GOOG 
 0.0011434115 -0.0001059164  0.0004870292 

estimation of positiv definite covariance matrix:
             AAPL          IBM         GOOG
AAPL 0.0003012226 0.0001177826 0.0001799097
IBM  0.0001177826 0.0002047608 0.0001158735
GOOG 0.0001799097 0.0001158735 0.0002728911
\end{verbatim}

This is all the data necessary to solve the MVP with \(\lambda \in \{0.01, 0.02, ..., 0.99, 1\}\). All 100 portfolios are computed by solving a quadratic minimization problem with the long only (\(w_i \geq 0 \ \forall \ i\)) constraint and the weights should sum to 1.

The resulting daily returns and standard deviations are plotted to create the efficiency frontier:\\
\includegraphics{Master_Thesis_files/figure-latex/MVP_ex5-1.png}
The portfolio compositions for each \(\lambda\) are:\\
\includegraphics{Master_Thesis_files/figure-latex/MVP_ex6-1.png}
It can be observed that the minimum variance portfolio was achieved with a diversified composition of the tree assets. With gradually decreased in \(\lambda\), the portfolio starts to ignore the variance and invest more in the most risky and highest return asset.

\hypertarget{mvp-compare-estimators}{%
\subsection{MVP Compare Estimators}\label{mvp-compare-estimators}}

The above MVP used a geometric mean returns to estimate the expected returns \(\mu\) and used it as well in the estimation of \(\textstyle\sum\). This rises the question if the outcome differs from the classical approach of estimating these parameters with the arithmetic mean returns. The following chart illustrates the efficient frontier for a MVP with arithmetic mean returns as estimator of \(\mu\) versus a MVP with geometric mean returns as estimator of \(\mu\). To compare the results in one chart, all resulting portfolios are evaluated on the historical data with the arithmetic mean returns:

\includegraphics{Master_Thesis_files/figure-latex/unnamed-chunk-4-1.png}
It can be seen, that both estimators produce portfolios on the same efficient frontier. If the aim of the MVP is to generate a portfolio with minimal variance for a given return target, the type of return needs to be specified to use the geometric or arithmetic mean. This specification will determine the type of estimator needed for the MVP. This analogy is not needed in the scope of this thesis, because the more generic portfolios specified with \(\lambda\) are sufficient to create test-cases for the PSO. The later examples with the MVP will always use the geometric mean returns as estimation for the expected returns.

\hypertarget{index-tracking-portfolio-itp}{%
\section{Index-Tracking Portfolio (ITP)}\label{index-tracking-portfolio-itp}}

Indices are baskets of assets that are used to track the performance of a particular asset group. For example, the well-known Standard and Poor's 500 Index (S\&P 500 for short) tracks the 500 largest companies in the United States. Indices are not for sale and serve only to visualize the performance of a particular asset group, without incurring transaction costs. Such indices, or a combination of indices, are used by asset managers as benchmarks to compare the performance of their funds. Each fund has its own benchmark, which contains roughly the same assets that the manager might buy. If the fund underperforms its benchmark, it may indicate that the fund manager has made poor decisions. Therefore, fund managers strive to outperform their benchmarks through carefully selected investments. Past experience has shown that this is rarely achieved with active management by cost \citep{Desm2016}. This has led to the growing popularity of passively managed funds whose goal is to track their benchmarks as closely as possible. This can be achieved through either full or sparse replication. Full replication is a portfolio that contains all the assets in the benchmark with the same weightings. The resulting performance equals the performance of the benchmark when transaction costs are neglected. The first problem is that a benchmark may contain assets that are not liquid or cannot be purchased. The second problem is the weighting scheme of the indices, because they are often weighted by their market capitalization, which changes daily. This would result in the need to reweight daily and increase transaction costs to replicate the performance of the benchmark as closely as possible. To avoid this, sparse replications are used that contain only a fraction of the benchmark's assets. To do so, the portfolio manager must define his benchmark, which should overlap with the investment universe of his fund. He then reduces this universe, taking into account investor constraints and availability, to create a pool of possible assets. For example, a pool that replicates the S\&P 500 might consist of the one hundred highest-weighted assets in the S\&P 500. The ITP can be modeled in two ways analysed in \citep{IuGa2019}.

\hypertarget{itp-with-tev-objective-itp-tev}{%
\subsection{ITP with TEV objective (ITP-TEV)}\label{itp-with-tev-objective-itp-tev}}

The classic and widely used model tries to reduce the variance of the tracking error (TEV) with the following formula:
\[
 \min \ \ Var(TE) = Var(r_{p}-r_{bm})
\]

To obtain the portfolio weights \(w\), one needs to substitute \(r_{p}\) as follows:
\[
  r_{p} = R \times w
\]

The variance is then solved until a quadratic problem is presented as a function of portfolio weights \(w\):
\begin{align*}
 Var(r_{p}-r_{bm}) &= Var(R \times w - r_{bm}) \\
 &= Var(R \times w) + Var(r_{bm}) - 2 \cdot Cov(R \times w,r_{bm}) 
\end{align*}

Now the three terms can be solved, starting with the simplest one.
\[
Var(r_{bm}) = \sigma_{bm}^2 = constant
\]

The variance of the portfolio can be solved with \ref{portvar}:
\[
Var(R \times w) = w^T \times Cov(R) \times w
\]
And the last term can be solved in the same way as in \citep{Eric2021}:
\begin{align*}
  Cov(A \times a, b) &= Cov(b, A \times a) \\
  &= E[(b-\mu_{b})(A \times a-\mu_{A} \times a)] \\
  &= E[(b-\mu_{b})(A-\mu_{A}) \times a] \\
  &= E[(b-\mu_{b})(A-\mu_{A})] \times a \\
  &= Cov(A,b) \times a
\end{align*}

This results in the final formula of the ITP:
\begin{align*}
  Var(r_{p}-r_{bm}) & = Var(R \times w - r_{bm}) \\
  & = Var(R \times w) - 2 \cdot Cov(R \times w,r_{bm}) + Var(r_{bm})  \\
  & = w^T \times Cov(R) \times w - 2 \cdot Cov(r_{bm}, R)^T \times w + \sigma_{bm}^2
  \label{eq:ITP}
\end{align*}

The minimization problem of the ITP in the general structure required by many optimizers is:
\[
  \min\limits_{w} \ \ \frac{1}{2} \cdot w^T \times D \times w -d^T \times w
\]

Minimization problems can ignore constant terms and global stretch coefficients and still find the same minimum. This leads to a general substitution of the ITP with TEV objective as follows:
\[
  D = Cov(R)
\]
and
\[
d = Cov(r_{bm}, R)
\]

It is possible to add some basic constraints, as in the MVP to sum the weights to 1 and be long only. Despite the fact that this model is often used, it has a big disadvantage in that it cannot detect constant deviations in the returns. For this reason, the following model exists, which focuses on the mean square tracking error of returns (MSTE).

\hypertarget{itp-with-mste-objective-itp-mste}{%
\subsection{ITP with MSTE objective (ITP-MSTE)}\label{itp-with-mste-objective-itp-mste}}

A good explanation of the ITP with MSTE objective can be found in \href{https://ahmedbadary.github.io/work_files/research/conv_opt/hw/iftp}{ahmedbadary}. The objective is to minimize the mean square tracking error (MSTE) of daily portfolio returns \(r_{t, p}\) and daily benchmark returns \(r_{t, bm}\) on \(T\) days:

\[
  \frac{1}{T} \sum^T_{t=1}(r_{t, p}-r_{t, bm})^2
\]
The formula can be rewritten as vector norm:
\[
  \frac{1}{T} \left\Vert r_{p}-r_{bm} \right\Vert_2^2
\]
Which results in the following minimization with neglected stretching factor:
\[
 min \ \  \left\Vert r_{p}-r_{bm} \right\Vert_2^2
\]
The portfolio returns \(r_p\) needs to be substituted to contain the portfolio weights \(w\) like in the TEV objective above. This results in the below transformation of the problem:
\begin{align*}
  \left\Vert r_{p}-r_{bm} \right\Vert_2^2 &= \left\Vert R \times w-r_{bm} \right\Vert_2^2 \\ 
  &= (R \times w-r_{bm})^T \times (R \times w-r_{bm}) \\ 
  &= (w^T \times R^T-r_{bm}^T) \times (R \times w-r_{bm}) \\ 
  &= w^T \times R^T \times R \times w - w^T \times A^T \times r_{bm} - r_{bm}^T \times R \times w + r_{bm}^T \times r_{bm} 
\end{align*}

The minimization and the fact that the scalars \(w^T \times R^T \times r_{bm}\) and \(r_{bm}^T \times R \times w\) are equal, transforms the problem to:
\[
  \min\limits_{w} \ \  \left\Vert r_{p}-r_{bm} \right\Vert_2^2 
  = w^T \times R^T \times R \times w - 2\cdot r_{bm}^T \times R \times w
\]
This leads to the equivalent general representation of the ITP with MSTE objective as follows:
\[
  \min\limits_{w} \ \ \frac{1}{2} \cdot w^T \times D \times w - d^T \times w
\]
with
\[
D = R^T \times R
\]
and
\[
  d = R^T \times r_{bm}
\]

\hypertarget{example-itp}{%
\subsection{Example ITP}\label{example-itp}}

This example shows the results of tracking the S\&P 500 with a tracking portfolio that can only invest in IBM, Apple and Google. The time frame ranges from 2018-01-01 till 2019-12-31 and the goal is to minimize the difference in returns between the portfolio and the benchmark. The fitted return changes of the ITP-TEV and ITP-MSTE are:

\includegraphics{Master_Thesis_files/figure-latex/ITP_ex-1.png}
The ITP-TEV and ITP-MSTE had almost the same results whith the compositions below:

\begin{verbatim}
      type      AAPL       IBM      GOOG
1  ITP-TEV 0.2594763 0.4164918 0.3240319
2 ITP-MSTE 0.2588587 0.4170364 0.3241049
\end{verbatim}

\hypertarget{analytic-solver-for-quadratic-programming-problems}{%
\chapter{Analytic Solver for Quadratic Programming Problems}\label{analytic-solver-for-quadratic-programming-problems}}

The advantages and disadvantages of analytical solvers for quadratic programming problems are discussed in this chapter. It is beyond the scope of this thesis to explain the underlying mathematical principles of how a solver solves quadratic problems; only the applications and analysis are discussed. The main reason for dealing with analytic solvers for quadratic programming problems is to use them as a benchmark for PSO.

\hypertarget{quadratic-programming-qp}{%
\section{Quadratic Programming (QP)}\label{quadratic-programming-qp}}

A quadratic program is a minimization problem of a function that returns a scalar value and consists of a quadratic term and a linear term that depend on the variable of interest. In addition, the problem may be constrained by several linear inequalities that bound the solution. The general formulation used is to find \(x\) that minimizes the following problem:
\[
  \min\limits_{x} \ \frac{1}{2} \cdot x^T \times D \times x - d^T \times x 
\]
and is valid under the linear constraints:
\[
  A^T \times x \geq b_0
\]

Some other sources notate the problem with different signs or coefficients, all of which are interchangeable with the above problem. In addition, the above problem has the same notation used in the R package \texttt{quadprog}, which reduces the substitution overhead. All modern programming languages have many solvers for quadratic problems. They differ mainly in the computation time for certain problems and the requirements. Some commercial QP solvers additionally accept more complex constraints, such as absolute (e.g., \(|A^T \times x| \geq a_0\)) or mixed-integer (e.g., \(x \in \mathbb{N}\)). Especially the mixed-integer constraint problems lead to a huge increase in memory requirements.

\hypertarget{qp-solver-from-quadprog}{%
\section{QP-Solver from quadprog}\label{qp-solver-from-quadprog}}

The most common free QP-Solver used in R comes from the package \href{https://cran.r-project.org/web/packages/quadprog/quadprog.pdf}{quadprog}, which consists of a single function called \texttt{solve.QP()}. Its implementation routine is the dual method of Goldfarb and Idnani published in \citep{GoId1982} and \citep{GoId1983}. It uses the above QP with the condition that \(D\) must be a symmetric positive definite matrix. This means that \(D\in \mathbb{R}^{N \times N}\) and \(x^T D x > 0 \ \forall \ x \in \mathbb{R}^N\), which is equivalent to all eigenvalues being greater than zero. In most cases this is not achieved by estimating the covariance matrix \(\sum\), but it is possible to find the nearest positive definite matrix of \(\textstyle\sum\) using the function \texttt{nearPD()} from the \href{https://cran.r-project.org/web/packages/Matrix/Matrix.pdf}{matrix} R package. The error encountered often does not exceed a percentage change in elements over \(10^{-15} \%\), which is negligible for the context of this work. The function \texttt{solve.QP()} for an \(N\) dimensional vector of interest, has the following arguments, which are also found in the above formulation of a QP:

\vspace{-0.4cm}

\begin{itemize}
\tightlist
\item
  \texttt{Dmat}: Symmetric positive definite matrix \(D \in \mathbb{R}^{N \times N}\) of the quadratic term
\item
  \texttt{dvec}: Vector \(d \in \mathbb{R}^{N}\) of the linear term
\item
  \texttt{Amat}: Constraint matrix \(A\)
\item
  \texttt{bvec}: Constraint vector \(b_0\)
\item
  \texttt{meq\ =\ 1}: means that the first row of \(A\) is treated as an equality constraint
\end{itemize}

The return of \texttt{solve.QP()} is a list and contains, among others, the following attributes of interest:
+ \texttt{solution}: Vector containing the solution \(x\) of the quadratic programming problem (e.g.~portfolio weights)
+ \texttt{value}: Scalar, the value of the quadratic function at the solution

\hypertarget{exampleanalyticalmvp}{%
\section{\texorpdfstring{Example: Solving MVP with \texttt{solve.QP()}}{Example: Solving MVP with solve.QP()}}\label{exampleanalyticalmvp}}

This section provides insights into the effects of diversification and the use of \texttt{solve.QP()} by creating ten different efficiency frontiers from a pool of ten assets. Each efficiency frontier \(i \in \{1, 2, \cdots, 10\}\) consists of \(N_i = i\) assets and is created by adding the asset with the next smallest variance first. After loading the returns for ten of the largest stocks in the U.S. market, the variance is calculated to rank all columns in ascending order of variance, as shown in the code below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{returns\_raw }\OtherTok{\textless{}{-}} \FunctionTok{buffer}\NormalTok{(}
  \FunctionTok{get\_yf}\NormalTok{(}
    \AttributeTok{tickers =} \FunctionTok{c}\NormalTok{(}\StringTok{"IBM"}\NormalTok{, }\StringTok{"GOOG"}\NormalTok{, }\StringTok{"AAPL"}\NormalTok{, }\StringTok{"MSFT"}\NormalTok{, }\StringTok{"AMZN"}\NormalTok{, }
                \StringTok{"NVDA"}\NormalTok{, }\StringTok{"JPM"}\NormalTok{, }\StringTok{"META"}\NormalTok{, }\StringTok{"V"}\NormalTok{, }\StringTok{"WMT"}\NormalTok{), }
    \AttributeTok{from =} \StringTok{"2018{-}01{-}01"}\NormalTok{, }
    \AttributeTok{to =} \StringTok{"2019{-}12{-}31"}
\NormalTok{  )}\SpecialCharTok{$}\NormalTok{returns, }
  \StringTok{"AS\_10\_assets"}
\NormalTok{)}

\CommentTok{\# re{-}arrange: low var first}
\NormalTok{vars }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(returns\_raw, var)}
\NormalTok{returns\_raw }\OtherTok{\textless{}{-}}\NormalTok{ returns\_raw[, }\FunctionTok{order}\NormalTok{(vars, }\AttributeTok{decreasing =}\NormalTok{ F)]}
\end{Highlighting}
\end{Shaded}

The next step is to create a function \texttt{mvp()} that has the arguments \texttt{return} and \texttt{lambda}. It computes the expected returns \texttt{mu} and the estimated positive definite covariance \texttt{cov}. It then solves an MVP with constraints \(\textstyle\sum w_i = 1\) and \(w_i \geq 0\), which yields the key features \texttt{mu}, \texttt{var} and \texttt{composition} of the portfolio.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mvp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(returns, lambda)\{}
\NormalTok{  tc }\OtherTok{\textless{}{-}} \FunctionTok{tryCatch}\NormalTok{(\{}
\NormalTok{    mu }\OtherTok{\textless{}{-}} \FunctionTok{ret\_to\_geomeanret}\NormalTok{(returns)}

\NormalTok{    cov }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{nearPD}\NormalTok{(}\FunctionTok{cov\_}\NormalTok{(returns, mu))}\SpecialCharTok{$}\NormalTok{mat)}

\NormalTok{    mat }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
      \AttributeTok{Dmat =}\NormalTok{ lambda }\SpecialCharTok{*}\NormalTok{ cov,}
      \AttributeTok{dvec =}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{lambda) }\SpecialCharTok{*}\NormalTok{ mu,}
      \AttributeTok{Amat =} \FunctionTok{t}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(}
        \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(returns)), }\CommentTok{\# sum up to 1}
        \FunctionTok{diag}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{nrow=}\FunctionTok{ncol}\NormalTok{(returns), }\AttributeTok{ncol=}\FunctionTok{ncol}\NormalTok{(returns)) }\CommentTok{\# long only}
\NormalTok{      )),}
      \AttributeTok{bvec =} \FunctionTok{c}\NormalTok{(}
        \DecValTok{1}\NormalTok{, }\CommentTok{\# sum up to 1}
        \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(returns)) }\CommentTok{\# long only}
\NormalTok{      ),}
      \AttributeTok{meq =} \DecValTok{1}
\NormalTok{    )}
  
\NormalTok{    qp }\OtherTok{\textless{}{-}} \FunctionTok{solve.QP}\NormalTok{(}
      \AttributeTok{Dmat =}\NormalTok{ mat}\SpecialCharTok{$}\NormalTok{Dmat, }\AttributeTok{dvec =}\NormalTok{ mat}\SpecialCharTok{$}\NormalTok{dvec, }
      \AttributeTok{Amat =}\NormalTok{ mat}\SpecialCharTok{$}\NormalTok{Amat, }\AttributeTok{bvec =}\NormalTok{ mat}\SpecialCharTok{$}\NormalTok{bvec, }\AttributeTok{meq =}\NormalTok{ mat}\SpecialCharTok{$}\NormalTok{meq}
\NormalTok{    )}
    
\NormalTok{    res }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
      \StringTok{"mu"} \OtherTok{=}\NormalTok{ mu }\SpecialCharTok{\%*\%}\NormalTok{ qp}\SpecialCharTok{$}\NormalTok{solution,}
      \StringTok{"var"} \OtherTok{=} \FunctionTok{t}\NormalTok{(qp}\SpecialCharTok{$}\NormalTok{solution) }\SpecialCharTok{\%*\%}\NormalTok{ cov }\SpecialCharTok{\%*\%}\NormalTok{ qp}\SpecialCharTok{$}\NormalTok{solution,}
      \StringTok{"composition"} \OtherTok{=} \FunctionTok{setNames}\NormalTok{(qp}\SpecialCharTok{$}\NormalTok{solution, }\FunctionTok{colnames}\NormalTok{(returns))}
\NormalTok{    )}
    \ConstantTok{TRUE}
\NormalTok{  \}, }\AttributeTok{error =} \ControlFlowTok{function}\NormalTok{(e)\{}\ConstantTok{FALSE}\NormalTok{\})}
  

  \ControlFlowTok{if}\NormalTok{(tc)\{}
    \FunctionTok{return}\NormalTok{(res)}
\NormalTok{  \}}\ControlFlowTok{else}\NormalTok{\{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
      \StringTok{"mu"} \OtherTok{=} \ConstantTok{NA}\NormalTok{,}
      \StringTok{"var"} \OtherTok{=} \ConstantTok{NA}\NormalTok{,}
      \StringTok{"composition"} \OtherTok{=} \ConstantTok{NA}
\NormalTok{    ))}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Each \(\lambda \in \{0.01, 0.02, \cdots, 1\}\) and each combination of ascending number of assets results in a portfolio that can be created with two for loops.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \StringTok{"index"}\OtherTok{=}\DecValTok{1}\NormalTok{, }
  \StringTok{"var"}\OtherTok{=}\FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{var}\NormalTok{(returns\_raw[, }\DecValTok{1}\NormalTok{])), }
  \StringTok{"return"} \OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{ret\_to\_geomeanret}\NormalTok{(returns\_raw[, }\DecValTok{1}\NormalTok{])), }
  \AttributeTok{row.names=}\ConstantTok{NULL}
\NormalTok{)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(returns\_raw))\{}
\NormalTok{  returns }\OtherTok{\textless{}{-}}\NormalTok{ returns\_raw[, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{i]}
  \ControlFlowTok{for}\NormalTok{(lambda }\ControlFlowTok{in} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{))\{}
\NormalTok{    res }\OtherTok{\textless{}{-}} \FunctionTok{mvp}\NormalTok{(returns, lambda)}
    
\NormalTok{    df }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(}
\NormalTok{      df, }
      \FunctionTok{data.frame}\NormalTok{(}\StringTok{"index"}\OtherTok{=}\NormalTok{i, }\StringTok{"var"}\OtherTok{=}\NormalTok{res}\SpecialCharTok{$}\NormalTok{var, }\StringTok{"return"} \OtherTok{=}\NormalTok{ res}\SpecialCharTok{$}\NormalTok{mu)}
\NormalTok{    )}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The result is filtered and names are added to represent the number of assets. Now the diagram can be created:

\includegraphics{Master_Thesis_files/figure-latex/qp_mvp4-1.png}

It can be seen, that each asset added results in a minimum variance portfolio with smaller or equal standard deviation. Nevertheless, we started with the asset that has the smallest standard deviation of 0.012459. This is the effect of diversification mentioned by Markowitz.

\hypertarget{exampleitpsolveqp}{%
\section{\texorpdfstring{Example: Solving ITP-MSTE with \texttt{solve.QP()}}{Example: Solving ITP-MSTE with solve.QP()}}\label{exampleitpsolveqp}}

This example analyzes how many assets are needed to minimize the mean square error between the replication and historical returns of the S\&P 500 from 2018-01-01 to 2019-12-31. The constraints are set to be long only and the weights should sum to one. To gradually reduce the number of assets, the five assets with the lowest weights are discarded and serve as the new asset pool for the next replication until only five assets are left. First, the required data can be downloaded from the \texttt{R/} directory using existing functions. The function \texttt{get\_spx\_composition()} uses web scraping to read the components of \href{https://en.wikipedia.org/wiki/List_of_S\%26P_500_companies}{wikipedia} and converts them into monthly compositions of the S\&P 500. The pool is formed from all assets present in the last month of the time frame, reduced by assets with missing values. The code below loads the returns of all assets in the pool and the S\&P 500:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{from }\OtherTok{\textless{}{-}} \StringTok{"2018{-}01{-}01"}
\NormalTok{to }\OtherTok{\textless{}{-}} \StringTok{"2019{-}12{-}31"}

\NormalTok{spx\_composition }\OtherTok{\textless{}{-}} \FunctionTok{buffer}\NormalTok{(}
  \FunctionTok{get\_spx\_composition}\NormalTok{(),}
  \StringTok{"AS\_spx\_composition"}
\NormalTok{)}


\NormalTok{pool\_returns\_raw }\OtherTok{\textless{}{-}} \FunctionTok{buffer}\NormalTok{(}
  \FunctionTok{get\_yf}\NormalTok{(}
    \AttributeTok{tickers =}\NormalTok{ spx\_composition }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{filter}\NormalTok{(Date}\SpecialCharTok{\textless{}=}\NormalTok{to) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{filter}\NormalTok{(Date}\SpecialCharTok{==}\FunctionTok{max}\NormalTok{(Date)) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{pull}\NormalTok{(Ticker), }
    \AttributeTok{from =}\NormalTok{ from, }
    \AttributeTok{to =}\NormalTok{ to}
\NormalTok{  )}\SpecialCharTok{$}\NormalTok{returns, }
  \StringTok{"AS\_sp500\_assets"}
\NormalTok{)}
\NormalTok{pool\_returns\_raw }\OtherTok{\textless{}{-}} 
\NormalTok{  pool\_returns\_raw[, }\FunctionTok{colSums}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(pool\_returns\_raw))}\SpecialCharTok{==}\DecValTok{0}\NormalTok{]}


\NormalTok{bm\_returns }\OtherTok{\textless{}{-}} \FunctionTok{buffer}\NormalTok{(}
  \FunctionTok{get\_yf}\NormalTok{(}\AttributeTok{tickers =} \StringTok{"\%5EGSPC"}\NormalTok{, }\AttributeTok{from =}\NormalTok{ from, }\AttributeTok{to =}\NormalTok{ to)}\SpecialCharTok{$}\NormalTok{returns, }
  \StringTok{"AS\_sp500"}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{setNames}\NormalTok{(., }\StringTok{"S\&P 500"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The required data is now available and the function for the ITP-MSTE can be created. It requires \texttt{pool\_returns} with variable number of columns and the single-column matrix \texttt{bm\_returns}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{itp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pool\_returns, bm\_returns)\{}
\NormalTok{  mat }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{Dmat =} \FunctionTok{t}\NormalTok{(pool\_returns) }\SpecialCharTok{\%*\%}\NormalTok{ pool\_returns,}
    \AttributeTok{dvec =} \FunctionTok{t}\NormalTok{(pool\_returns) }\SpecialCharTok{\%*\%}\NormalTok{ bm\_returns,}
    \AttributeTok{Amat =} \FunctionTok{t}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(}
      \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(pool\_returns)), }\CommentTok{\# sum up to 1}
      \FunctionTok{diag}\NormalTok{(}\DecValTok{1}\NormalTok{, }
           \AttributeTok{nrow=}\FunctionTok{ncol}\NormalTok{(pool\_returns), }
           \AttributeTok{ncol=}\FunctionTok{ncol}\NormalTok{(pool\_returns)) }\CommentTok{\# long only}
\NormalTok{    )),}
    \AttributeTok{bvec =} \FunctionTok{c}\NormalTok{(}
      \DecValTok{1}\NormalTok{, }\CommentTok{\# sum up to 1}
      \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(pool\_returns)) }\CommentTok{\# long only}
\NormalTok{    ),}
    \AttributeTok{meq =} \DecValTok{1}
\NormalTok{  )}
  
\NormalTok{  qp }\OtherTok{\textless{}{-}} \FunctionTok{solve.QP}\NormalTok{(}
    \AttributeTok{Dmat =}\NormalTok{ mat}\SpecialCharTok{$}\NormalTok{Dmat, }\AttributeTok{dvec =}\NormalTok{ mat}\SpecialCharTok{$}\NormalTok{dvec, }
    \AttributeTok{Amat =}\NormalTok{ mat}\SpecialCharTok{$}\NormalTok{Amat, }\AttributeTok{bvec =}\NormalTok{ mat}\SpecialCharTok{$}\NormalTok{bvec, }\AttributeTok{meq =}\NormalTok{ mat}\SpecialCharTok{$}\NormalTok{meq}
\NormalTok{  )}

\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \StringTok{"var"} \OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(}
      \FunctionTok{var}\NormalTok{(pool\_returns }\SpecialCharTok{\%*\%}\NormalTok{ qp}\SpecialCharTok{$}\NormalTok{solution }\SpecialCharTok{{-}}\NormalTok{ bm\_returns)),}
    \StringTok{"solution"} \OtherTok{=} \FunctionTok{setNames}\NormalTok{(qp}\SpecialCharTok{$}\NormalTok{solution, }\FunctionTok{colnames}\NormalTok{(pool\_returns))}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The duplication and successive discarding of assets can begin. The results are stored in \texttt{res} and used to display the results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{n\_assets }\OtherTok{\textless{}{-}} \FunctionTok{rev}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{5}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(pool\_returns\_raw), }\DecValTok{5}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in}\NormalTok{ n\_assets)\{}
\NormalTok{  temp }\OtherTok{\textless{}{-}} \ControlFlowTok{if}\NormalTok{(i}\SpecialCharTok{==}\FunctionTok{max}\NormalTok{(n\_assets))\{}
    \FunctionTok{itp}\NormalTok{(pool\_returns\_raw, bm\_returns)}
\NormalTok{  \}}\ControlFlowTok{else}\NormalTok{\{}
    \FunctionTok{itp}\NormalTok{(}
\NormalTok{      pool\_returns\_raw[, }\FunctionTok{names}\NormalTok{(}\FunctionTok{sort}\NormalTok{(temp}\SpecialCharTok{$}\NormalTok{solution, }\AttributeTok{decreasing =}\NormalTok{ T)[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{i])], }
\NormalTok{      bm\_returns}
\NormalTok{    )}
\NormalTok{  \}}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(}
\NormalTok{    res, }
    \FunctionTok{data.frame}\NormalTok{(}\StringTok{"N"}\OtherTok{=}\NormalTok{i, }\StringTok{"var"}\OtherTok{=}\NormalTok{temp}\SpecialCharTok{$}\NormalTok{var, }\StringTok{"sd"}\OtherTok{=}\FunctionTok{sqrt}\NormalTok{(temp}\SpecialCharTok{$}\NormalTok{var), }\AttributeTok{row.names =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\includegraphics{Master_Thesis_files/figure-latex/qp_itp4-1.png}

It can be seen that the standard deviation stagnates at about \(N=100\). This leads to the conclusion that a sparse replication with one hundred assets is sufficient in this particular case to track the historical performance of the S\&P 500 over this period.

\hypertarget{particle-swarm-optimization-pso}{%
\chapter{Particle Swarm Optimization (PSO)}\label{particle-swarm-optimization-pso}}

The PSO was developed by J. Kennedy as a global optimization method based on swarm intelligence and presented to the public in 1995 by Eberhart and Kennedy \citep{KeEb1995}. The original PSO was intended to resemble a flock of birds flying through the sky without collisions. Therefore, its first applications were found in particle physics to analyze moving particles in high-dimensional spaces, which the name Particle recalls. Later, it was adapted in Evolutionary Computation to exploit a set of potential solutions in high dimensions and to find the optima by cooperating with other particles in the swarm \citep{PaVr2002}. Since it does not require gradient information, it is easier to apply than other global optimization methods. It can find the optimum by considering only the result of the function to be optimized. This means that the function can be arbitrarily complex and it is still possible to reach the global optimum. Other advantages are the low computational costs, since only basic mathematical operators are used.

\hypertarget{the-algorithm}{%
\section{The Algorithm}\label{the-algorithm}}

Each particle \(d\) with position \(x_d\) moves in the search space \(\mathbb{R}^N\) and has its own velocity \(v_d\) and remembers its previous best position \(P_d\). After each iteration, the velocity changes in the direction of the intrinsic velocity, the best previous position, and the global best position \(p_g\) of all particles. A position change from \(i\) to \(i+1\) can be calculated by the following two equations \citep{PaVr2002}:
\begin{align*}
  v_d^{i+1} &= wv_d^{i} + c_p r_1^i (P_d^i - x_d^i) + c_g r_2^i (p_g^i - x_d^i) \\
  x_d^{i+1} &= x_d^i + v_d^{i+1}
\end{align*}

Where \(r_1\) and \(r_2\) are uniformly distributed random numbers in {[}0, 1{]}. The cognitive parameter \(c_p\) acts as a weighting of the direction to its previous best position of the particle. This contrasts with the social parameter \(c_g\), which is a weighting of the direction to the global best position. The inertial weight \(w\) is crucial for the convergence behavior by remembering part of its previous trajectory. A study reviewed in \citep{PaVr2002} showed that these parameters can be set to \(c_p=c_g=0.5\) and \(w\) should decrease from \(1.2\) to \(0\). However, some problems benefit from a more precise tuning of these parameters. To allow effortless translation to code, the above formula for \(d = 1, 2, \cdots, D\) particles can be given in the following matrix notation:
\begin{align*}
  V^{i+1} &= w \cdot V^{i} + c_p \cdot r_1^i \cdot (P^i-X^i) + c_g \cdot r_2^i \cdot (p_g^i - X^i) \\
  X^{i+1} &= X^i + V^{i+1}
\end{align*}

With current positions \(X \in \mathbb{R}^{N \times D}\), current velocities \(V \in \mathbb{R}^{N \times D}\), previous best positions \(P \in \mathbb{R}^{N \times D}\), and global best position \(p_g \in \mathbb{R}^{N}\). The parameters \(w\), \(c_p\) and \(c_g\) are stile scalars. The random numbers are transformed into vectors \(r_1\) and \(r_2\), which contain uniformly distributed random numbers that are multiplied element-wise.

\hypertarget{pso-function}{%
\section{\texorpdfstring{\texttt{pso()} Function}{pso() Function}}\label{pso-function}}

In this section, a general PSO function is created that follows the structure of other optimization heuristics in R, in particular the existing PSO implementation from the R package \texttt{pso}. The key component of the problem is a objective function called \texttt{fn()}, which returns a scalar that needs to be minimized. The function itself mainly needs a vector \texttt{pos} that describes the position of a particle (e.g.~weights). The other main parameters for the PSO function are \texttt{par}, which is a position of a particle used to derive the dimension of the problem and used as the initial position of one particle. The argument can only contain \texttt{NA}'s, resulting in completely random starting positions. The last two arguments are \texttt{lower} and \texttt{upper} bounds (e.g.~weights greater than 0 and less than 1). All other parameters have default values that can be overridden by passing a list called \texttt{control}. The resulting structure is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pso }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}
\NormalTok{    par, }
\NormalTok{    fn, }
\NormalTok{    lower, }
\NormalTok{    upper, }
    \AttributeTok{control =} \FunctionTok{list}\NormalTok{()}
\NormalTok{  )\{}

\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Before the main data structure can be initialized, some sample inputs must be created for the \texttt{pso()} function as described below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{par }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x)\{}\FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(x)))\}}
\NormalTok{lower }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{10}
\NormalTok{upper }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{control }\OtherTok{=} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{s =} \DecValTok{10}\NormalTok{, }\CommentTok{\# swarm size}
  \AttributeTok{c.p =} \FloatTok{0.5}\NormalTok{, }\CommentTok{\# inherit best}
  \AttributeTok{c.g =} \FloatTok{0.5}\NormalTok{, }\CommentTok{\# global best}
  \AttributeTok{maxiter =} \DecValTok{100}\NormalTok{, }\CommentTok{\# iterations}
  \AttributeTok{w0 =} \FloatTok{1.2}\NormalTok{, }\CommentTok{\# starting inertia weight}
  \AttributeTok{wN =} \DecValTok{0}\NormalTok{, }\CommentTok{\# ending inertia weight}
  \AttributeTok{save\_traces =}\NormalTok{ F }\CommentTok{\# save more information}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now it is time to initialize the random positions \texttt{X}, their fitness \texttt{X\_fit} and their random velocities \texttt{V} with the function \texttt{mrunif()} which produces a matrix of uniformly distributed random numbers between \texttt{lower} and \texttt{upper}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{mrunif}\NormalTok{(}
  \AttributeTok{nr =} \FunctionTok{length}\NormalTok{(par), }\AttributeTok{nc=}\NormalTok{control}\SpecialCharTok{$}\NormalTok{s, }\AttributeTok{lower=}\NormalTok{lower, }\AttributeTok{upper=}\NormalTok{upper}
\NormalTok{)}
\ControlFlowTok{if}\NormalTok{(}\FunctionTok{all}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(par)))\{}
\NormalTok{  X[, }\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ par}
\NormalTok{\}}
\NormalTok{X\_fit }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(X, }\DecValTok{2}\NormalTok{, fn)}
\NormalTok{V }\OtherTok{\textless{}{-}} \FunctionTok{mrunif}\NormalTok{(}
  \AttributeTok{nr =} \FunctionTok{length}\NormalTok{(par), }\AttributeTok{nc=}\NormalTok{control}\SpecialCharTok{$}\NormalTok{s, }
  \AttributeTok{lower=}\SpecialCharTok{{-}}\NormalTok{(upper}\SpecialCharTok{{-}}\NormalTok{lower), }\AttributeTok{upper=}\NormalTok{(upper}\SpecialCharTok{{-}}\NormalTok{lower)}
\NormalTok{)}\SpecialCharTok{/}\DecValTok{10}
\end{Highlighting}
\end{Shaded}

The velocities are compressed by a factor of 10 to start with a maximum movement of one tenth of the space in each axis. The personal best positions \texttt{P} are the same as \texttt{X} and the global best position is the position with the smallest fitness:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{P }\OtherTok{\textless{}{-}}\NormalTok{ X}
\NormalTok{P\_fit }\OtherTok{\textless{}{-}}\NormalTok{ X\_fit}
\NormalTok{p\_g }\OtherTok{\textless{}{-}}\NormalTok{ P[, }\FunctionTok{which.min}\NormalTok{(P\_fit)]}
\NormalTok{p\_g\_fit }\OtherTok{\textless{}{-}} \FunctionTok{min}\NormalTok{(P\_fit)}
\end{Highlighting}
\end{Shaded}

The required data structure is available and the optimization can start with the calculation of the new velocities and the transformation of the old positions. When particles have left the valid space, they are pushed back to the edge and the velocities are set to zero. Then the fitness is calculated and the personal best and global best positions are saved if they have improved.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trace\_data }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{control}\SpecialCharTok{$}\NormalTok{maxiter)\{}
  \CommentTok{\# move particles}
\NormalTok{  V }\OtherTok{\textless{}{-}} 
\NormalTok{    (control}\SpecialCharTok{$}\NormalTok{w0}\SpecialCharTok{{-}}\NormalTok{(control}\SpecialCharTok{$}\NormalTok{w0}\SpecialCharTok{{-}}\NormalTok{control}\SpecialCharTok{$}\NormalTok{wN)}\SpecialCharTok{*}\NormalTok{i}\SpecialCharTok{/}\NormalTok{control}\SpecialCharTok{$}\NormalTok{maxiter) }\SpecialCharTok{*}\NormalTok{ V }\SpecialCharTok{+} 
\NormalTok{    control}\SpecialCharTok{$}\NormalTok{c.p }\SpecialCharTok{*} \FunctionTok{runif}\NormalTok{(}\FunctionTok{length}\NormalTok{(par)) }\SpecialCharTok{*}\NormalTok{ (P}\SpecialCharTok{{-}}\NormalTok{X) }\SpecialCharTok{+} 
\NormalTok{    control}\SpecialCharTok{$}\NormalTok{c.g }\SpecialCharTok{*} \FunctionTok{runif}\NormalTok{(}\FunctionTok{length}\NormalTok{(par)) }\SpecialCharTok{*}\NormalTok{ (p\_g}\SpecialCharTok{{-}}\NormalTok{X)}
\NormalTok{  X }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{+}\NormalTok{ V}
  
  \CommentTok{\# set velocity to zeros if not in valid space}
\NormalTok{  V[X }\SpecialCharTok{\textgreater{}}\NormalTok{ upper] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  V[X }\SpecialCharTok{\textless{}}\NormalTok{ lower] }\OtherTok{\textless{}{-}} \DecValTok{0}
  
  \CommentTok{\# move into valid space}
\NormalTok{  X[X }\SpecialCharTok{\textgreater{}}\NormalTok{ upper] }\OtherTok{\textless{}{-}}\NormalTok{ upper}
\NormalTok{  X[X }\SpecialCharTok{\textless{}}\NormalTok{ lower] }\OtherTok{\textless{}{-}}\NormalTok{ lower}
  
  \CommentTok{\# evaluate objective function}
\NormalTok{  X\_fit }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(X, }\DecValTok{2}\NormalTok{, fn)}
  
  \CommentTok{\# save new previews best}
\NormalTok{  P[, P\_fit }\SpecialCharTok{\textgreater{}}\NormalTok{ X\_fit] }\OtherTok{\textless{}{-}}\NormalTok{ X[, P\_fit }\SpecialCharTok{\textgreater{}}\NormalTok{ X\_fit]}
\NormalTok{  P\_fit[P\_fit }\SpecialCharTok{\textgreater{}}\NormalTok{ X\_fit] }\OtherTok{\textless{}{-}}\NormalTok{ X\_fit[P\_fit }\SpecialCharTok{\textgreater{}}\NormalTok{ X\_fit]}
  
  \CommentTok{\# save new global best}
  \ControlFlowTok{if}\NormalTok{(}\FunctionTok{any}\NormalTok{(P\_fit }\SpecialCharTok{\textless{}}\NormalTok{ p\_g\_fit))\{}
\NormalTok{    p\_g }\OtherTok{\textless{}{-}}\NormalTok{ P[, }\FunctionTok{which.min}\NormalTok{(P\_fit)]}
\NormalTok{    p\_g\_fit }\OtherTok{\textless{}{-}} \FunctionTok{min}\NormalTok{(P\_fit)}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The best fitness after \(100\) iterations is 0.0000038 and the best possible solution is \(0\).

\hypertarget{animation-2-dimensional}{%
\section{Animation 2-Dimensional}\label{animation-2-dimensional}}

This section provides insights into the behavior of the PSO by visualizing multiple iterations in a GIF. The GIF only works in Adobe Acrobat DC or in the Markdown/HTML version of this thesis. The amazing animation template is inspired by \href{https://www.r-bloggers.com/2021/10/how-to-build-a-basic-particle-swarm-optimiser-from-scratch-in-r/}{R'tichoke}. The PSO core from the above chapter was used to complete the \texttt{pso()} function and is tested here with seed 0. The function \texttt{fn} to be evaluated can be found in \href{https://www.r-bloggers.com/2021/10/how-to-build-a-basic-particle-swarm-optimiser-from-scratch-in-r/}{R'tichoke}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}

\NormalTok{fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pos)\{}
  \SpecialCharTok{{-}}\DecValTok{20} \SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.2} \SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\FloatTok{0.5} \SpecialCharTok{*}\NormalTok{((pos[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ (pos[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))) }\SpecialCharTok{{-}} 
  \FunctionTok{exp}\NormalTok{(}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{(}\FunctionTok{cos}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{pos[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{+} \FunctionTok{cos}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{pos[}\DecValTok{2}\NormalTok{]))) }\SpecialCharTok{+} 
  \FunctionTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+} \DecValTok{20}
\NormalTok{\}}

\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{pso}\NormalTok{(}
  \AttributeTok{par =} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{),}
  \AttributeTok{fn =}\NormalTok{ fn,}
  \AttributeTok{lower =} \SpecialCharTok{{-}}\DecValTok{10}\NormalTok{,}
  \AttributeTok{upper =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{s =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{maxiter =} \DecValTok{30}\NormalTok{,}
    \AttributeTok{w0 =} \FloatTok{1.2}\NormalTok{,}
    \AttributeTok{save\_traces =}\NormalTok{ T}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The function \texttt{fn} has many local minima and a global minima at \((1,1)\) with the value \(0\). The background color scale ranges from 0 as red to 20 as purple. The PSO has 10 particles, iterated 30 times with an inertia weight decreasing from 0.8 to 0. The iterations are visualized in the following GIF:

\begin{center}
\animategraphics[loop, width=10cm]{10}{./gifs/pso_2dim/gganim_plot}{1}{80}
\end{center}

\hypertarget{simple-constraint-handling}{%
\section{Simple Constraint Handling}\label{simple-constraint-handling}}

The simplest method for dealing with constraints is the penalty method, which takes into account the intensity of constraint breaks by increasing the objective value of a minimization problem. The two common problems studied in the last two chapters are quadratic problems with the same structure. This can be used to create a generic constraint handling function for these particular QP's. Both problems must satisfy the following equation:
\[
  A^T \times x - b_0 \geq \vec{0}
\]
Because of \texttt{meq=1}, the first value is considered as equality condition. This can be achieved by transforming the first value with the \texttt{abs()} function, which returns the absolute value. Equality is hard to achieve with the PSO, so it is transformed into a boundary constraint, with upper and lower boundary. Since \texttt{meq=1} always describes the constraint \(\sum w_i = 1\), it can be transformed into \(1 \leq \textstyle\sum w_i \geq 0.99\) to obtain a constraint sufficient for most practical applications. The final vector contains constraint breaks if the elements are negative. All of these negative elements are squared and summed to calculate a value representing the intensity of the constraint breaks. All this can be done with the following R function, which requires a \texttt{mat} object in the parent environment containing the matrices for the QP:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{calc\_const }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x)\{}
\NormalTok{  const }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(mat}\SpecialCharTok{$}\NormalTok{Amat) }\SpecialCharTok{\%*\%}\NormalTok{ x }\SpecialCharTok{{-}}\NormalTok{ mat}\SpecialCharTok{$}\NormalTok{bvec}
\NormalTok{  const[mat}\SpecialCharTok{$}\NormalTok{meq] }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\FunctionTok{pmax}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{abs}\NormalTok{(const[mat}\SpecialCharTok{$}\NormalTok{meq]}\SpecialCharTok{+}\FloatTok{0.005}\NormalTok{)}\SpecialCharTok{{-}}\FloatTok{0.005}\NormalTok{)}
  \FunctionTok{sum}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(}\DecValTok{0}\NormalTok{, const)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The new objective function \texttt{fn()} is transformed and consists of two parts. The first part is to evaluate the unconstrained objective of the QP with the following function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{calc\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x)\{}
  \FloatTok{0.5} \SpecialCharTok{*} \FunctionTok{t}\NormalTok{(x) }\SpecialCharTok{\%*\%}\NormalTok{ mat}\SpecialCharTok{$}\NormalTok{Dmat }\SpecialCharTok{\%*\%}\NormalTok{ x }\SpecialCharTok{{-}} \FunctionTok{t}\NormalTok{(mat}\SpecialCharTok{$}\NormalTok{dvec) }\SpecialCharTok{\%*\%}\NormalTok{ x}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The second part is the function \texttt{calc\_const()}. Since breaking constraints is much worse than losing fitness, it must have a higher weight (e.g.~5) which must be fine-tuned. This results in the final \texttt{fn()} function composition:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x)\{}
\NormalTok{  fitness }\OtherTok{\textless{}{-}} \FunctionTok{calc\_fit}\NormalTok{(x)}
\NormalTok{  constraints }\OtherTok{\textless{}{-}} \FunctionTok{calc\_const}\NormalTok{(x)}
  \FunctionTok{return}\NormalTok{(fitness }\SpecialCharTok{+} \DecValTok{5} \SpecialCharTok{*}\NormalTok{ constraints)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This approach to dealing with constraints is called the penalization method and is definitely the most straightforward approach. Its disadvantage is the fact that the PSO has to find a balance between the violation of constraints and the goal. As explained in \citep{InSi2008}, there are three other constraint handling methods, but the results show that none of them is superior. The treatment of constraints should be chosen appropriately for the given problem. For example, it may be useful to use the feasibility preservation technique to obtain a solution that is guaranteed not to break any constraints. The disadvantages here are longer computation time and less exploration of particles, since only feasible solutions can be stored as personal or global best solutions.

\hypertarget{example-mvp}{%
\section{Example MVP}\label{example-mvp}}

This example uses the \texttt{solve.QP()} approach from \ref{exampleanalyticalmvp} with ten assets as the benchmark. Briefly, the goal is to create an MVP from ten of the largest U.S. stocks between 2018-01-01 and 2019-12-31 for each possible \(\lambda\). The PSO has 300 particles and 200 iterations for each lambda. The main characteristics of all portfolios created with the \texttt{solve.QP()} compared to the PSO are shown below:

\includegraphics{Master_Thesis_files/figure-latex/pso7-1.png}

The dots for each \(\lambda\) are connected with a grey line to visualize the error of the PSO. It turns out that it is possible to solve MVP's with a PSO approach. It is noticeable that some PSO runs are trapped in local minimas and thus show a deviation from the \texttt{solve.QP()} approach, which can often be fixed by repeated runs.

\hypertarget{example-itp-mste}{%
\section{Example: ITP-MSTE}\label{example-itp-mste}}

The same ITP-MSTE solved with \texttt{solve.QP()} in \ref{exampleitpsolveqp} is used as the benchmark for the PSO. In summary, the goal is to create a portfolio that minimizes the mean square error of the returns of itself and the S\&P 500 between 2018-01-01 and 2019-12-31. The pool of assets includes all assets that are present in 2019-12-31 and have no missing values. The constraints are long only and the weights should sum to one. The parameters for the PSO are a swarm size of 100, 100 iterations, the inertia weight starts at \(1.2\), the upper bound is \(0.05\), and a starting position is the zero vector. The PSO was run ten times, and the aggregated best and mean runs are compared to the \texttt{solve.QP()} approach for seed 0 in the table below:
\includegraphics{Master_Thesis_files/figure-latex/pso8-1.png}

It can be seen that in all PSO runs, sufficient fitness was achieved with negligible constraint breaks, but much more computation time was required.

\hypertarget{pros-and-cons-for-continuous-problems}{%
\section{Pros and Cons for Continuous Problems}\label{pros-and-cons-for-continuous-problems}}

A PSO approach has advantages and disadvantages, since on the one hand any problem can theoretically be solved, but it cannot be guaranteed that the solution is also optimal. In addition, the calculations take much longer than with the \texttt{solve.QP()} approach, which raises the question why a PSO approach should have any benefit at all. This is exactly the case, if the solution of the problem is no longer possible by the \texttt{solve.QP()} alone, as it is for example the case with mixed-integer-quadratic-problems. In these types of problems, the condition for th variable of interest \(x\) is to be a integer vector. These kind of problems could be solved by the \texttt{solve.QP()} approach only continuously and then rounded. However, this rounding error can become arbitrarily large, which is why the chances of the PSO approach to achieve a better solution are greater than with the \texttt{solve.QP()} approach.

\hypertarget{discrete-problems}{%
\section{Discrete Problems}\label{discrete-problems}}

A continuous solution for a portfolio is not sufficient for practical purposes, since usually only integer amounts of assets can be purchased. It's even worse if lot sizes are needed, because these can only be bought in minimum denomination of e.g.~ten thousand. Lot sizes are often used in fixed income products. The biggest drawbacks of rounding a continuous solution are the disregarding of conditions and the difference in the objective value, which often cant reach the new optimum. A solution with broken conditions is not acceptable in practice and a \texttt{solve.QP()} approach only produces one solution, which is why its insecure to hope for a sufficient solution after rounding. The PSO doesn't have these drawbacks and can be easily used for discrete problems by rounding the input of the objective function \texttt{fn()}. In a portfolio with net asset value (\texttt{nav}) consisting of only American stocks with weights \(w_i\) and closing prices \(p_i\) can be discretized to \(w_i^d\) by the following formula:

\[
  w_i^d =\text{round}(w_i \cdot \frac{\text{nav}}{p_i})\cdot \frac{p_i}{\text{nav}}
\]

\hypertarget{example-discrete-itp-mste}{%
\section{Example: Discrete ITP-MSTE}\label{example-discrete-itp-mste}}

This example analyses the error of rounding a solution with the \texttt{solve.QP()} approach and compares it to a discrete PSO. A second discrete PSO is added, that takes the continuous solution of the \texttt{solve.QP()} and uses it as starting position of one particle. The ITP-MSTE focuses to track the S\&P 500 with its top 100 weighted assets and tries to construct a portfolio with the constraints long only, \(1 \leq \textstyle\sum w_i \geq 0.99\) and \(\text{nav} = 10000\) in the time frame from 2018-01-01 to 2019-12-31. The used prices are closing prices and both PSO's have 200 particles and 200 iterations. The results can be observed in the table below:
\includegraphics{Master_Thesis_files/figure-latex/pso9-1.png}

It can be seen that the rounded \texttt{solve.QP()} solution still has a good fitness but the constraints are not satisfied. The PSO has no constraint breaks and still reached a fitness close to the rounded \texttt{solve.QP()}. The PSO with \texttt{solve.QP()} solution as starting position has beaten both approaches. This indicates that a hybrid approach consisting of both the \texttt{solve.QP()} and afterwards the PSO for intelligent rounding with observed constraints would be a good heuristic for problems in practice.

\hypertarget{pso-variations}{%
\chapter{PSO Variations}\label{pso-variations}}

The standard PSO analyzed in the previous chapter is capable of solving a wide range of problems, but often gets stuck in local minima. In this chapter, different variants of the standard PSO are analyzed using a problem from the financial domain. The first variant is the PSO with function stretching, which is designed to allow the PSO to escape from local minima if they are discovered. The second variant is the local PSO, which is designed to reduce the probability of getting stuck in local minima by limiting the spread of information in the swarm. The third variant, the PSO with feasibility preservation, tries to optimize within the feasibility space and therefore provide only feasible solutions. The last variant is the PSO with self-adaptive velocity, which tries to adjust the control parameters according to certain rules and randomness.

\hypertarget{testproblem-discrete-itp-mste}{%
\subsection{Testproblem: Discrete ITP-MSTE}\label{testproblem-discrete-itp-mste}}

All variants are tested on a discrete ITP-MSTE to replicate the S\&P 500 with a tracking portfolio consisting of the top 50 assets in the S\&P 500. The daily data used to solve the ITP ranges from 2018-01-01 to 2019-12-31, and the assets must be in the S\&P 500 at the end of the time frame and have no missing values. The top 50 assets are selected by solving a continuous ITP-MSTE using an \texttt{solve.QP()} approach, and the assets with the 50 highest weights are selected. The tracking portfolio is discrete and has a net asset value of twenty thousand USD. The tracking portfolio is discretized using closing prices on 2019-12-31, and returns are calculated as simple returns using the adjusted closing prices. The maximum weighting for each asset is 10\% to reduce the dimension space of the problem. Additional constraints are long only and portfolio weights \(w\) should satisfy \(1 \leq \textstyle\sum w_i \geq 0.99\). All variants are run 100 times and compared to 100 runs of the standard PSO function created in the previous chapter. The swarm size for the PSO and all variants is 100 and the iterations are set to 400.

The next plot analyzes the behavior of the 100 standard PSO runs in each iteration by plotting the median of the best fitness achieved in each iteration. The confidence bands for the 95\% and 5\% quantiles of the best fitness values are plotted in the same color as the median, with less transparency:

\includegraphics{Master_Thesis_files/figure-latex/variants1-1.png}

The aggregate statistics of the last iterations of all 100 runs can be found in the table below:

\includegraphics{Master_Thesis_files/figure-latex/variants2-1.png}

\hypertarget{function-stretching}{%
\section{Function Stretching}\label{function-stretching}}

PSO often gets stuck in local minima, i.e., if the current best global position is a local minima with a larger environment around it, with only higher fitness, it is hard for the PSO to escape and find the global minima. Function stretching tries to make the PSO escape from such local minima by transforming the fitness function in a way described in \citep{PaVr2002}. It states that after finding a local minimum, a two-stage transformation proposed by Vrahatis in 1996 can be used to stretch the original function so that the discovered local minimum is transformed into a maximum, but any position with less fitness remains unchanged. The two stages of the transformation with a discovered local minimum \(\bar{x}\) are:
\[
  G(x) = f(x) +  \gamma_1 \cdot \| x-\bar{x} \| \cdot (\text{sign}(f(x)-f(\bar{x}))+1)
\]
and
\[
  H(x) = G(x) + \gamma_2 \cdot \frac{\text{sign}\biggl(f(x)-f(\bar{x})\biggr)+1}{\text{tanh}\biggl( \mu \cdot (G(x)-G(\bar{x})) \biggr)}
\]
The function \(G(\bar{x})\) can be simplified to \(f(\bar{x})\) and the \(\text{sign}()\) function is defined as follows:
\[
  \text{sign}(x) = 
  \begin{cases}
    1, & \text{if}\ \ x > 0\\
    0, & \text{if}\ \ x = 0\\
    -1, & \text{if}\ \ x < 0
  \end{cases}
\]
In the source it is suggested to select the following parameter values as default:
\begin{align*}
  \gamma_1 &= 5000 \\
  \gamma_2 &= 0.5 \\
  \mu &= 10^{-10}
\end{align*}

To better understand the transformation, it is used to stretch a simple function in \(\mathbb{R}^1\) defined as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pos)\{}
  \FunctionTok{cos}\NormalTok{(pos) }\SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{/}\DecValTok{10} \SpecialCharTok{*}\NormalTok{ pos}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

and the domain of definition is chosen as \(x \in [-20, 20]\). Suppose the PSO gets stuck in the local minima at \(\bar{x} = \pi - \text{arcsin}(\frac{1}{10}) \approx 3.04\). The original function and the transformed function are shown in the following graph:
\includegraphics{Master_Thesis_files/figure-latex/unnamed-chunk-9-1.png}
It can be seen that the fitness is stretched upward around the local minima \(\bar{x}\), making it much easier for the PSO to move down the hill and fall into new minima with lower fitness. All the lower fitness regions remain unchanged, as can be seen in the zoomed version of the bottom diagram from above:
\includegraphics{Master_Thesis_files/figure-latex/unnamed-chunk-10-1.png}

\hypertarget{implementation}{%
\subsection{Implementation}\label{implementation}}

Since it is not possible to know if the PSO is stuck in a local minima, a stagnation value was added that increases by one if the global best particle does not change. After ten iterations with no change, a local minima is assumed and the transformation of the objective function takes place. After that, all personal best fitness values must be re-evaluated to work with the evaluated space and the stagnation value is set to zero. To prevent transformation just at the end of all iterations, the current iteration must be less than the maximum iteration minus twenty to allow transformation to occur.

\hypertarget{test-pso-with-function-stretching}{%
\subsection{Test PSO with Function Stretching}\label{test-pso-with-function-stretching}}

The PSO with function stretching is called \texttt{PSO-fnS} and is evaluated on the test problem with \(\gamma_1 = 5000\), \(\gamma_2 = 0.5\) and \(\mu = 10^{-10}\):

\includegraphics{Master_Thesis_files/figure-latex/variants3-1.png}

The aggregate statistics of the last iterations of all 100 runs can be found in the table below:

\includegraphics{Master_Thesis_files/figure-latex/variants4-1.png}

\hypertarget{local-pso}{%
\section{Local PSO}\label{local-pso}}

A local PSO is a more general case of the global PSO. The only difference is the selection of the global best particle by defining a neighborhood. Each particle \(x_i\) has a neighborhood \(N(x_i, \bar{k})\), and the global best particle in its neighborhood is called the local best particle of \(x_i\). If the neighborhood is chosen large enough to contain all particles, it corresponds to the standard PSO (global PSO). A simple definition of a neighborhood with \(k\) neighbors for particles \(x_i\) given in \citep{Enge2013} would be:

\[
  N(x_i, k) = \{ x_{i-\bar{k}}, x_{i-(\bar{k}-1)}, x_{i-(\bar{k}-2)}, \cdots, x_{i}, \cdots, x_{i+(\bar{k}-2)}, x_{i+(\bar{k}-1)}, x_{i+\bar{k}} \}
\]
with
\[
  \bar{k} = floor(\frac{k}{2}) = \lfloor \frac{k}{2} \rfloor
\]
To illustrate this, the following figure defines the neighborhoods \(N(x_4, 4)\) and \(N(x_1, 4)\):

\includegraphics{img/PSO_local_chart.jpg}
In the latter case, it can be seen that the overflowing boundary will continue on the opposite side of the arranged particles.

\hypertarget{implementation-1}{%
\subsection{Implementation}\label{implementation-1}}

First, the neighbors for each particle are stored in a suitable data structure before the main part of the PSO is executed. In the local version there is no global best particle, instead the global best particle for the neighborhood of each particle has to be calculated in each step.

\hypertarget{test-local-pso}{%
\subsection{Test Local PSO}\label{test-local-pso}}

The PSO with particle neighborhoods is called \texttt{PSO-local} and evaluated on the test problem with \(k=10\):

\includegraphics{Master_Thesis_files/figure-latex/variants5-1.png}

The aggregate statistics of the last iterations of all 100 runs can be found in the table below:

\includegraphics{Master_Thesis_files/figure-latex/variants6-1.png}

It can be seen that it is superior to the standard PSO in this case. Especially in preventing stagnation in local minimas, which can be seen in the narrower quantile bands at the end.

\hypertarget{preserving-feasibility}{%
\section{Preserving Feasibility}\label{preserving-feasibility}}

SCI2002Constrained.pdf

\hypertarget{self-adaptive-velocity}{%
\section{Self-Adaptive Velocity}\label{self-adaptive-velocity}}

A self-adaptive velocity PSO approach that attempts to reduce hyperparameters was analyzed in \citep{FaYa2014}. The self-adaptive velocity is enabled by multiple velocity update schemes that are used randomly. In addition, all hyperparameters are self-adaptive in the way that each particle has its own coefficients \(c_g\), \(c_p\), and \(w\), which change after each iteration depending on the distance to maximum fitness, among other factors. The resulting PSO has no real hyperparameters to adjust, which allows it to be used as a general-purpose PSO.

\hypertarget{implementation-2}{%
\subsection{Implementation}\label{implementation-2}}

The process of this PSO variant is too different from the standard PSO, so all changes are combined in steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Initialize\\
  Each particle \(d\) must initialize its own inertial weight \(w_d^0=0.5\) and acceleration coefficients \(c_{p,d}^0 = c_{g,d}^0 = 2\).
\item
  Velocity and positions\\
  Update the velocity of each particle \(d\) with the following switch-case for a uniform random number \(r = \text{Unif}(0,1)\) in iteration \(i+1\):
  \begin{align*}
    v_d^{i+1} &= w_d^i \cdot v_d^{i}+c_{p,d}^i \cdot Z \cdot (P_{d}^i-x_d^i) + c_{g,d}^i \cdot Z \cdot (p_{g}^i-x_d^i) \\
    Z &= \begin{cases}
   \text{Unif}(0,1), & \text{if}\ \ r > 0.8\\
   \text{Cauchy}(\mu_1, \sigma_1), & \text{if}\ \ 0.8 \geq r > 0.4\\
   \text{Cauchy}(\mu_2, \sigma_2), & \text{if}\ \ 0.4 \geq r\\
    \end{cases}
  \end{align*}
  with
  \begin{align*}
    \mu_1 &= 0.1 \cdot (1-(\frac{i}{i_{max}})^2) + 0.3 \\
    \sigma_1 &= 0.1 \\
    \mu_2 &= 0.4 \cdot (1-(\frac{i}{i_{max}})^2) + 0.2 \\
    \sigma_2 &= 0.4
  \end{align*}
  and \(\text{Cauchy}(\mu, \sigma)\) is a random number generated from the Cauchy distribution obtained with \texttt{rcauchy()} in R. The position update is the same as for the standard PSO. When a particle \(d\) has left the feasible search space in its coordinate \(z\), it is moved back with the following switch-case for \(r = \text{Unif}(0,1)\):
  \[
    x_{d,z} = 
    \begin{cases}
   \text{generate uniform in feasable space}, & \text{if}\ \ r > 0.7\\
   \text{push back to boundary}, & \text{otherwise}\ \ \\
    \end{cases}
  \]
\item
  Fitness evaluation\\
  In the same way as for the standard PSO.
\item
  Self-adaptive control parameters\\
  For an objective function \(f()\) and the maximum fitness of all particles \(f_{max} = \text{max}(f(X^{i+1}))\), the parameters \(w_d^{i}\), \(c_{p,d}^{i}\) and \(c_{g,d}^{i}\) are adjusted for each particle \(d\) as follows:
  \begin{align*}
    W^i_d &= \frac{\left| f(x_d^{i+1})-f_{max} \right|}{\sum_d\left| f(x_d^{i+1})-f_{max} \right|} \\
    w_d^{i+1} &= \text{Cauchy}(\sum_d W^i_d \cdot w_d^{i}, 0.2) \\
    c_{p,d}^{i+1} &= \text{Cauchy}(\sum_d W^i_d \cdot c_{p,d}^{i}, 0.3) \\
    c_{g,d}^{i+1} &= \text{Cauchy}(\sum_d W^i_d \cdot c_{g,d}^{i}, 0.3)
  \end{align*}
  Then, the parameters are adjusted to their limits using the following formulas:
  \begin{align*}
  w_d^{i+1} &= 
    \begin{cases}
   \text{Unif}(0,1), & \text{if}\ \ w_d^{i+1} > 1\\
   \text{Unif}(0,0.1), & \text{if}\ \  0 > w_d^{i+1}\\
   w_d^{i+1}, & \text{otherwise}
    \end{cases}\\
  c_{p,d}^{i+1} &= 
    \begin{cases}
   \text{Unif}(0,1) \cdot 4, & \text{if}\ \ c_{p,d}^{i+1} > 4\\
   \text{Unif}(0,1), & \text{if}\ \  0 > c_{p,d}^{i+1}\\
   c_{p,d}^{i+1}, & \text{otherwise}
    \end{cases}\\
  c_{g,d}^{i+1} &= 
    \begin{cases}
   \text{Unif}(0,1) \cdot 4, & \text{if}\ \ c_{g,d}^{i+1} > 4\\
   \text{Unif}(0,1), & \text{if}\ \  0 > c_{g,d}^{i+1}\\
   c_{g,d}^{i+1}, & \text{otherwise}
    \end{cases}\\
  \end{align*}
\item
  Update the best positions\\
  Update the personal best \(P\) and global best \(p_g\) positions as in the standard PSO.
\item
  Repeat\\
  Steps 2 to 5 are repeated until the maximum iteration number \(i_{max}\) is reached.
\end{enumerate}

\hypertarget{analyse-implementation}{%
\subsection{Analyse Implementation}\label{analyse-implementation}}

The random use of the distributions for the velocity update increases the diversity of the swarm. The coefficients of iteration \(i\) with 100 maximum iterations are distributed as follows:\\
\includegraphics{Master_Thesis_files/figure-latex/unnamed-chunk-11-1.png}
It can be seen that the randomness of the motion increases compared to the uniform distribution and the center of the Cauchy distributions slowly decreases towards the absolute term. In addition, the two Cauchy distributions differ in explorability and exploitability, indicated by probabilities outside \([0, 1]\).

Even more difficult to interpret is the adjusting of the control parameters. The value \(W_d^i\) is a weighting of the distances to the worst fitness, resulting in a higher weighting of the particles with good fitness. Later, the control parameters are adjusted using the Cauchy distribution with a weighted value of the previous control parameters as the center, giving higher weights to the control parameters that produced better fitness. This results in random control parameters distributed around the best previous control parameters. The resulting behavior can be described with a small quote, ``If exploration is beneficial, more exploration is done. If not, more is exploited.''

\hypertarget{test-pso-with-self-adaptive-velocity}{%
\subsection{Test PSO with Self-Adaptive Velocity}\label{test-pso-with-self-adaptive-velocity}}

The PSO with self-adaptive speed is called \texttt{PSO-SAvel} and is evaluated for the test problem with the constants used in the implementation section:

\includegraphics{Master_Thesis_files/figure-latex/variants7-1.png}

The aggregate statistics of the last iterations of all 100 runs can be found in the table below:

\includegraphics{Master_Thesis_files/figure-latex/variants8-1.png}

\hypertarget{pso-r-package}{%
\section{PSO R-Package}\label{pso-r-package}}

Analyse:
Unterschied zu meiner implementation:
PSO-pkg hat eine bessere veolcity initialisierung aber kann volle länge haben und ich habe 1/10.

\includegraphics{Master_Thesis_files/figure-latex/variants_pkg-1.png}

\hypertarget{real-life-itp-example}{%
\chapter{Real Life ITP Example}\label{real-life-itp-example}}

(Vorhaben)
focus auf testphase legen, diskrete (20k), long only, 0\textless w\textless0.1, ITP S\&P500 tracken mit weniger als 50 titeln und mehr als 30, vorauswahl von assetpool?, umschichtungs constraint, transaktionskosten 1 euro,

\hypertarget{transaction-costs}{%
\section{Transaction Costs}\label{transaction-costs}}

asd

\hypertarget{rebalancing-constraint}{%
\section{Rebalancing Constraint}\label{rebalancing-constraint}}

asd

\hypertarget{analyse-objectives}{%
\section{Analyse Objectives}\label{analyse-objectives}}

asd

\hypertarget{complete-itp-example}{%
\section{Complete ITP Example}\label{complete-itp-example}}

asd

\hypertarget{conclusion}{%
\chapter{Conclusion}\label{conclusion}}

asd

  \bibliography{book.bib,packages.bib}

\end{document}
