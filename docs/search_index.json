[["index.html", "Asset Allocation using Particle Swarm Optimization in R Preface", " Asset Allocation using Particle Swarm Optimization in R Axel Roth 2022-09-15 Preface |||in progress||| (soll vor dem TOC kommen denke ich) "],["abstract.html", "Abstract", " Abstract |||in progress||| (zusammenfassung: Vor dem TOC) motivation structure results "],["software-information-and-usage.html", "Chapter 1 Software information and usage 1.1 R-Version and Packages 1.2 Reproducibility 1.3 R-functions", " Chapter 1 Software information and usage |||in progress||| wie ich das buch schreibe, R markodwn bookdown und so und welche versionen ich nutze 1.1 R-Version and Packages 1.2 Reproducibility github und code im bookdown 1.3 R-functions zb plotly_save "],["open-data-sources.html", "Chapter 2 Open Data Sources 2.1 R-Functions", " Chapter 2 Open Data Sources To increase reproducibility, all data are free and can be loaded from the quantmod R package with the function getSymbols(). It is possible to choose between different data sources like yahoo-finance (default), alpha-vantage, google and others. 2.1 R-Functions The following functions were created to increase the ease of data collection with the quantmod R package, which can be found in the R/ directory in the attached github repository. 2.1.1 get_yf() This function is the main wrapper for collecting data with getSymbols() from yahoo-finance, and converts prices to returns with the pri_to_ret() function explained in 3.5.1. The output is a list containing prices and returns as xts objects. The arguments that can be passed to get_yf() are: tickers: Vector of symbols (asset names, e.g. APPL, GOOG, ) from =\"2018-01-01\": R-Date to = \"2019-12-31\": R-Date price_type = \"close\": Type of prices to be recorded (e.g. open, high, low, closed, adjusted) return_type = \"adjusted\": Type of return to be recorded (e.g. open, high, low, closed, adjusted) print = F: Should the function print the return of getSymbols() 2.1.2 buffer() To make data reusable and reduce compilation time, this function stores the data collected with get_yf(). It receives an R expression, evaluates it and stores it in the buffer_data/ directory under the specified name. If this name already exists, it loads the R object from the RData files without evaluating the expression. The evaluation and overwriting of the existing RData file can be forced with force=T. "],["mathfundations.html", "Chapter 3 Mathematical Fundations 3.1 Basic Operators 3.2 Return Calculation 3.3 Markowitz Modern Portfolio Theory (MPT) 3.4 Portfolio Math 3.5 R-Functions", " Chapter 3 Mathematical Fundations This chapter provides an overview of the mathematical calculations and conventions used in this thesis. It is important to note that most mathematical formulas are written in matrix notation. In most cases, this will result in a direct translation to R code. All necessary assumptions required for the modeled return structure are listed in this chapter so that any reader can understand the formulas given. It is important to note that reality is too complex and can only be partially modeled. Simple, basic models are used that do not stand up to reality, but these models or variations of them are commonly used in the financial world and have proven to be helpful. The complexity of solving advanced and basic models does not differ in PSO because the dimension of the objective function is based on the number of elements that can be selected, see chapter 5. 3.1 Basic Operators A compendium comparing commonly used mathematical symbols with R code and their meaning is given in the following table: 3.2 Return Calculation Any portfolio optimization strategy based on historical data must start with returns. These returns are calculated using adjusted closing prices, which show the percentage change over time. Adjusted closing prices reflect dividends and are adjusted for stock splits and rights offerings. These returns are essential for comparing assets and analyzing dependencies. 3.2.1 Simple Returns The default time frame for all raw data in this thesis is one working day and only simple rates of return are used. Assuming there is an asset with price \\(P\\) on working day \\(t_i\\) and the following working day \\(t_{i+1}\\), it follows that the simple rate of return on \\(t_{i+1}\\) can be calculated as follows: \\[ R_{i+1} = \\frac{P_{t_{i+1}}}{P_{t_i}}-1 \\] 3.3 Markowitz Modern Portfolio Theory (MPT) In 1952, Harry Markowitz published his first seminal paper, which had a significant impact on modern finance, primarily by outlining the implications of diversification and efficient portfolios. The definition of an efficient portfolio is a portfolio that has either the maximum expected return for a given risk target or the minimum risk for a given expected return target. A simple quote to define diversification might be, A portfolio has the same return but less variance than the sum of its parts. This is true when assets are not perfectly correlated, as bad and good performance can offset each other, reducing the likelihood of extreme events. For more information, see (Maringer 2005). 3.3.1 Assumptions of Markowitz Portfolio Theory The following list contains all Markowitz assumptions according to (Maringer 2005): Perfect market without taxes or transaction costs Short sales are disallowed Assets are infinitely divisible Expected Returns, Variances and Covariances contain all information Investors are risk-adverse, they will only accept greater risk if they are compensated with a higher expected return The assumption that the returns are normally distributed is not required, but is assumed in this case to simplify the problem. It is obvious that these assumptions are unrealistic in reality. More details on the requirements for using other distributions can be found in (Maringer 2005). 3.4 Portfolio Math Proofs of the basic calculations required for portfolio optimization, as shown in (Zivot 2021), are provided in this section. Returns are presented differently than in most sources, as this is the most common data format used in practice. Suppose there are \\(N\\) assets described by a return vector \\(R\\) of random variables and a portfolio weight vector \\(w\\), respectively: \\[ R = \\begin{bmatrix} R_{1} &amp; R_{2} &amp; \\cdots &amp; R_{N} \\end{bmatrix} , \\ \\ w = \\begin{bmatrix} w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{N} \\end{bmatrix} \\] In this thesis, each return is simplified as being normally distributed with \\(R_i = \\mathcal{N}(\\mu_i, \\sigma_i^2)\\). As a result, linear combinations of normally distributed random variables are jointly normally distributed and have a mean, variance, and covariance that can be used to fully describe them. 3.4.1 Expected Returns The following formula can be used to get the expected returns of a vector with normally distributed random variables \\(R \\in \\mathbb{R}^{1\\times N}\\): \\[\\begin{align*} E[R] &amp;= \\begin{bmatrix} E[R_{1}] &amp; E[R_{2}] &amp; \\cdots &amp; E[R_{N}] \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} \\mu_{1} &amp; \\mu_{2} &amp; \\cdots &amp; \\mu_{N} \\end{bmatrix} = \\mu \\end{align*}\\] and \\(\\mu_i\\) can be estimated in R using historical data and the formula for the geometric mean of returns (also called compound returns). The function to calculate the geometric mean of returns from an xts object can be found in 3.5.3. 3.4.2 Expected Portfolio Returns The following equation can be used to obtain the linear combination of expected returns \\(\\mu\\) and a weighting vector \\(w\\) (e.g. portfolio weights): \\[\\begin{align*} \\mu \\times w &amp;= \\begin{bmatrix} E[\\mu_{1}] &amp; E[\\mu_{2}] &amp; \\cdots &amp; E[\\mu_{N}] \\end{bmatrix} \\times \\begin{bmatrix} w_{1} \\\\ w_{2} \\\\ \\cdots \\\\ w_{N} \\end{bmatrix} \\\\ &amp;= E[\\mu_{1}] \\cdot w_1 + E[\\mu_{2}] \\cdot w_2 + \\cdots + E[\\mu_{N}] \\cdot w_{N} = \\mu_P \\end{align*}\\] 3.4.3 Covariance The general formula of the covariance matrix \\(\\textstyle\\sum\\) of a random vector \\(R\\) with \\(N\\) normally distributed elements and \\(\\sigma_{i,j}\\) as correlation of two unique values is described as follows: \\[\\begin{align*} Cov(R) &amp;= E[(R-\\mu)^T \\otimes (R-\\mu)] \\\\ &amp;= \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{1,2} &amp; \\cdots &amp; \\sigma_{1,N} \\\\ \\sigma_{2, 1} &amp; \\sigma_2^2 &amp; \\cdots &amp; \\sigma_{2, N} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{N, 1} &amp; \\sigma_{N, 2} &amp; \\cdots &amp; \\sigma_N^2 \\\\ \\end{bmatrix}\\\\ &amp;=\\textstyle\\sum \\end{align*}\\] and can be estimated in R with the basis function cov() and historical data. 3.4.4 Portfolio Variance Let \\(R\\) be a random vector with \\(N\\) normally distributed elements and \\(w\\) a weight vector. Assuming that the covariance matrix \\(\\sum\\) of \\(R\\) is known, the variance of the linear combination of \\(R\\) can be calculated as follows: \\[\\begin{align*} Var(R \\times w) &amp;= E[(R \\times w - \\mu \\times w)^2] \\\\ &amp;= E[((R - \\mu) \\times w)^2] \\end{align*}\\] Since \\((R - \\mu) \\times w\\) is a scalar, it can be transformed from \\(((R - \\mu) \\times w)^2\\) to \\(((R - \\mu) \\times w)^T \\cdot ((R - \\mu) \\times w)\\) and results in: \\[\\begin{align*} Var(R \\times w) &amp;= E[((R - \\mu) \\cdot w)^T \\times ((R - \\mu) \\times w)]\\\\ &amp;= E[(w^T \\times (R - \\mu)^T) \\cdot ((R - \\mu) \\times w)]\\\\ &amp;= w^T \\times E[(R - \\mu)^T \\otimes (R - \\mu)] \\times w \\\\ &amp;= w^T \\times Cov(R) \\times w \\\\ &amp;= w^T \\times \\textstyle\\sum \\times w \\end{align*}\\] The same is true for an estimate of \\(\\textstyle\\sum\\). 3.4.5 Portfolio Returns Suppose there are \\(N\\) assets forming a portfolio with weights \\(w\\) at time \\(t_0\\), and the portfolio is to pass through several time steps until \\(t_T\\) without rebalancing. What are the portfolio returns at each time step \\(t_i\\)? Clearly, assets with positive performance in the current time step will have a higher weight in the next time step. This can be done by adjusting the weights after each time step depending on the returns. The formula for holding a portfolio with weights \\(\\textstyle\\sum w = 1\\) and return matrix \\(R \\in \\mathbb{R}^{T \\times N}\\), has return \\(Z_i-1\\) on \\(t_i\\) with \\(i=0, 1, \\cdots, T\\) for: \\[ Z_i = \\begin{cases} (1+R_i)\\cdot w &amp;\\text{ if }i=0\\\\ (1+R_i)\\cdot \\frac{Z_{i-1}}{\\sum Z_{i-1}} &amp;\\text{ if }i&gt;0 \\end{cases} \\] This calculation of portfolio returns is implemented in the function calc_portfolio_returns() below. 3.5 R-Functions |||in progress||| 3.5.1 pri_to_ret() |||in progress||| ((prices to returns)) 3.5.2 ret_to_cumret() |||in progress||| ((returns to cumulated returns normalized to 100)) 3.5.3 ret_to_geomeanret() The geometric mean of returns is a better estimator than the arithmetic mean of returns because it captures the exact mean price changes over a period of time. The variance estimated from the daily returns is a daily variance, so the returns must have the same time base. This can be done by calculating the geometric mean of the returns from multiple daily returns. Assuming there is an asset with returns \\(r_1 = 0.01\\), \\(r_2=0.03\\), and \\(r_3=0.02\\), it follows that the geometric mean return \\(r^{id}\\) can be calculated as: \\[ r^{id} = ((1+r_1) \\cdot (1+r_2) \\cdot (1+r_3))^{1/3}-1 = 0.01996732 \\] And the advantage is that it is a daily average return that gives exactly the same result as the real return, that is: \\[ (1+r^{id})^3 = (1+r_1) \\cdot (1+r_2) \\cdot (1+r_3) \\] This is not the case with the arithmetic mean of the returns. The general formula for calculating the mean geometric return of \\(n\\) days is: \\[ r^{id} = (\\prod_{i=1}^n (1+r_i))^{\\frac{1}{n}}-1 \\] and as R code: ret_to_geomeanret &lt;- function(xts_ret){ sapply((1+xts_ret), prod)^(1/nrow(xts_ret))-1 } 3.5.4 calc_portfolio_returns() This is the implementation of a vectorial calculation of portfolio returns over multiple periods with a weighting vector weights at \\(t_0\\) and no re-balancing: calc_portfolio_returns &lt;- function(xts_returns, weights, name=&quot;portfolio&quot;){ if(sum(weights)!=1){ xts_returns$temp___X1 &lt;- 0 weights &lt;- c(weights, 1-sum(weights)) } res &lt;- cumprod((1+xts_returns)) * matrix( rep(weights, nrow(xts_returns)), ncol=length(weights), byrow=T) res &lt;- xts( rowSums(res/c(1, rowSums(res[-nrow(xts_returns),])))-1, order.by=index(xts_returns)) %&gt;% setNames(., name) return(res) } This function has the same results as the Return.portfolio() function from the PortfolioAnalytics package. References "],["activ-vs-passiv-investing.html", "Chapter 4 Activ vs Passiv Investing", " Chapter 4 Activ vs Passiv Investing |||in progress||| The fundation of Asset Management passiv vs activ studie https://www.scirp.org/journal/paperinformation.aspx?paperid=92983 gut gut file:///C:/Users/Axel/Desktop/Master-Thesis-All/Ziel%20was%20beantwortet%20werden%20soll/Quellen%20nur%20wichtige/Rasmussen2003_Book_QuantitativePortfolioOptimisat.pdf "],["challenges.html", "Chapter 5 Challenges of Passiv Investing 5.1 Mean-Variance Portfolio (MVP) 5.2 Index-Tracking Portfolio (ITP)", " Chapter 5 Challenges of Passiv Investing In this chapter, we analyze two common challenges of passive investing and create simple use cases to test the PSO. The first challenge is the mean-variance portfolio (MVP) from Markowitzs modern portfolio theory, which, simply put, is an optimal allocation of assets in terms of risk and return. The second challenge is the index tracking problem, which attempts to construct a portfolio with minimal tracking error to a given benchmark. 5.1 Mean-Variance Portfolio (MVP) Markowitz showed that diversifying risk across multiple assets reduces overall portfolio risk. This result was the beginning of the widely used modern portfolio theory, which uses mathematical models to create portfolios with minimal variance for a given return target. All such optimal portfolios for a given return target are called efficient and constitute the efficient frontier. 5.1.1 MVP Objective Let there be \\(N\\) assets and their returns on \\(T\\) different days, creating a return matrix \\(R \\in \\mathbb{R}^{T \\times N}\\). Each element \\(R_{t,i}\\) contains the return of the \\(i\\)-th asset on day \\(t\\). The covariance matrix of the returns is \\(\\textstyle\\sum \\in \\mathbb{R}^{N \\times N}\\) and the expected returns are \\(\\mu \\in \\mathbb{R}^{N}\\). The MVP with the risk aversion parameter \\(\\lambda \\in [0,1]\\), as shown in (Maringer 2005), can be formalized as follows: \\[\\begin{equation} \\underset{w}{min} \\ \\ \\ \\lambda \\ w^T \\textstyle\\sum w - (1-\\lambda) \\ \\mu^T w \\tag{5.1} \\end{equation}\\] The risk aversion parameter \\(\\lambda\\) defines the tradeoff between risk and return. With \\(\\lambda = 1\\), the minimization problem contains only the variance term, leading to a minimum variance portfolio, and \\(\\lambda = 0\\) transforms the problem into a minimization of negative expected returns, leading to a maximum return portfolio. All possible portfolios created by \\(\\lambda \\in [0, 1]\\) define the efficient frontier. 5.1.2 MVP example All possible MVPs together define the efficiency frontier, which is analyzed in this section without going into the details of its calculation. This example uses three assets (stocks: IBM, Google, Apple) and calculates the solution of the MVP for each \\(\\lambda\\). First, the daily returns of these three assets from 2018-01-01 to 2019-12-31 are loaded. returns &lt;- buffer( get_yf(tickers = c(&quot;IBM&quot;, &quot;GOOG&quot;, &quot;AAPL&quot;), from = &quot;2018-01-01&quot;, to = &quot;2019-12-31&quot;)$returns, &quot;CPI_3_assets&quot; ) The cumulative daily returns are: The expected daily returns and the covariance matrix for the three assets can be estimated using the formulas from chapter 3: mu &lt;- sapply((1+returns), prod)^(1/nrow(returns))-1 cat0(&quot;estimation of expected daily returns:&quot;) mu cat(&quot;\\n&quot;) cov &lt;- as.matrix(nearPD(cov(returns))$mat) cat0(&quot;estimation of positiv definite covariance matrix:&quot;) cov estimation of expected daily returns: AAPL IBM GOOG 0.0011434115 -0.0001059164 0.0004870292 estimation of positiv definite covariance matrix: AAPL IBM GOOG AAPL 0.0003011998 0.0001177670 0.0001798891 IBM 0.0001177670 0.0002047502 0.0001158595 GOOG 0.0001798891 0.0001158595 0.0002728725 This is all the data necessary to solve the MVP with \\(\\lambda \\in \\{0.01, 0.02, ..., 0.99, 1\\}\\). All 100 portfolios are computed by solving a quadratic minimization problem with the long only (\\(w_i \\geq 0 \\ \\forall \\ i\\)) constraint and the weights should sum to 1. portfolios &lt;- data.frame() mu_and_var &lt;- NULL for(lambda in seq(0.01,1, 0.01)){ mat &lt;- list( Dmat = lambda * cov, dvec = (1 - lambda) * mu, Amat = t(rbind( rep(1, ncol(returns)), # sum up to 1 diag(1, nrow=ncol(returns), ncol=ncol(returns)) # long only )), bvec = c( 1, # sum up to 1 rep(0, ncol(returns)) # long only ), meq = 1 ) qp &lt;- solve.QP(Dmat = mat$Dmat, dvec = mat$dvec, Amat = mat$Amat, bvec = mat$bvec, meq = mat$meq) port &lt;- xts(returns %*% qp$solution, order.by=index(returns)) mu_and_var &lt;- rbind( mu_and_var, data.frame(&quot;lambda&quot; = lambda, &quot;mu&quot; = mu %*% qp$solution, &quot;sd&quot; = sqrt(t(qp$solution) %*% cov %*% qp$solution)) ) portfolios &lt;- rbind( portfolios, qp$solution ) } portfolios &lt;- data.frame(portfolios) colnames(portfolios) &lt;- colnames(returns) The resulting daily returns and standard deviations are converted to annual returns and standard deviations and plotted to create the efficiency frontier: # annualize mu_and_var$mu &lt;- (1+mu_and_var$mu)^250-1 mu_and_var$sd &lt;- mu_and_var$sd * sqrt(250) plot_ly(data = mu_and_var) %&gt;% add_lines(y = ~mu, x = ~sd, name = &quot;efficient frontier&quot;) %&gt;% add_trace(x = ~sd, y=~mu, mode=&quot;markers&quot;, name = &quot;lambda steps&quot;) %&gt;% layout( title = &quot;3-Asset MVP&quot;, yaxis = list(range=c(min(mu_and_var$mu)*0.9, max(mu_and_var$mu)*1.1)), xaxis = list(range=c(min(mu_and_var$sd)*0.95, max(mu_and_var$sd)*1.05)), margin = list( l = 10, r = 10, b = 70, t = 50, pad = 4 ) ) %&gt;% html_save() The portfolio compositions for each \\(\\lambda\\) are: p &lt;- plot_ly(type=&quot;bar&quot;) %&gt;% layout(title=&quot;Portfolio compositions&quot;, barmode=&quot;stack&quot;, xaxis = list(title=&quot;lambda&quot;, autorange = &quot;reversed&quot;), yaxis = list(title=&quot;wgt&quot;)) for(i in 1:ncol(portfolios)){ p &lt;- p %&gt;% add_trace(x=seq(0.01,1, 0.01), y=portfolios[, i], name = colnames(portfolios)[i]) } p %&gt;% html_save() It can be observed that the minimum variance portfolio was achieved with a diversified composition of the tree assets. With gradually decreased \\(\\lambda\\), the portfolio starts to ignore the variance and invest more in the most risky and highest return asset. 5.2 Index-Tracking Portfolio (ITP) Indices are baskets of assets that are used to track the performance of a particular asset group. For example, the well-known Standard and Poors 500 Index (S&amp;P 500 for short) tracks the 500 largest companies in the United States. Indices are not for sale and serve only to visualize the performance of a particular asset group, without incurring transaction costs. Such indices, or a combination of indices, are used by asset managers as benchmarks to compare the performance of their funds. Each fund has its own benchmark, which contains roughly the same assets that the manager might buy. If the fund underperforms its benchmark, it may indicate that the fund manager has made poor decisions. Therefore, fund managers strive to outperform their benchmarks through carefully selected investments. Past experience has shown that this is rarely achieved with active management by cost (Desmond Pace and Grima 2016). This has led to the growing popularity of passively managed funds whose goal is to track their benchmarks as closely as possible. This can be achieved through either full or sparse replication. Full replication is a portfolio that contains all the assets in the benchmark with the same weightings. The resulting performance equals the performance of the benchmark when transaction costs are neglected. The first problem is that a benchmark may contain assets that are not liquid or cannot be purchased. The second problem is the weighting scheme of the indices, because they are often weighted by their market capitalization, which changes daily. This would result in the need to reweight daily and increase transaction costs to replicate the performance of the benchmark as closely as possible. To avoid this, sparse replications are used that contain only a fraction of the benchmarks assets. To do so, the portfolio manager must define his benchmark, which should overlap with the investment universe of his fund. He then reduces this universe, taking into account investor constraints and availability, to create a pool of possible assets. For example, a pool that replicates the S&amp;P 500 might consist of the one hundred highest-weighted assets in the S&amp;P 500. The ITP can be modeled in two ways analysed in (Iuliia Gavriushina 2019). 5.2.1 ITP with TEV objective (ITP-TEV) The classic and widely used model tries to reduce the variance of tracking error (TEV) with the following formula: \\[ min \\ \\ Var(TE) = Var(r_{p}-r_{bm}) \\] To obtain the portfolio weights \\(w\\), one needs to substitute \\(r_{p}\\) as follows: \\[ r_{p} = R \\times w \\] The variance is then solved until a quadratic problem is presented as a function of portfolio weights \\(w\\): \\[\\begin{align*} Var(r_{p}-r_{bm}) &amp;= Var(R \\times w - r_{bm}) \\\\ &amp;= Var(R \\times w) + Var(r_{bm}) - 2 \\cdot Cov(R \\times w,r_{bm}) \\end{align*}\\] Now the three terms can be solved, starting with the simplest one. \\[ Var(r_{bm}) = \\sigma_{bm}^2 = constant \\] The variance of the portfolio can be solved with 3.4.4: \\[ Var(R \\times w) = w^T \\times Cov(R) \\times w \\] And the last term can be solved in the same way as in (Zivot 2021): \\[\\begin{align*} Cov(A \\times a, b) &amp;= Cov(b, A \\times a) \\\\ &amp;= E[(b-\\mu_{b})(A \\times a-\\mu_{A} \\times a)] \\\\ &amp;= E[(b-\\mu_{b})(A-\\mu_{A}) \\times a] \\\\ &amp;= E[(b-\\mu_{b})(A-\\mu_{A})] \\times a \\\\ &amp;= Cov(A,b) \\times a \\end{align*}\\] This results in the final formula of the ITP: \\[\\begin{align*} Var(r_{p}-r_{bm}) &amp; = Var(R \\times w - r_{bm}) \\\\ &amp; = Var(R \\times w) - 2 \\cdot Cov(R \\times w,r_{bm}) + Var(r_{bm}) \\\\ &amp; = w^T \\times Cov(R) \\times w - 2 \\cdot Cov(r_{bm}, R)^T \\times w + \\sigma_{bm}^2 \\tag{5.2} \\end{align*}\\] The minimization problem of the ITP in the general structure required by many optimizers is: \\[ \\min\\limits_{w} \\ \\ \\frac{1}{2} \\cdot w^T \\times D \\times w -d^T \\times w \\] Minimization problems can ignore constant terms and global stretch coefficients and still find the same minimum. This leads to a general substitution of the ITP with TEV objective as follows: \\[ D = Cov(R) \\] and \\[ d = Cov(r_{bm}, R) \\] It is possible to add some basic constraints, as in the MVP to sum the weights to 1 and be long only. Despite the fact that this model is often used, it has a big disadvantage in that it cannot detect constant deviations in the returns. For this reason, the following model exists, which focuses on the mean square tracking error of returns (MSTE). 5.2.2 ITP with MSTE objective (ITP-MSTE) A good explanation of the ITP with MSTE objective can be found in ahmedbadary. The objective is to minimize the mean square tracking error (MSTE) of daily portfolio returns \\(r_{t, p}\\) and daily benchmark returns \\(r_{t, bm}\\) on \\(T\\) days: \\[ \\frac{1}{T} \\sum^T_{t=1}(r_{t, p}-r_{t, bm})^2 \\] The formula can be rewritten as vector norm: \\[ \\frac{1}{T} \\left\\Vert r_{p}-r_{bm} \\right\\Vert_2^2 \\] Which results in the following minimization with neglected stretching factor: \\[ min \\ \\ \\left\\Vert r_{p}-r_{bm} \\right\\Vert_2^2 \\] The portfolio returns \\(r_p\\) needs to be substituted to contain the portfolio weights \\(w\\) like in the TEV objective above. This results in the below transformation of the problem: \\[\\begin{align*} \\left\\Vert r_{p}-r_{bm} \\right\\Vert_2^2 &amp;= \\left\\Vert R \\times w-r_{bm} \\right\\Vert_2^2 \\\\ &amp;= (R \\times w-r_{bm})^T \\times (R \\times w-r_{bm}) \\\\ &amp;= (w^T \\times R^T-r_{bm}^T) \\times (R \\times w-r_{bm}) \\\\ &amp;= w^T \\times R^T \\times R \\times w - w^T \\times A^T \\times r_{bm} - r_{bm}^T \\times R \\times w + r_{bm}^T \\times r_{bm} \\end{align*}\\] The minimization and the fact that the scalars \\(w^T \\times R^T \\times r_{bm}\\) and \\(r_{bm}^T \\times R \\times w\\) are equal, transforms the problem to: \\[ \\min\\limits_{w} \\ \\ \\left\\Vert r_{p}-r_{bm} \\right\\Vert_2^2 = w^T \\times R^T \\times R \\times w - 2\\cdot r_{bm}^T \\times R \\times w \\] This leads to the equivalent general representation of the ITP with MSTE objective as follows: \\[ \\min\\limits_{w} \\ \\ \\frac{1}{2} \\cdot w^T \\times D \\times w - d^T \\times w \\] with \\[ D = R^T \\times R \\] and \\[ d = R^T \\times r_{bm} \\] 5.2.3 Example ITP This example shows the results of tracking the S&amp;P 500 with a tracking portfolio that can only invest in IBM, Apple and Google. The time frame is 2018-01-01/2019-12-31 and the goal is to minimize the variance of the difference in returns between the portfolio and the benchmark. The fitted return changes of the ITP-TEV and ITP-MSTE are: pool_returns &lt;- buffer( get_yf(tickers = c(&quot;IBM&quot;, &quot;GOOG&quot;, &quot;AAPL&quot;), from = &quot;2018-01-01&quot;, to = &quot;2019-12-31&quot;)$returns, &quot;CPI_3_assets&quot; ) bm_returns &lt;- buffer( get_yf(tickers = &quot;%5EGSPC&quot;, from = &quot;2018-01-01&quot;, to = &quot;2019-12-31&quot;)$returns, &quot;CPI_sp500&quot; ) %&gt;% setNames(., &quot;S&amp;P 500&quot;) mat &lt;- list( Dmat = cov(pool_returns), dvec = cov(pool_returns, bm_returns), Amat = t(rbind( rep(1, ncol(pool_returns)), # sum up to 1 diag(1, nrow=ncol(pool_returns), ncol=ncol(pool_returns)) # long only )), bvec = c( 1, # sum up to 1 rep(0, ncol(pool_returns)) # long only ), meq = 1 ) qp &lt;- solve.QP(Dmat = mat$Dmat, dvec = mat$dvec, Amat = mat$Amat, bvec = mat$bvec, meq = mat$meq) composition_TEV &lt;- data.frame(&quot;type&quot;=&quot;ITP-TEV&quot;, t(setNames(qp$solution, colnames(pool_returns)))) port_returns_TEV &lt;- xts(pool_returns %*% qp$solution, order.by=index(pool_returns)) %&gt;% setNames(., &quot;ITP-TEV&quot;) mat &lt;- list( Dmat = t(pool_returns) %*% pool_returns, dvec = t(pool_returns) %*% bm_returns, Amat = t(rbind( rep(1, ncol(pool_returns)), # sum up to 1 diag(1, nrow=ncol(pool_returns), ncol=ncol(pool_returns)) # long only )), bvec = c( 1, # sum up to 1 rep(0, ncol(pool_returns)) # long only ), meq = 1 ) qp &lt;- solve.QP(Dmat = mat$Dmat, dvec = mat$dvec, Amat = mat$Amat, bvec = mat$bvec, meq = mat$meq) composition_MSTE &lt;- data.frame(&quot;type&quot;=&quot;ITP-MSTE&quot;, t(setNames(qp$solution, colnames(pool_returns)))) port_returns_MSTE &lt;- xts(pool_returns %*% qp$solution, order.by=index(pool_returns)) %&gt;% setNames(., &quot;ITP-MSTE&quot;) plotly_line_chart_xts(ret_to_cumret(cbind.xts(port_returns_TEV, port_returns_MSTE, bm_returns))) %&gt;% html_save() The ITP-TEV and ITP-MSTE had almost the same results whith the compositions below: type AAPL IBM GOOG 1 ITP-TEV 0.2594763 0.4164918 0.3240319 2 ITP-MSTE 0.2588587 0.4170364 0.3241049 References "],["analytic-solver-for-quadratic-programming-problems.html", "Chapter 6 Analytic Solver for Quadratic Programming Problems 6.1 Quadratic Programming (QP) 6.2 QP Solver from quadprog 6.3 Example: Solving MVP with solve.QP() 6.4 Example: Solving ITP-MSTE with solve.QP()", " Chapter 6 Analytic Solver for Quadratic Programming Problems The advantages and disadvantages of analytical solvers for quadratic programming problems are discussed in this chapter. It is beyond the scope of this thesis to explain the underlying mathematical principles of how a solver solves quadratic problems; only the applications and analysis are discussed. The main reason for dealing with analytic solvers for quadratic programming problems is to use them as a benchmark for PSO. 6.1 Quadratic Programming (QP) A quadratic program is a minimization problem of a function that returns a scalar value and consists of a quadratic term and a linear term that depend on the variable of interest. In addition, the problem may be constrained by several linear inequalities that bound the solution. The general formulation used is to find \\(x\\) that minimizes the following problem: \\[ \\min\\limits_{x} \\ \\frac{1}{2} \\cdot x^T \\times D \\times x - d^T \\times x \\] and is valid under the linear constraints: \\[ A^T \\times x \\geq b_0 \\] Some other sources notate the problem with different signs or coefficients, all of which are interchangeable with the above problem. In addition, the above problem has the same notation used in the R package quadprog, which reduces the substitution overhead. All modern programming languages have many solvers for quadratic problems. They differ mainly in the computation time for certain problems and the requirements. Some commercial QP solvers additionally accept more complex constraints, such as absolute (e.g., \\(|A^T \\times x| \\geq a_0\\)) or mixed-integer (e.g., \\(x \\in \\mathbb{N}\\)). Especially the mixed-integer constraint problems lead to a huge increase in memory requirements. 6.2 QP Solver from quadprog The most common free QP solver used in R comes from the package quadprog, which consists of a single function called solve.QP(). Its implementation routine is the dual method of Goldfarb and Idnani published in (Goldfarb and Idnani 1982) and (Goldfarb and Idnani 1983). It uses the above QP with the condition that \\(D\\) must be a symmetric positive definite matrix. This means that \\(D\\in \\mathbb{R}^{N \\times N}\\) and \\(x^T D x &gt; 0 \\ \\forall \\ x \\in \\mathbb{R}^N\\), which is equivalent to all eigenvalues being greater than zero. In most cases this is not achieved by estimating the covariance matrix \\(\\sum\\), but it is possible to find the nearest positive definite matrix of \\(\\textstyle\\sum\\) using the function nearPD() from the matrix R package. The error encountered often does not exceed a percentage change in elements over \\(10^{-15} \\%\\), which is negligible for the context of this work. The function solve.QP() for an \\(N\\) dimensional vector of interest, has the following arguments, which are also found in the above formulation of a QP: + Dmat: Symmetric positive definite matrix \\(D \\in \\mathbb{R}^{N \\times N}\\) of the quadratic term + dvec: Vector \\(d \\in \\mathbb{R}^{N}\\) of the linear term + Amat: Constraint matrix \\(A\\) + bvec: Constraint vector \\(b_0\\) + meq = 1: means that the first row of \\(A\\) is treated as an equality constraint The return of solve.QP() is a list and contains, among others, the following attributes of interest: + solution: Vector containing the solution \\(x\\) of the quadratic programming problem (e.g. portfolio weights) + value: Scalar, the value of the quadratic function at the solution 6.3 Example: Solving MVP with solve.QP() This section provides insights into the effects of diversification and the use of solve.QP() by creating ten different efficiency frontiers from a pool of ten assets. Each efficiency frontier \\(i \\in \\{1, 2, \\cdots, 10\\}\\) consists of \\(N_i = i\\) assets and is created by adding the asset with the next smallest variance first. After loading the returns for ten of the largest stocks in the U.S. market, the variance is calculated to rank all columns in ascending order of variance, as shown in the code below: returns_raw &lt;- buffer( get_yf( tickers = c(&quot;IBM&quot;, &quot;GOOG&quot;, &quot;AAPL&quot;, &quot;MSFT&quot;, &quot;AMZN&quot;, &quot;NVDA&quot;, &quot;JPM&quot;, &quot;META&quot;, &quot;V&quot;, &quot;WMT&quot;), from = &quot;2018-01-01&quot;, to = &quot;2019-12-31&quot; )$returns, &quot;AS_10_assets&quot; ) # re-arrange: low var first vars &lt;- sapply(returns_raw, var) returns_raw &lt;- returns_raw[, order(vars, decreasing = F)] The next step is to create a function mvp() that has the arguments return and lambda. It computes the expected returns mu and the estimated positive definite covariance cov. It then solves an MVP with constraints \\(\\textstyle\\sum w_i = 1\\) and \\(w_i \\geq 0\\), which yields the key features mu, var and composition of the portfolio. mvp &lt;- function(returns, lambda){ tc &lt;- tryCatch({ mu &lt;- ret_to_geomeanret(returns) cov &lt;- as.matrix(nearPD(cov(returns))$mat) mat &lt;- list( Dmat = lambda * cov, dvec = (1-lambda) * mu, Amat = t(rbind( rep(1, ncol(returns)), # sum up to 1 diag(1, nrow=ncol(returns), ncol=ncol(returns)) # long only )), bvec = c( 1, # sum up to 1 rep(0, ncol(returns)) # long only ), meq = 1 ) qp &lt;- solve.QP( Dmat = mat$Dmat, dvec = mat$dvec, Amat = mat$Amat, bvec = mat$bvec, meq = mat$meq ) res &lt;- list( &quot;mu&quot; = mu %*% qp$solution, &quot;var&quot; = t(qp$solution) %*% cov %*% qp$solution, &quot;composition&quot; = setNames(qp$solution, colnames(returns)) ) TRUE }, error = function(e){FALSE}) if(tc){ return(res) }else{ return(list( &quot;mu&quot; = NA, &quot;var&quot; = NA, &quot;composition&quot; = NA )) } } Each \\(\\lambda \\in \\{0.01, 0.02, \\cdots, 1\\}\\) and each combination of ascending number of assets results in a portfolio that can be created with two for loops. df &lt;- data.frame( &quot;index&quot;=1, &quot;var&quot;=as.numeric(var(returns_raw[, 1])), &quot;return&quot; = as.numeric(ret_to_geomeanret(returns_raw[, 1])), row.names=NULL ) for(i in 2:ncol(returns_raw)){ returns &lt;- returns_raw[, 1:i] for(lambda in seq(0.01, 1, 0.01)){ res &lt;- mvp(returns, lambda) df &lt;- rbind( df, data.frame(&quot;index&quot;=i, &quot;var&quot;=res$var, &quot;return&quot; = res$mu) ) } } The result is filtered and names are added to represent the number of assets. Now the diagram can be created: df &lt;- df %&gt;% filter(!is.na(return)) %&gt;% distinct() %&gt;% mutate(name = paste0(&quot;n_&quot;, index)) %&gt;% arrange(name) %&gt;% mutate(name = factor(name, levels=paste0(&quot;n_&quot;, ncol(returns_raw):1))) max_show_sd &lt;- df %&gt;% group_by(index) %&gt;% summarise(max_x = max(var)) %&gt;% pull(max_x) %&gt;% mean() %&gt;% sqrt() plot_ly( data = df[df$index!=1,], x=~sqrt(var), y=~return, name=~name, mode=&quot;lines&quot;, type = &#39;scatter&#39;, color = ~name, colors = c(&quot;green&quot;, &quot;red&quot;) ) %&gt;% add_trace( data=df[df$index==1,], x=~sqrt(var), y=~return, showlegend=T, marker=list(color=&quot;red&quot;), mode=&quot;markers&quot;, name=&quot;n_1&quot;) %&gt;% layout( xaxis=list(range=c(sqrt(min(df$var))*0.9, max_show_sd), title=&quot;standard deviation&quot;), yaxis=list(range=c(min(df$return)*0.9, (max(df$return)+mean(df$return))*0.5), title=&quot;return&quot;)) %&gt;% html_save() It can be seen, that each asset added results in a minimum variance portfolio with smaller or equal standard deviation. Nevertheless, we started with the asset that has the smallest standard deviation of 0.012459. This is the effect of diversification mentioned by Markowitz. 6.4 Example: Solving ITP-MSTE with solve.QP() This example analyzes how many assets are needed to minimize the mean square error between the replication and historical returns of the S&amp;P 500 from 2018-01-01 to 2019-12-31. The constraints are set to be long only and the weights should sum to one. To gradually reduce the number of assets, the five assets with the lowest weights are discarded and serve as the new asset pool for the next replication until only five assets are left. First, the required data can be downloaded from the R/ directory using existing functions. The function get_spx_composition() uses web scraping to read the components of wikipedia and converts them into monthly compositions of the S&amp;P 500. The pool is formed from all assets present in the last month of the time frame, reduced by assets with missing values. The code below loads the returns of all assets in the pool and the S&amp;P 500: from &lt;- &quot;2018-01-01&quot; to &lt;- &quot;2019-12-31&quot; spx_composition &lt;- buffer( get_spx_composition(), &quot;AS_spx_composition&quot; ) pool_returns_raw &lt;- buffer( get_yf( tickers = spx_composition %&gt;% filter(Date&lt;=to) %&gt;% filter(Date==max(Date)) %&gt;% pull(Ticker), from = from, to = to )$returns, &quot;AS_sp500_assets&quot; ) pool_returns_raw &lt;- pool_returns_raw[, colSums(is.na(pool_returns_raw))==0] bm_returns &lt;- buffer( get_yf(tickers = &quot;%5EGSPC&quot;, from = from, to = to)$returns, &quot;AS_sp500&quot; ) %&gt;% setNames(., &quot;S&amp;P 500&quot;) The required data is now available and the function for the ITP-MSTE can be created. It requires pool_returns with variable number of columns and the single-column matrix bm_returns. itp &lt;- function(pool_returns, bm_returns){ mat &lt;- list( Dmat = t(pool_returns) %*% pool_returns, dvec = t(pool_returns) %*% bm_returns, Amat = t(rbind( rep(1, ncol(pool_returns)), # sum up to 1 diag(1, nrow=ncol(pool_returns), ncol=ncol(pool_returns)) # long only )), bvec = c( 1, # sum up to 1 rep(0, ncol(pool_returns)) # long only ), meq = 1 ) qp &lt;- solve.QP( Dmat = mat$Dmat, dvec = mat$dvec, Amat = mat$Amat, bvec = mat$bvec, meq = mat$meq ) res &lt;- list( &quot;var&quot; = as.numeric( var(pool_returns %*% qp$solution - bm_returns)), &quot;solution&quot; = setNames(qp$solution, colnames(pool_returns)) ) } The duplication and successive discarding of assets can begin. The results are stored in res and used to display the results. res &lt;- NULL n_assets &lt;- rev(seq(5, ncol(pool_returns_raw), 5)) for(i in n_assets){ temp &lt;- if(i==max(n_assets)){ itp(pool_returns_raw, bm_returns) }else{ itp( pool_returns_raw[, names(sort(temp$solution, decreasing = T)[1:i])], bm_returns ) } res &lt;- rbind( res, data.frame(&quot;N&quot;=i, &quot;var&quot;=temp$var, &quot;sd&quot;=sqrt(temp$var), row.names = NULL) ) } plot_ly(data=res, x=~N, y=~sd, mode=&quot;lines&quot;, type = &#39;scatter&#39;) %&gt;% layout(yaxis=list(range=c(0, mean(max(res$sd),mean(res$sd)) ))) %&gt;% html_save() It can be seen that the standard deviation stagnates at about \\(N=100\\). This leads to the conclusion that a sparse replication with one hundred assets is sufficient in this particular case to track the historical performance of the S&amp;P 500 over this period. References "],["particle-swarm-optimization-pso.html", "Chapter 7 Particle Swarm Optimization (PSO) 7.1 The Algorithm 7.2 pso() Function 7.3 Animation 2-Dimensional 7.4 Example MVP 7.5 Example: ITP-MSTE 7.6 Pros and Cons for Continuous Problems 7.7 Discrete Problems and Transaction Costs 7.8 Example: Discrete ITP-MSTE", " Chapter 7 Particle Swarm Optimization (PSO) The PSO was developed by J. Kennedy as a global optimization method based on swarm intelligence and presented to the public in 1995 by Eberhart and Kennedy (James Kennedy 1995). The original PSO was intended to resemble a flock of birds flying through the sky without collisions. Therefore, its first applications were found in particle physics to analyze moving particles in high-dimensional spaces, which the name Particle recalls. Later, it was adapted in Evolutionary Computation to exploit a set of potential solutions in high dimensions and to find the optima by cooperating with other particles in the swarm (Konstantinos Parsopoulos 2002). Since it does not require gradient information, it is easier to apply than other global optimization methods. It can find the optimum by considering only the result of the function to be optimized. This means that the function can be arbitrarily complex and it is still possible to reach the global optimum. Other advantages are the low computational costs, since only basic mathematical operators are used. 7.1 The Algorithm Each particle \\(d\\) with position \\(x_d\\) moves in the search space \\(\\mathbb{R}^N\\) and has its own velocity \\(v_d\\) and remembers its previous best position \\(P_d\\). After each iteration, the velocity changes in the direction of the intrinsic velocity, the best previous position, and the global best position \\(p_g\\) of all particles. A position change from \\(i\\) to \\(i+1\\) can be calculated by the following two equations (Konstantinos Parsopoulos 2002): \\[\\begin{align*} v_d^{i+1} &amp;= wv_d^{i} + c_p r_1^i (P_d^i - x_d^i) + c_g r_2^i (p_g^i - x_d^i) \\\\ x_d^{i+1} &amp;= x_d^i + v_d^{i+1} \\end{align*}\\] Where \\(r_1\\) and \\(r_2\\) are uniformly distributed random numbers in [0, 1]. The cognitive parameter \\(c_p\\) acts as a weighting of the direction to its previous best position of the particle. This contrasts with the social parameter \\(c_g\\), which is a weighting of the direction to the global best position. The inertial weight \\(w\\) is crucial for the convergence behavior by remembering part of its previous trajectory. A study reviewed in (Konstantinos Parsopoulos 2002) showed that these parameters can be set to \\(c_p=c_g=0.5\\) and \\(w\\) should decrease from \\(1.2\\) to \\(0\\). However, some problems benefit from a more precise tuning of these parameters. To allow effortless translation to code, the above formula for \\(d = 1, 2, \\cdots, D\\) particles can be given in the following matrix notation: \\[\\begin{align*} V^{i+1} &amp;= w \\cdot V^{i} + c_p \\cdot r_1^i \\cdot (P^i-X^i) + c_g \\cdot r_2^i \\cdot (p_g^i - X^i) \\\\ X^{i+1} &amp;= X^i + V^{i+1} \\end{align*}\\] With current positions \\(X \\in \\mathbb{R}^{N \\times D}\\), current velocities \\(V \\in \\mathbb{R}^{N \\times D}\\), previous best positions \\(P \\in \\mathbb{R}^{N \\times D}\\), and global best position \\(p_g \\in \\mathbb{R}^{N}\\). The parameters \\(w\\), \\(c_p\\) and \\(c_g\\) are stile scalars. The random numbers are transformed into vectors \\(r_1\\) and \\(r_2\\), which contain uniformly distributed random numbers that are multiplied element-wise. 7.2 pso() Function In this section, a general PSO function is created that follows the structure of other optimization heuristics in R, in particular the existing PSO implementation from the R package pso. The key component of the problem is a objective function called fn(), which returns a scalar that needs to be minimized. The function itself mainly needs a vector pos that describes the position of a particle (e.g. weights). The other main parameters for the PSO function are par, which is a position of a particle used to derive the dimension of the problem and used as the initial position of one particle. The argument can only contain NAs, resulting in completely random starting positions. The last two arguments are lower and upper bounds (e.g. weights greater than 0 and less than 1). All other parameters have default values that can be overridden by passing a list called control. The resulting structure is: pso &lt;- function( par, fn, lower, upper, control = list() ){ } Before the main data structure can be initialized, some sample inputs must be created for the pso() function as described below: par &lt;- rep(NA, 2) fn &lt;- function(x){return(sum(abs(x)))} lower &lt;- -10 upper &lt;- 10 control = list( s = 10, # swarm size c.p = 0.5, # inherit best c.g = 0.5, # global best maxiter = 100, # iterations w0 = 1.2, # starting inertia weight wN = 0, # ending inertia weight save_traces = F # save more information ) Now it is time to initialize the random positions X, their fitness X_fit and their random velocities V with the function mrunif() which produces a matrix of uniformly distributed random numbers between lower and upper: X &lt;- mrunif( nr = length(par), nc=control$s, lower=lower, upper=upper ) if(all(!is.na(par))){ X[, 1] &lt;- par } X_fit &lt;- apply(X, 2, fn) V &lt;- mrunif( nr = length(par), nc=control$s, lower=-(upper-lower), upper=(upper-lower) )/4 The velocities are compressed by a factor of 4 to start with a maximum movement of one quarter of the space in each axis. The personal best positions P are the same as X and the global best position is the position with the smallest fitness: P &lt;- X P_fit &lt;- X_fit p_g &lt;- P[, which.min(P_fit)] p_g_fit &lt;- min(P_fit) The required data structure is available and the optimization can start with the calculation of the new velocities and the transformation of the old positions. When particles have left the valid space, they are pushed back to the edge and the velocities are set to zero. Then the fitness is calculated and the personal best and global best positions are saved if they have improved. trace_data &lt;- NULL for(i in 1:control$maxiter){ # move particles V &lt;- (control$w0-(control$w0-control$wN)*i/control$maxiter) * V + control$c.p * runif(length(par)) * (P-X) + control$c.g * runif(length(par)) * (p_g-X) X &lt;- X + V # set velocity to zeros if not in valid space V[X &gt; upper] &lt;- 0 V[X &lt; lower] &lt;- 0 # move into valid space X[X &gt; upper] &lt;- upper X[X &lt; lower] &lt;- lower # evaluate objective function X_fit &lt;- apply(X, 2, fn) # save new previews best P[, P_fit &gt; X_fit] &lt;- X[, P_fit &gt; X_fit] P_fit[P_fit &gt; X_fit] &lt;- X_fit[P_fit &gt; X_fit] # save new global best if(any(P_fit &lt; p_g_fit)){ p_g &lt;- P[, which.min(P_fit)] p_g_fit &lt;- min(P_fit) } if(control$save_traces==TRUE){ trace_data &lt;- rbind(trace_data, data.frame(&quot;iter&quot;=i, t(X))) } } The best fitness after \\(100\\) iterations is 0.0001256 and the best possible solution is \\(0\\). # the resulting pso() function pso &lt;- function( par, fn, lower, upper, control = list() ){ # use default control values if not set control_ = list( s = 10, # swarm size c.p = 0.5, # inherit best c.g = 0.5, # global best maxiter = 200, # iterations w0 = 1.2, # starting inertia weight wN = 0, # ending inertia weight save_traces = F # save more information ) control &lt;- c(control, control_[!names(control_) %in% names(control)]) # init data-structure X &lt;- mrunif( nr = length(par), nc=control$s, lower=lower, upper=upper ) if(all(!is.na(par))){ X[, 1] &lt;- par } X_fit &lt;- apply(X, 2, fn) V &lt;- mrunif( nr = length(par), nc=control$s, lower=-(upper-lower), upper=(upper-lower) )/4 P &lt;- X P_fit &lt;- X_fit p_g &lt;- P[, which.min(P_fit)] p_g_fit &lt;- min(P_fit) trace_data &lt;- NULL for(i in 1:control$maxiter){ # move particles V &lt;- (control$w0-(control$w0-control$wN)*i/control$maxiter) * V + control$c.p * runif(length(par)) * (P-X) + control$c.g * runif(length(par)) * (p_g-X) X &lt;- X + V # set velocity to zeros if not in valid space V[X &gt; upper] &lt;- -V[X &gt; upper] V[X &lt; lower] &lt;- -V[X &lt; lower] # move into valid space X[X &gt; upper] &lt;- upper X[X &lt; lower] &lt;- lower # evaluate objective function X_fit &lt;- apply(X, 2, fn) # save new previews best P[, P_fit &gt; X_fit] &lt;- X[, P_fit &gt; X_fit] P_fit[P_fit &gt; X_fit] &lt;- X_fit[P_fit &gt; X_fit] # save new global best if(any(P_fit &lt; p_g_fit)){ p_g &lt;- P[, which.min(P_fit)] p_g_fit &lt;- min(P_fit) } if(control$save_traces){ trace_data &lt;- rbind(trace_data, data.frame(&quot;iter&quot;=i, t(X))) } } res &lt;- list( &quot;solution&quot; = p_g, &quot;fitness&quot; = p_g_fit ) if(control$save_traces){ res$trace_data &lt;- trace_data } return(res) } 7.3 Animation 2-Dimensional This section provides insights into the behavior of the PSO by visualizing multiple iterations in a GIF. The GIF only works in Adobe Acrobat DC or in the Markdown/HTML version of this thesis. The amazing animation template is inspired by Rtichoke. The PSO core from the above chapter was used to complete the pso() function and is tested here with seed 0. The function fn to be evaluated can be found in Rtichoke. set.seed(0) fn &lt;- function(pos){ -20 * exp(-0.2 * sqrt(0.5 *((pos[1]-1)^2 + (pos[2]-1)^2))) - exp(0.5*(cos(2*pi*pos[1]) + cos(2*pi*pos[2]))) + exp(1) + 20 } res &lt;- pso( par = rep(NA, 2), fn = fn, lower = -10, upper = 10, control = list( s = 10, maxiter = 30, w0 = 0.8, save_traces = T ) ) The function fn has many local minima and a global minima at \\((1,1)\\) with the value \\(0\\). The background color scale ranges from 0 as red to 20 as purple. The PSO has 10 particles, iterated 30 times with an inertia weight decreasing from 0.8 to 0. The iterations are visualized in the following GIF: 7.4 Example MVP This example uses the solve.QP() approach from 6.3 with ten assets as the benchmark. Briefly, the goal is to create an MVP from ten of the largest U.S. stocks between 2018-01-01 and 2019-12-31 for each possible \\(\\lambda\\). The PSO has 300 particles and 200 iterations for each lambda. The main characteristics of all portfolios created with the solve.QP() compared to the PSO are shown below: set.seed(0) returns_raw &lt;- buffer( get_yf( tickers = c(&quot;IBM&quot;, &quot;GOOG&quot;, &quot;AAPL&quot;, &quot;MSFT&quot;, &quot;AMZN&quot;, &quot;NVDA&quot;, &quot;JPM&quot;, &quot;META&quot;, &quot;V&quot;, &quot;WMT&quot;), from = &quot;2018-01-01&quot;, to = &quot;2019-12-31&quot; )$returns, &quot;AS_10_assets&quot; ) # re-arrange: low var first vars &lt;- sapply(returns_raw, var) returns_raw &lt;- returns_raw[, order(vars, decreasing = F)] mvp_QP &lt;- function(returns, lambda){ tc &lt;- tryCatch({ mu &lt;- ret_to_geomeanret(returns) cov &lt;- as.matrix(nearPD(cov(returns))$mat) mat &lt;- list( Dmat = lambda * cov, dvec = (1-lambda) * mu, Amat = t(rbind( rep(1, ncol(returns)), # sum up to 1 diag(1, nrow=ncol(returns), ncol=ncol(returns)) # long only )), bvec = c( 1, # sum up to 1 rep(0, ncol(returns)) # long only ), meq = 1 ) qp &lt;- solve.QP( Dmat = mat$Dmat, dvec = mat$dvec, Amat = mat$Amat, bvec = mat$bvec, meq = mat$meq ) res &lt;- list( &quot;mu&quot; = mu %*% qp$solution, &quot;var&quot; = t(qp$solution) %*% cov %*% qp$solution, &quot;composition&quot; = setNames(qp$solution, colnames(returns)) ) TRUE }, error = function(e){FALSE}) if(tc){ return(res) }else{ return(list( &quot;mu&quot; = NA, &quot;var&quot; = NA, &quot;composition&quot; = NA )) } } mvp_PSO &lt;- function(returns, lambda, silent = T){ tc &lt;- tryCatch({ mu &lt;- ret_to_geomeanret(returns) cov &lt;- as.matrix(nearPD(cov(returns))$mat) mat &lt;- list( Dmat = lambda * cov, dvec = (1-lambda) * mu, Amat = t(rbind( rep(1, ncol(returns)), # sum up to 1 diag(1, nrow=ncol(returns), ncol=ncol(returns)) # long only )), bvec = c( 1, # sum up to 1 rep(0, ncol(returns)) # long only ), meq = 1 ) calc_fit &lt;- function(x){ 0.5 * t(x) %*% mat$Dmat %*% x - t(mat$dvec) %*% x } calc_const &lt;- function(x){ const &lt;- t(mat$Amat) %*% x - mat$bvec const[mat$meq] &lt;- -max(0, abs(const[mat$meq]+0.01)-0.01) -min(0, const) } pso_res &lt;- pso( par = rep(NA, ncol(returns)), fn = function(x){ fitness &lt;- calc_fit(x) constraints &lt;- calc_const(x) return(fitness + 10 * constraints) }, lower = 0, upper = 1, control = list( s = 300, # swarm size c.p = 0.5, # inherit best c.g = 0.5, # global best maxiter = 200, # iterations w0 = 1.2, # starting inertia weight wN = 0, # ending inertia weight save_traces = F # save more information ) ) if(!silent){ p0(&quot;constraint: &quot;, sum(calc_const(pso_res$solution))) p0(&quot;fitness: &quot;, calc_fit(pso_res$solution)) } res &lt;- list( &quot;mu&quot; = mu %*% pso_res$solution, &quot;var&quot; = t(pso_res$solution) %*% cov %*% pso_res$solution, &quot;composition&quot; = setNames(pso_res$solution, colnames(returns)), &quot;fit&quot; = calc_fit(pso_res$solution), &quot;constraint&quot; = sum(calc_const(pso_res$solution)) ) TRUE }, error = function(e){FALSE}) if(tc){ return(res) }else{ return(list( &quot;mu&quot; = NA, &quot;var&quot; = NA, &quot;composition&quot; = NA )) } } df &lt;- NULL runs &lt;- 100 for(i in 0:round((runs-1)/2)){ lambda &lt;- 1-i/runs temp &lt;- mvp_QP(returns = returns_raw, lambda = lambda) df &lt;- rbind(df, data.frame(&quot;type&quot;=&quot;QP_MVP&quot;, &quot;lambda&quot;=lambda, &quot;mu&quot;=temp$mu, &quot;var&quot;=temp$var, &quot;constraint&quot;=0)) temp &lt;- mvp_PSO(returns = returns_raw, lambda = lambda) df &lt;- rbind(df, data.frame(&quot;type&quot;=&quot;PSO_MVP&quot;, &quot;lambda&quot;=lambda, &quot;mu&quot;=temp$mu, &quot;var&quot;=temp$var, &quot;constraint&quot;=temp$constraint)) } df$sd = sqrt(df$var) df_qp &lt;- df[df$type==&quot;QP_MVP&quot; &amp; df$lambda %in% seq(1,0.8,-0.1),] df_diff &lt;- data.frame( &quot;mu_pso&quot; = df[df$type==&quot;PSO_MVP&quot;,]$mu, &quot;sd_pso&quot; = df[df$type==&quot;PSO_MVP&quot;,]$sd, &quot;mu_qp&quot; = df[df$type==&quot;QP_MVP&quot;,]$mu, &quot;sd_qp&quot; = df[df$type==&quot;QP_MVP&quot;,]$sd ) shapes &lt;- list() for(i in 1:nrow(df_diff)){ shapes[[i]] &lt;- list( type = &quot;line&quot;, y0 = df_diff[i,]$mu_pso, y1 = df_diff[i,]$mu_qp, yref = &quot;y&quot;, xref = &quot;x&quot;, x0 = df_diff[i,]$sd_pso, x1 = df_diff[i,]$sd_qp, line = list(color = &quot;lightgrey&quot;), layer=&#39;below&#39; ) } plot_ly( data = df, x=~sd, y=~mu, name=~type, mode=&quot;markers&quot;, type = &#39;scatter&#39;, color = ~type, colors = c(&quot;green&quot;, &quot;red&quot;) ) %&gt;% add_annotations( data=df_qp, x=~sd, y=~mu, text = ~paste0(&quot;lambda: &quot;,lambda), ay = -30, ax = -30 ) %&gt;% # add_segments( # #data=df_diff, # x=df_diff$sd_pso, # y=df_diff$mu_pso, # yend = df_diff$sd_diff, # xend = df_diff$mu_diff # ) %&gt;% layout( xaxis=list(range=c(0.5*min(df$sd), 1.2*max(df[df$type==&quot;QP_MVP&quot;,]$sd)), showgrid = FALSE), yaxis=list(range=c(0.5*min(df$mu), 1.2*max(df[df$type==&quot;QP_MVP&quot;,]$mu)), showgrid = FALSE), shapes = shapes, legend = list(x = 0.1, y = 0.9) ) %&gt;% html_save() The dots for each \\(\\lambda\\) are connected with a grey line to visualize the error of the PSO. It turns out that it is possible to solve MVPs with a PSO approach. It is noticeable that some PSO runs are trapped in local minimas and thus show a deviation from the solve.QP() approach, which can often be fixed by repeated runs. 7.5 Example: ITP-MSTE The same ITP-MSTE solved with solve.QP() in 6.4 is used as the benchmark for the PSO. In summary, the goal is to create a portfolio that minimizes the mean square error of the returns of itself and the S&amp;P 500 between 2018-01-01 and 2019-12-31. The pool of assets includes all assets that are present in 2019-12-31 and have no missing values. The constraints are long only and the weights should sum to one. The parameters for the PSO are a swarm size of 100, 100 iterations, the inertia weight starts at \\(1.2\\), the upper bound is \\(0.05\\), and a starting position is the zero vector. The PSO was run ten times, and the aggregated best and mean runs are compared to the solve.QP() approach for seed 0 in the table below: set.seed(0) from &lt;- &quot;2018-01-01&quot; to &lt;- &quot;2019-12-31&quot; spx_composition &lt;- buffer( get_spx_composition(), &quot;AS_spx_composition&quot; ) pool_returns_raw &lt;- buffer( get_yf( tickers = spx_composition %&gt;% filter(Date&lt;=to) %&gt;% filter(Date==max(Date)) %&gt;% pull(Ticker), from = from, to = to )$returns, &quot;AS_sp500_assets&quot; ) pool_returns_raw &lt;- pool_returns_raw[, colSums(is.na(pool_returns_raw))==0] bm_returns &lt;- buffer( get_yf(tickers = &quot;%5EGSPC&quot;, from = from, to = to)$returns, &quot;AS_sp500&quot; ) %&gt;% setNames(., &quot;S&amp;P 500&quot;) itp_QP &lt;- function(pool_returns, bm_returns){ mat &lt;- list( Dmat = t(pool_returns) %*% pool_returns, dvec = t(pool_returns) %*% bm_returns, Amat = t(rbind( rep(1, ncol(pool_returns)), # sum up to 1 diag(1, nrow=ncol(pool_returns), ncol=ncol(pool_returns)) # long only )), bvec = c( 1, # sum up to 1 rep(0, ncol(pool_returns)) # long only ), meq = 1 ) qp &lt;- solve.QP( Dmat = mat$Dmat, dvec = mat$dvec, Amat = mat$Amat, bvec = mat$bvec, meq = mat$meq ) res &lt;- list( &quot;value&quot; = qp$value, &quot;var&quot; = as.numeric( var(pool_returns %*% qp$solution - bm_returns)), &quot;solution&quot; = setNames(qp$solution, colnames(pool_returns)) ) } itp_PSO &lt;- function(pool_returns, bm_returns, silent = T){ mat &lt;- list( Dmat = t(pool_returns) %*% pool_returns, dvec = t(pool_returns) %*% bm_returns, Amat = t(rbind( rep(1, ncol(pool_returns)), # sum up to 1 diag(1, nrow=ncol(pool_returns), ncol=ncol(pool_returns)) # long only )), bvec = c( 1, # sum up to 1 rep(0, ncol(pool_returns)) # long only ), meq = 1 ) calc_fit &lt;- function(x){ as.numeric(0.5 * t(x) %*% mat$Dmat %*% x - t(mat$dvec) %*% x) } calc_const &lt;- function(x){ const &lt;- t(mat$Amat) %*% x - mat$bvec const[mat$meq] &lt;- -max(0, abs(const[mat$meq]+0.01)-0.01) #-sum(pmin(0, const)) -min(0, const) } pso_res &lt;- pso( par = rep(0, ncol(pool_returns)), fn = function(x){ fitness &lt;- calc_fit(x) constraints &lt;- calc_const(x) return(fitness+10*constraints) }, lower = 0, upper = 0.05, control = list( s = 100, # swarm size c.p = 0.5, # inherit best c.g = 0.5, # global best maxiter = 100, # iterations w0 = 1.2, # starting inertia weight wN = 0, # ending inertia weight save_traces = F # save more information ) ) if(!silent){ p0(&quot;constraint: &quot;, sum(calc_const(pso_res$solution))) p0(&quot;fitness: &quot;, calc_fit(pso_res$solution)) } res &lt;- list( &quot;composition&quot; = setNames(pso_res$solution, colnames(pool_returns)), &quot;fit&quot; = calc_fit(pso_res$solution), &quot;constraint&quot; = calc_const(pso_res$solution), &quot;var&quot; = as.numeric( var(pool_returns %*% pso_res$solution - bm_returns)) ) return(res) } df &lt;- NULL time &lt;- system.time( temp_qp &lt;- itp_QP(pool_returns_raw, bm_returns) ) df &lt;- data.frame(&quot;type&quot;=&quot;ITP-MSTE_QP&quot;, &quot;var&quot;=temp_qp$var, &quot;fitness&quot;=temp_qp$value, &quot;constraint&quot;=0, &quot;time&quot;=time[3]) for(i in 1:10){ time &lt;- system.time( temp_pso &lt;- itp_PSO(pool_returns = pool_returns_raw, bm_returns) ) df &lt;- rbind( df, data.frame( &quot;type&quot;=&quot;ITP-MSTE_PSO&quot;, &quot;var&quot;=temp_pso$var, &quot;fitness&quot;=temp_pso$fit, &quot;constraint&quot;=temp_pso$constraint, &quot;time&quot;=time[3] ) ) } df$sd &lt;- sqrt(df$var) df_summary &lt;- rbind( df %&gt;% filter(type == &quot;ITP-MSTE_QP&quot;), df %&gt;% filter(type == &quot;ITP-MSTE_PSO&quot;) %&gt;% filter(fitness == min(fitness)) %&gt;% mutate(type = &quot;ITP-MSTE_PSO_best&quot;), df %&gt;% filter(type == &quot;ITP-MSTE_PSO&quot;) %&gt;% group_by(type) %&gt;% summarise_all(., mean) %&gt;% mutate(type = &quot;ITP-MSTE_PSO_mean&quot;) ) rownames(df_summary) &lt;- NULL df_summary$var &lt;- round(df_summary$var, 12) df_summary$sd &lt;- round(df_summary$sd, 6) df_summary$fitness &lt;- round(df_summary$fitness, 7) df_summary$constraint &lt;- round(df_summary$constraint, 18) df_summary$time &lt;- round(df_summary$time, 1) reactable( df_summary %&gt;% select(type, sd, var, fitness, constraint, time), wrap = FALSE, #compact = T, columns = list( type = colDef(width=180), var = colDef(format = colFormat(digits=9), show =F), sd = colDef(format = colFormat(digits=5)), constraint = colDef(name = &quot;constraint break&quot;) )#, #theme = reactableTheme(style = list(fontFamily = &quot;-apple-system, BlinkMacSystemFont, Segoe UI, Helvetica, Arial, sans-serif&quot;)) ) %&gt;% html_save(., vwidth = 800) It can be seen that in all PSO runs, sufficient fitness was achieved with negligible constraint breaks, but much more computation time was required. 7.6 Pros and Cons for Continuous Problems A PSO approach has advantages and disadvantages, since on the one hand any problem can theoretically be solved, but it cannot be guaranteed that the solution is also optimal. In addition, the calculations take much longer than with the solve.QP() approach, which raises the question why a PSO approach should have any benefit at all. This is exactly the case, if the solution of the problem is no longer possible by the solve.QP() alone, as it is for example the case with mixed-integer-quadratic-problems. In these types of problems, the condition for \\(x\\) is to be a integer vector. These problems could be solved by the solve.QP() approach only continuously and then rounded. However, this rounding error can become arbitrarily large, which is why the chances of the PSO approach to achieve a better solution are greater than with the solve.QP() approach. 7.7 Discrete Problems and Transaction Costs A continuous solution for a portfolio is not sufficient for practical purposes, since usually only integer amounts of assets can be purchased. Its even worse if lot sizes are needed, because these can only be bought in minimum denomination of e.g. ten thousand. Lot sizes are often used in fixed income products. The biggest drawbacks of rounding a continuous solution are the disregarding of conditions and the difference in the objective value, which often cant reach the new optimum. A solution with broken conditions is not acceptable in practice and a solve.QP() approach only produces one solution, which is why its insecure to hope for a sufficient solution after rounding. The PSO doesnt have these drawbacks and can be easily used for discrete problems by rounding the input of the objective function fn(). In a portfolio with net asset value (nav) consisting of only American stocks with weights \\(w_i\\) and closing prices \\(p_i\\) can be discretized to \\(w_i^d\\) by the following formula: \\[ w_i^d =\\text{round}(w_i \\cdot \\frac{\\text{nav}}{p_i})\\cdot \\frac{p_i}{\\text{nav}} \\] 7.8 Example: Discrete ITP-MSTE This example analyses the error of rounding a solution with the solve.QP() approach and compares it to a discrete PSO. A second discrete PSO is added, that takes the continuous solution of the solve.QP() and uses it as starting position of one particle. The ITP-MSTE focuses to track the S&amp;P 500 with its top 100 weighted assets and tries to construct a portfolio with long only, \\(1 \\leq \\textstyle\\sum w_i \\geq 0.99\\) and \\(\\text{nav} = 10000\\) in the time frame from 2018-01-01 to 2019-12-31. The used prices are closing prices and both PSOs have 200 particles and 100 iterations. The results can be observed in the table below: set.seed(0) nav &lt;- 10000 from &lt;- &quot;2018-01-01&quot; to &lt;- &quot;2019-12-31&quot; spx_composition &lt;- buffer( get_spx_composition(), &quot;AS_spx_composition&quot; ) pool_data &lt;- buffer( get_yf( tickers = spx_composition %&gt;% filter(Date&lt;=to) %&gt;% filter(Date==max(Date)) %&gt;% pull(Ticker), from = from, to = to ), &quot;AS_sp500_asset_data&quot; ) pool_data$returns &lt;- pool_data$returns[, colSums(is.na(pool_data$returns))==0] pool_data$prices &lt;- pool_data$prices[, colnames(pool_data$returns)] bm_returns &lt;- buffer( get_yf(tickers = &quot;%5EGSPC&quot;, from = from, to = to)$returns, &quot;AS_sp500&quot; ) %&gt;% setNames(., &quot;S&amp;P 500&quot;) pool_returns &lt;- pool_data$returns mat &lt;- list( Dmat = t(pool_returns) %*% pool_returns, dvec = t(pool_returns) %*% bm_returns, Amat = t(rbind( rep(1, ncol(pool_returns)), # sum up to 1 diag(1, nrow=ncol(pool_returns), ncol=ncol(pool_returns)) # long only )), bvec = c( 1, # sum up to 1 rep(0, ncol(pool_returns)) # long only ), meq = 1 ) # search 100 best tickers qp &lt;- solve.QP( Dmat = mat$Dmat, dvec = mat$dvec, Amat = mat$Amat, bvec = mat$bvec, meq = mat$meq ) sub_ticker &lt;- colnames(pool_returns)[order(qp$solution, decreasing = T)[1:100]] # Default solve.QP time_qp &lt;- system.time({ res_qp &lt;- itp_QP(pool_data$returns[,sub_ticker], bm_returns) prices &lt;- last(pool_data$prices[,sub_ticker]) res_qp_discrete &lt;- setNames(as.vector(round(res_qp$solution*nav/prices)*prices/nav), names(res_qp$solution)) }) res_qp_discrete_fit &lt;- as.numeric(0.5 * t(res_qp_discrete) %*% mat$Dmat[names(res_qp_discrete), names(res_qp_discrete)] %*% res_qp_discrete - t(as.vector(mat$dvec[names(res_qp_discrete), ])) %*% res_qp_discrete) res_qp_discrete_sum_wgt &lt;- sum(res_qp_discrete) # Default PSO pool_returns &lt;- pool_data$returns[, sub_ticker] mat &lt;- list( Dmat = t(pool_returns) %*% pool_returns, dvec = t(pool_returns) %*% bm_returns, Amat = t(rbind( rep(1, ncol(pool_returns)), # sum up to 1 diag(1, nrow=ncol(pool_returns), ncol=ncol(pool_returns)) # long only )), bvec = c( 1, # sum up to 1 rep(0, ncol(pool_returns)) # long only ), meq = 1 ) calc_fit &lt;- function(x){ as.numeric(0.5 * t(x) %*% mat$Dmat %*% x - t(mat$dvec) %*% x) } calc_const &lt;- function(x){ const &lt;- t(mat$Amat) %*% x - mat$bvec const[mat$meq] &lt;- -max(0, abs(const[mat$meq]+0.01)-0.01) -min(0, const) } time_pso &lt;- system.time({ pso_res &lt;- pso( par = rep(0, ncol(pool_returns)), fn = function(x){ x &lt;- as.vector(round(x*nav/prices)*prices/nav) fitness &lt;- calc_fit(x) constraints &lt;- calc_const(x) return(fitness + 10*constraints) }, lower = 0, upper = 1, control = list( s = 200, # swarm size c.p = 0.5, # inherit best c.g = 0.5, # global best maxiter = 100, # iterations w0 = 1.2, # starting inertia weight wN = 0, # ending inertia weight save_traces = F # save more information ) ) }) pso_res$solution &lt;- setNames(as.vector(round(pso_res$solution*nav/prices)*prices/nav), names(res_qp$solution)) res_pso_fit &lt;- as.numeric(0.5 * t(pso_res$solution) %*% mat$Dmat[names(pso_res$solution), names(pso_res$solution)] %*% pso_res$solution - t(as.vector(mat$dvec[names(pso_res$solution),])) %*% pso_res$solution) # PSO with solve.QP starting position time_pso_2 &lt;- system.time({ pso_2_res &lt;- pso( par = res_qp$solution, fn = function(x){ x &lt;- as.vector(round(x*nav/prices)*prices/nav) fitness &lt;- calc_fit(x) constraints &lt;- calc_const(x) return(fitness + 10*constraints) }, lower = 0, upper = 1, control = list( s = 200, # swarm size c.p = 0.5, # inherit best c.g = 0.5, # global best maxiter = 100, # iterations w0 = 1.2, # starting inertia weight wN = 0, # ending inertia weight save_traces = F # save more information ) ) }) pso_2_res$solution &lt;- setNames(as.vector(round(pso_2_res$solution*nav/prices)*prices/nav), names(res_qp$solution)) res_pso_2_fit &lt;- as.numeric(0.5 * t(pso_2_res$solution) %*% mat$Dmat[names(pso_2_res$solution), names(pso_2_res$solution)] %*% pso_2_res$solution - t(as.vector(mat$dvec[names(pso_2_res$solution),])) %*% pso_2_res$solution) reactable( data.frame( &quot;type&quot; = c(&quot;solve.QP discrete&quot;, &quot;PSO&quot;, &quot;PSO with solve.QP as init solution&quot;), &quot;fitness&quot; = c(res_qp_discrete_fit, res_pso_fit, res_pso_2_fit), &quot;sum_wgt&quot; = c(res_qp_discrete_sum_wgt, sum(pso_res$solution), sum(pso_2_res$solution)), &quot;time&quot; = c(time_qp[3], time_pso[3], time_pso_2[3]) ), columns = list( fitness = colDef(format = colFormat(digits=9)), sum_wgt = colDef(format = colFormat(digits=3)), time = colDef(format = colFormat(digits=3)) ) ) %&gt;% html_save() It can be seen that the rounded solve.QP() solution still has a good fitness but the constraints are not satisfied. The PSO has no constrain breaks and still reached a fitness close to the rounded solve.QP(). The PSO with solve.QP() solution as starting position has beaten both approaches. This indicates that a hybrid approach consisting of both the solve.QP() and afterwards the PSO for intelligent rounding with observed constraints would be a good heuristic for problems in practice. References "],["pso-variations.html", "Chapter 8 PSO Variations 8.1 Function Stretching", " Chapter 8 PSO Variations The standard PSO analysed in the previews chapter is capable to solve a wide range of problems but is very slow and often gets stuck in local minima. This chapter analyses different variations of the standard PSO in finance related problems. The first variant is the PSO with functional stretching, which reduces stagnation in local minimas.  8.1 Function Stretching pso &lt;- function( par, fn, lower, upper, control = list() ){ # use default control values if not set control_ = list( s = 10, # swarm size c.p = 0.5, # inherit best c.g = 0.5, # global best maxiter = 200, # iterations w0 = 1.2, # starting inertia weight wN = 0, # ending inertia weight save_traces = F, # save more information fn_stretching = F ) control &lt;- c(control, control_[!names(control_) %in% names(control)]) fn1 &lt;- function(pos){fn(pos)} # init data-structure X &lt;- mrunif( nr = length(par), nc=control$s, lower=lower, upper=upper ) if(all(!is.na(par))){ X[, 1] &lt;- par } X_fit &lt;- apply(X, 2, fn1) V &lt;- mrunif( nr = length(par), nc=control$s, lower=-(upper-lower), upper=(upper-lower) )/4 P &lt;- X P_fit &lt;- X_fit p_g &lt;- P[, which.min(P_fit)] p_g_fit &lt;- min(P_fit) trace_data &lt;- NULL for(i in 1:control$maxiter){ # move particles V &lt;- (control$w0-(control$w0-control$wN)*i/control$maxiter) * V + control$c.p * runif(length(par)) * (P-X) + control$c.g * runif(length(par)) * (p_g-X) X &lt;- X + V # set velocity to zeros if not in valid space V[X &gt; upper] &lt;- -V[X &gt; upper] V[X &lt; lower] &lt;- -V[X &lt; lower] # move into valid space X[X &gt; upper] &lt;- upper X[X &lt; lower] &lt;- lower # evaluate objective function X_fit &lt;- apply(X, 2, fn1) # save new previews best P[, P_fit &gt; X_fit] &lt;- X[, P_fit &gt; X_fit] P_fit[P_fit &gt; X_fit] &lt;- X_fit[P_fit &gt; X_fit] # save new global best if(any(P_fit &lt; p_g_fit)){ p_g &lt;- P[, which.min(P_fit)] p_g_fit &lt;- min(P_fit) } if(control$fn_stretching){ fn1 &lt;- function(pos){ res &lt;- fn(pos) G &lt;- res + 1.5 * sqrt(sum((pos - p_g)^2)) * (sign(res - p_g_fit) + 1) H &lt;- G + 0.5 * (sign(res - p_g_fit) + 1)/(tanh(0.1 * (G - p_g_fit))) return(H) } } if(control$save_traces){ trace_data &lt;- rbind(trace_data, data.frame(&quot;iter&quot;=i, &quot;mean_fit&quot; = mean(P_fit), &quot;best_fit&quot; = p_g_fit, t(X))) } } res &lt;- list( &quot;solution&quot; = p_g, &quot;fitness&quot; = p_g_fit ) if(control$save_traces){ res$trace_data &lt;- trace_data } return(res) } set.seed(0) res_pso_time &lt;- system.time({ res_pso &lt;- pso( par &lt;- rep(NA, 2), fn &lt;- function(pos){ -20 * exp(-0.2 * sqrt(0.5 *((pos[1]-1)^2 + (pos[2]-1)^2))) - exp(0.5*(cos(2*pi*pos[1]) + cos(2*pi*pos[2]))) + exp(1) + 20 }, lower &lt;- -10, upper &lt;- 10, control = list( s = 10, # swarm size c.p = 0.5, # inherit best c.g = 0.5, # global best maxiter = 100, # iterations w0 = 1.2, # starting inertia weight wN = 0, # ending inertia weight save_traces = T, # save more information fn_stretching = F ) ) }) set.seed(0) res_pso_fns_time &lt;- system.time({ res_pso_fns &lt;- pso( par &lt;- rep(NA, 2), fn &lt;- function(pos){ -20 * exp(-0.2 * sqrt(0.5 *((pos[1]-1)^2 + (pos[2]-1)^2))) - exp(0.5*(cos(2*pi*pos[1]) + cos(2*pi*pos[2]))) + exp(1) + 20 }, lower &lt;- -10, upper &lt;- 10, control = list( s = 10, # swarm size c.p = 0.5, # inherit best c.g = 0.5, # global best maxiter = 100, # iterations w0 = 1.2, # starting inertia weight wN = 0, # ending inertia weight save_traces = T, # save more information fn_stretching = T ) ) }) df &lt;- rbind(data.frame( &quot;type&quot; = &quot;PSO&quot;, res_pso$trace_data %&gt;% select(iter, mean_fit, best_fit) ), data.frame( &quot;type&quot; = &quot;PSO-fnS&quot;, res_pso_fns$trace_data %&gt;% select(iter, mean_fit, best_fit) ) ) plot_ly(data = df, x=~iter, y=~mean_fit, name = ~paste0(&quot;mean_fit_&quot;, type), mode=&quot;lines&quot;, type = &#39;scatter&#39;) %&gt;% add_trace(x=~iter, y=~best_fit, name = ~paste0(&quot;best_fit_&quot;, type), mode=&quot;lines&quot;, type = &#39;scatter&#39;) set.seed(0) from &lt;- &quot;2018-01-01&quot; to &lt;- &quot;2019-12-31&quot; spx_composition &lt;- buffer( get_spx_composition(), &quot;AS_spx_composition&quot; ) pool_returns_raw &lt;- buffer( get_yf( tickers = spx_composition %&gt;% filter(Date&lt;=to) %&gt;% filter(Date==max(Date)) %&gt;% pull(Ticker), from = from, to = to )$returns, &quot;AS_sp500_assets&quot; ) pool_returns_raw &lt;- pool_returns_raw[, colSums(is.na(pool_returns_raw))==0] bm_returns &lt;- buffer( get_yf(tickers = &quot;%5EGSPC&quot;, from = from, to = to)$returns, &quot;AS_sp500&quot; ) %&gt;% setNames(., &quot;S&amp;P 500&quot;) mat &lt;- list( Dmat = t(pool_returns) %*% pool_returns, dvec = t(pool_returns) %*% bm_returns, Amat = t(rbind( rep(1, ncol(pool_returns)), # sum up to 1 diag(1, nrow=ncol(pool_returns), ncol=ncol(pool_returns)) # long only )), bvec = c( 1, # sum up to 1 rep(0, ncol(pool_returns)) # long only ), meq = 1 ) calc_fit &lt;- function(x){ as.numeric(0.5 * t(x) %*% mat$Dmat %*% x - t(mat$dvec) %*% x) } calc_const &lt;- function(x){ const &lt;- t(mat$Amat) %*% x - mat$bvec const[mat$meq] &lt;- -max(0, abs(const[mat$meq]+0.01)-0.01) -min(0, const) } pso_res &lt;- pso( par = rep(0, ncol(pool_returns)), fn = function(x){ fitness &lt;- calc_fit(x) constraints &lt;- calc_const(x) return(fitness+10*constraints) }, lower = 0, upper = 0.1, control = list( s = 100, # swarm size c.p = 0.5, # inherit best c.g = 0.5, # global best maxiter = 100, # iterations w0 = 1.2, # starting inertia weight wN = 0, # ending inertia weight save_traces = F, # save more information fn_stretching = T ) ) 10.1.1.301.4441.pdf function stretching Boudt-Wan2020_Article_TheEffectOfVelocitySparsityOnT SV-PSO CV-PSO oh ist nur binary PSO pso_cardinality_constrained.pdf DissertationHelwig (1).pdf "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
